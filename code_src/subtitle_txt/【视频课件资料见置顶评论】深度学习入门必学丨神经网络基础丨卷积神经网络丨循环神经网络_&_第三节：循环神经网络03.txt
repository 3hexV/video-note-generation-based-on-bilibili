各位同学大家好
欢迎回来
我们接着往下讲
第四部分以及第五部分带门框的选项
增加网络
在第三部分给大家讲解rn的时候
我们就知道了
它很容易引起梯度消失或者梯度爆炸
对于梯度爆炸呢
我们经常可以用一个梯度的裁剪
也就是限制梯度大小就可以解决
但是梯度消失我们只消失了
是没有办法去恢复它的
因此我们要引入另外一种方法来解决梯度消失
在这里呢就是引入门的概念
引入门的概念
它也可以用来控制信息的流动
使得模型更好地去记住长远的信息
并缓解梯度消失
在这里给大家先介绍第一种引入门的循环神经网络叫做gr u
叫做门控循环单元
jiu呢它引入了两种人
一种是重置门
另外一种叫更新文
同志们
它的作用是用来控制哪些信息需要被遗忘
而更新门呢它用来控制哪些信息需要去注意
我们来看右边这个示意图
就是ju两个门的一个计算的方式
我们来看到下面这一个
我们从这个视图可以看到两个门它的计算输入的数据都是一样的
而输入的数据呢就是依赖于我们当前时间步的速度
xt以及上一个时间步的隐藏状态
hp减一
我们看到下面有两个公式
就是这两个门的计算公式
它的计算公式是这样的
第一个重置门rt呢它的计算公式是这样的
我们看到输入的xt呢要乘以一个权重矩阵w x
然后加上ht减一乘以w hr
再加上一个偏置
再往下更新门呢它是乘以权重矩阵w x z以及w h z
在这里我们可以看到这四个权重矩阵是不一样的
看一下下标
大家注意看一下细节的地方
下标呢它的确是表示我们的权重矩阵是不一样的
然后去计算得到门门之后要经过一个这个叫激活函数
这个激活函数呢比较特殊了
就不是我们前面所用到的
比如说rror或者是天h这里要用到s函数sigmoid激活函数
大家是否还记得sigma的值域是啥
是0~1
正因为semi它的值域是0~1
所以采用它来作为门的基本函数
为什么呢
因为零和一正好可以表示一种遗忘的程度
当这个门的值是接近于零的时候
那么我们的网络就会认为这一个元素这一个信息我们要把它忘掉
而接近于一的话
我们就要表示它是要保留
这就是门的概念
大家一定要记住
到后面我们讲l s t m的时候
它主要就是利用了这个值域0~1这个区间来控制信息的流动
是否要往后传输
还是要遗忘好
通过右边这个示意图以及的公式
我们就知道了门它是怎样去计算的
但是我们后面还没画完
我们的门计算得来之后怎么去使用呢
我们接着往下看
这个状态呢是用来辅助我们计算最终的隐藏状态的
这里与我们前面讲到rn是有所不同的
在这里我们来看右边这个c图
它的计算方式呢怎么来呢
我们来看一下左边这一公式
这个公式它的公式看起来很复杂
我们一项一项来看
第一项呢是输入数据
这应该很好地理解我们输入数据可能要用的
然后要乘以w x h
然后再加上这里就要用到了我们的同事们了
从事们在这里使用同志们乘以这一个圆圈
加一点是逐元素相乘
也是对我们上一层的隐藏状态hp减一当中的元素做一个选择
做一个重置
我们是否要遗忘上一个隐藏状态呢
就通过这一个重置门rt来控制
然后对隐藏状态进行一个选择
进行重置之后
再把这个状态乘以w hh这个权重矩阵就得到第二项
然后是一个偏置项
再经过激活函数五天h就可以了
在rnn当中
它的隐藏状态是这样的
r n当中隐藏状态它是通过输入数据xt以及上一隐藏状态
hp减一去计算得来的
而在这里呢我们的后选隐藏状态也类似
他也要通过说出去xt以及上一个隐藏状态hp减一只
不过这里多了一个门概念
重置门多了做一个重置门概念来计算
与r n当中隐藏状态的一个计算公式比较类似
但是我们发现对比这两项它还不是不对等的东西
一个是后选隐藏状态
一个是隐藏状态
所以下面我们来看一下在挤压游当中
它的隐藏状态要如何去计算
在gr u当中
隐藏状态它是由后选隐藏状态以及上一时间部的隐藏状态得来
好像很绕口
我们直接看公式
这一个ht呢有两项
一项是ht减一
另外一项是后选隐藏状态
通过这两项的一个组合而得来组合
怎样去组合呢
就要利用到我们的更新完了再更新门
直接与ht减一相乘
然后一到z t一减这个更新文字
在前面讲门的概念
我们知道这一个门它的值域是0~1之间的
所以这里用了一个e减z t一减这个门值来进行一个组合
这个方式好
我们来看右边这个示意图
右边这个示意图我们得到了重置门
重置们会用于隐藏状态的计算
然后呢更新呢更新门会用于我们最终隐藏状态
它的输出的计算
这一个相机门的使用方法是这样的
先与ht减一相乘
然后一减去这一个更新门
也就是进行一个组合
得到我们最终的ht了
到这里我们整个gu门控循环单元就给大家讲解完毕了
其实它也不是特别复杂
它就是引入了两个门
由于引入了两个门之后呢
我们的隐藏状态不会直接去输出
ht这个htone怎么得来呢
要利用到重置门
我们就可以计算最终的隐藏状态了
我们看到这边这一部分最终的ht计算的方式呢
是要利用到ht减一以及ht一弯
这里把它们进行组合
组合过程当中会用到更新门
这就是gr u当中的两个门的作用以及引入门控单元的循环网络了
在这里我们来总结一下gr u它的特点
首先它引入了门的机制
采用s型函数
可以使得门的值在0~1之间用来控制信息的流动
零呢就表示1万或者接近零的这个门值
我们表示1万接近于一的这些门值
我们就我们就表示这些信息要保留
这是加油的一个特点
第二大特点呢是如果我们更新文当中的
从第一个时间步一直到上一个时间
不也是t减一的时间过程当中一直保持是一的话
它的信息是可以有效地传递到当前时间过的
我们怎么去理解这个呢
我们再回过头来看一下隐藏状态
它的计算公式
隐藏状态它的计算公式有两项构成
也就是上映时间部的隐藏状态
这个组合的方法就是利用到了我们的更新文
如果说我们的更新文它一直保持的是一的话
我们看一下
如果这里是一
那这里后面这一项就是零了
就是零了
这就可以使得我们之前的信息毫无损失地向后传递
通过这个方式呢
我们可以从时间簿
比如说我们时间不有一开始1231直到比如说1000
我们可以让这一个信息从一阈值无损失地传递到1000
好最后我们再来总结一下gr u它的两个门的概念
lsd m
在这里呢我们先把gu的这两个文总结一下
待会我再讲l s t m的时候
你要能区分区别gr由于l s t m这两种循环网络它的门的作用
在这里我们先总结gu它的门的作用
它有两个门
一个是虫子们
一个是更新门
重置门用于控制哪些信息需要被遗忘
更新门用于控制哪些信息需要注意
而在这里呢
同志们
遗忘上一时间不隐藏状态中的哪一些信息
这句话可能比较比较拗口
再听一遍
用来控制上一时间不隐藏状态
要遗忘哪一些信息
这是从正门的作用
更新文的作用呢是更新当前时间不隐藏状态的时候
去组合上一时间不隐藏状态
ht一般这样去组合
它得到我们最终的ht
好以上就是关于gr u的知识
长短期记忆网络l s t m l s tm他带了三种门
以及引入了一个叫做记忆细胞的概念来控制信息的传递
首先我们来看l s t m当中它有哪三个门呢
这三个门分别是遗忘门
输入门
输出门
我们直接看到右边这个c图
遗忘门输入门输出门
我们看到这一个计算的方式呢
通过是这个结构图
我们就知道应该都是一样的
我们看到下面的公式
它的计算方式都是一样的
都要利用到我们的当前时间过的思维x以及隐藏状态
这一个大家应该是没有什么难度的
没有什么难度的
他们每个门呢它都有它们对应的这一个权重w
然后还有基础函数
就是s型函数
因为我们的门值呢要让它在0~1之间
这三个门的计算应该没什么问题
下面我们看这三个门它有什么用呢
遗忘门它的作用就是用来控制哪些信息需要被遗忘
而收入呢就是控制哪些信息需要输入
流入到当前的一个记忆性记忆细胞
记忆细胞是什么呢
记忆细胞它是一种特殊的隐藏状态
用来记忆历史信息
也就是我们的历史信息呢会存储在记忆细胞这个单元当中
这个概念当中
再来看输出门
输出门它是用来控制哪一些记忆信息要流入隐藏状态
我们的隐藏状态呢在这里是用来辅助我们去计算一些门值
这里我们反复讲到了一个记忆细胞
那么什么是记忆细胞呢
我们往下看
记忆细胞我们可以理解为一种特殊的隐藏状态
它是用来存储历史时刻的信息
我们可以把它类比为rnn网络当中的ht
也就是r n当中的隐藏状态
在这里呢引入了g细胞c
把历史信息存储在了这一个c这一细胞这一个单元当中
我们来看一下
h t e y它的计算也是差不多的
它只需要拿到输入数据x t以及上映时间过的隐藏状态
hdn一进行计算
然后经过一个t h就可以了
大家注意
我们如果是记一些记忆信息的话
记的数据我们直接用tan h
如果是门控的话
我们就用s s型函数
sigma函数好
往下我们就可以计算这个记忆细胞了
这里的记忆细胞的计算方式呢可以类比于gr u单元
它的隐藏状态的一个计算方式
我们先来看记忆细胞
它的计算方式
一个是上一时间部的记忆信息
就是c t减一
另外一个就是我们当前的这个这有一弯比较模糊
把这两个数据进行组合得来
当然了
这个组合方式与ju当中还有所不同
这里引入了两个门进行组合
下面我们就来看右边这个c图
看一下我们的g细胞c是如何计算得来的
我们来返回到通过这个键的
回到这一个加号
这里就知道它是进行组合相加
那么我们先看左边这一个
它是通过上映时间部的记忆细胞c t减一
然后乘以一个1万文
这一个1万文的作用呢我们就很好理解了
用来控制我们传进来的记忆的信息机的数据是否要遗忘
是遗忘的话
我们这一个门框输出的那一个值
那个位置呢就是零做的附近
如果要保留呢
那个门框的值呢就是一附近
这就是我们利用一个门控进行逐元素的相乘来遗忘我们历史信息
然后我们再加上一项这一项呢
在这里这里也是一个主元素相乘的乘哪一些内容呢
这就是一个输入门
所以在计算cp也就是当前时间步的记忆细胞的时候
我们要用到两个门
一个是1万门
一个是输入门
这就是c的一个计算
好到这里我们知道c的计算之后
我们还有一个重要的信息要去计算
叫做隐藏状态
我们的隐藏状态呢会被输出门进行控制
去控制我们的记忆细胞流入隐藏状态的一个多少
下面我们来看右边这个示意图
ht ht怎么计算呢
我们找到这个曲线
这条直线在这里它还是一个主元素相乘的
主元素相乘呢就表示它有一个门控的概念
乘了一个信息呢
乘的是我们ct的信息
c还要加入一个天h的一个计划函数
我们看到
加入t h的基函数之后
再与这个门框去组合
得到我们最终的ht
左边呢就是这一个计算公式了
他先将我们的c t记忆细胞输入到t h
得到我们-一到正一的这一个记忆信息的数据的一个形式
然后呢再经过输出门的控制来控制我们记忆细胞当中的信息有多少
去流入隐藏状态ht当中
好到这里
我们整个l s t m长短期记忆网络里面的基本的其组件
我们就讲解完毕了
它主要就是有三个门
三个门
一个遗忘门输入我们输出门
还有引入了一个记忆细胞
这个记忆细胞是用来控制信息的传递
我们可以把这个记忆细胞类比于2n网络当中的隐藏状态
用来记历史的信息
在这里这三个门的作用呢就可以体现在左边这三个公式
这两个下最下面两个公式当中
两个门
第一个遗忘门用来遗忘上一次进步的记忆细胞输入门
输出门是用来控制当前时间部的记忆细胞有多少
流入当前时间部的隐藏状态
这一切呢都可以通过右边这个示意图来描述
因此我们通过这一个视图辅助我们去理解这些公式的运算
以及理解整个l s t m它运行的机制
然后呢大家只要记住右边这个示意图
就能知道l s t m它是如何工作的了
好到这里
我们这一节关于循环神经网络的知识就讲完了
下面我们对这节课的知识点进行一个简单的回顾
我们这节课主要讲了五个部分
首先我们介绍了序列数据
我们在日常生活当中很多任务都是序列数据的形式
然后给大家讲解了自然语言处理技术当中非常重要的语言模型
同时也通过一个小例子给大家讲解语言模型它的作用是什么
我们通过了序列数据和语言模型的铺垫
就正式进入了针对序列数据处理的循环神经网络模型
在循环神经网络模型当中
在2n网络模型当中给大家详细讲解了数据是如何前向传播的
我们的模型是怎样循环使用的
以及讲了前向传播之后
给大家详细推导了梯度的反向传播
在i n的梯度反向传播
它有一个神奇的名字
叫做穿越时间的反向传播
这是由于它会递归的去使用我们的梯度
而递归的使用梯度的一个过程当中
会引发一个梯度消失和梯度爆炸
针对于梯度爆炸呢
我们可以用梯度裁剪的方法来解决
而梯度的消失我们很难去解决
因此就引入了门控单元的循环直接网络
在第四部分
我们开始讲解第一个门控单元
第一个门控循环神经网络gu
在机甲鱼当中呢
它引入了两个门
第一个是重置门
用来控制哪些信息需要遗忘
第二个是更新门
控制哪些信息需要去注意
l s t m长短期记忆网络
在这个网络当中呢
它引入了三个门
同时还引入了一个叫做记忆细胞的概念
记忆细胞的概念
这个c这三个门呢分别是1万门
输入门
输出门
用三个门来控制我们的信息的流动
同时把我们的记忆信息存储在的记忆细胞c当中
以上就是整个关于循环神经网络的内容了
好到这里呢
我们选修至当中的神经网络部分就接近尾声了
我们用了一天时间
用了三个课时
找中晚三篇给大家介绍神经网络基础知识
针对图像的卷积神经网络
针对序列数据的2n网络
这一天的内容我们知识点还是非常丰富的
下面我将用一个思维导图给大家回顾一下我们这一个神经网络的课程
介绍哪些知识点
辅助大家课后做笔记
好我们看一下这个思维导图主要是三个部分
我们在第一节课当中
神经网络基础里讲了非常多关于神经网络的概念
首先我们重点讲了多层感知机
我们通过多层感知机知道神经网络有前向传播和反向传播的一个过程
在前向传播过程当中呢
引入了一个非常重要的激活函数的概念机
黄色对于多层感知机而言
就像水对鱼的重要性
没有水就没有鱼的存在
没有这些非线性基函数
多层感知机它就不能叫多层感知机了
这是由于我们矩阵乘法有结合性
再往下我们就开始讲解反向传播
在反向传播的讲解过程当中
我们介绍了计算图的使用
我们通过计算图可以很好的去理解梯度是如何从后从这个loss
从这个l从后向前传播的
以及讲解了我们得到梯度之后怎样去更新我们的权重
我们会利用梯度下降法让我们的权重更新
然后使得模型的输出更接近标签
我们的模型输出更接近标签
怎样去衡量呢
我们就介绍了损失函数
在损失函数当中
我们还介绍了损失函数与代价函数与目标函数这三者之间的差异
这三者之间的差异
一开始我们作为入门的同学很容易去混淆
大家一定要回到这里去
重点去做一下笔记
去区分这三者之间的差异
在损失函当中呢
我们就介绍两种最常用的损失函数
第一个m s e均方误差
它主要在回归任务当中去使用
第二个是交叉三所参数
交叉熵损失函数呢它主要用在分类任务当中
它可以衡量两个概率分布之间的差距差异
而我们的模型输出怎样能符合一个概率分布的形式呢
我们知道我们在计算的时候
我们的模型速度很有可能是负的
也很有可能是大于一的值
这些都不符合概率分布的要求的性质
所以我们使用交叉上的时候
通常要用到一个激活函数
叫做so max函数
soft max函数很巧妙地将数据将我们输出的向量压缩到10~1之间
并且让它们求和等于一
就符合了概率分布的输出的一个性质
然后就可以输入到我们的交叉熵损失函数去计算了
再往下我们介绍了对于神经网络训练非常关键的一个技术
叫做权重初始化
在权重初始化当中给大家介绍一种错误的初始化方法
就是全能的初始化
而正确的初始化方法呢
我们要利用到随机分布
也就是从一个概率分布当中随机的去采样
得到值之后
赋予我们的权重进行初始化
而在采用随机分布
采用分布进行初始化的时候呢
这一分布有两个重要的参数
一个是均值
另外一个标准差
均值呢我们通常要设置为零
而标准差呢我们要设置为比较小的数
我们讲解三西格玛准则
通过三西格玛准则
而标准差该怎么设置
多大是大
多小是小呢
我们就要用到自适应的方法
在制定方法当中提供了两个
一个是zio初始化方法
另外一个是何凯明初始化方法
再往下我们就介绍了正则化方法
正则化方法它是音译过来的一个名词叫做regulation
我们通过regulation很难去理解这个方法它到底是干什么的
什么是正则化呀
其实正的话它就是做减小方差的
减轻过拟合的一系列的策略
这些策略有哪些呢
我们给大家介绍了l one l two
而l two呢又是最常用的l two它会称之为with decay
为什么l two又称之为rate decay
权重衰减
全职衰减呢
我们又通过梯度下降的求导公式给大家推导了
加入了l2 正则项之后
它的权重又多乘以了一项因子
而这一项因子呢恰恰是小于一的
从而就体现出来了权重被收缩了
它缩小了因子会称之为权重衰减
然后我们又介绍了一个实用的jpd随机失活的这个正规化方法
在随机复活过程当中呢
有一个实现的小细节
这里就不讲了
如果大家还不记得的话
就回到我们第一个课程当中去听一下
最后还给大家介绍了四种高级的正当化方法
best ation g n l n i n这四种政治化方法呢就可以换成两个部分
第一部分就是b n
后面三种都是基于b针对特定的应用场景去改进得来的
normalization的方法啊
b呢我们而bn我们要详细去讲
因为bn这篇paper呢我会在cv的baseline当中给大家详细讲解这篇论文
好这里就是我们第一节课
我们主要介绍了什么是人工神经网络
什么是神经元以及神经网络当中有前向传播和反向传播
我们会数据怎样进行前向传播和反向传播
以及我们怎样去优化更新我们的网络模型以及更新过程当中
我们也会用到哪一些技术
这些都是神经网络的基础
讲完基础之后呢
我们用了两个课时来讲解针对于图像领域
序列数据领域去使用的这两个神经网络模型结构
第一个是卷积神经网络
它是针对图像应用领域提出来的
我们介绍它的发展史
它怎样通过猫的视觉系统实验去启发
然后一步一步经过数代人的发展
去构造这样一个神奇的强大的卷积神经网络模型结构
那现在这一个cn结构已经统治了大部分的图像任务
在卷积神经网络当中有两个非常重要的操作
叫做卷积操作以及策划操作
再往下就这一个课时给大家介绍的新方式一网络了
在希望这些网络中主要给大家讲了三种结构
第一个是最基础的rnn结构
这是由于r n当中有它的缺点
这个缺点就是反向传播过程当中
它的梯度会随着时间不t指数级的变化
而这个变化呢大家是否还记得我给大家喝了个鸡汤
就是你每天进步一点点
365天就会进步很大
你每天退步一点点
365天之后就退步很大
在这里它的梯度也是一样的
也不断的相乘
这个值就会消失
接近于零
而如果这个值大于一的话
不断相乘
它就会不断的增长
最终会变成很大的一个数
使得我们的梯度爆炸
而针对这个缺点呢
就提出了门控单元
在门控单元当中gu是有两个门
l s t m有三个门
这五个门具体是干什么的呢
我就不再详细讲解了
好到这里就是我们整个选修知识当中神经网络部分的知识点了
讲到这里
大家有没有发现我们这一天的课程三个课时
他讲的知识点还是非常多的
我们一天其实很难去理解透它
在这里我尽可能用在短的时间内把大家基础给弥补上来
在第一个课时的开篇的时候
我也给大家提
在第一课时开篇的时候
我也给大家打了预防针
也给大家提供了一些课外的补充资料
神经网络它是一个比较大的概念
我们一天的时间很难去把所有的知识点都了如之大
在这个部分
我尽可能用较短的时间
把所有的基础的必须必备的知识给大家总结出来
而且用简短的时间给大家讲解了
因为此这一个部分这三个课时呢
更重要的还是给大家构建一个知识体系
就像现在这一个思维导图给大家展示的一样
你一开始要学神经网络基础
学好这些基础之后
你再去对不同的应用领域
比如说图像领域
你去学卷积神经网络
如果你是做序列数据的
比如说做一些n2 p技术的
因此还推荐大家回到我们开篇的时候
或者可以去看一下李宏毅老师的课程
他用一个学期一个学期的时间给大家讲解神经网络
或者可以看一下花叔
花叔呢可以通过深度之眼的训练营去吹西
就是周志华老师的西瓜书
这一个西瓜书呢在神州之前也有相应的训练营
好到这里
我们整个选修知识的神经网络的部分就到此结束了
最后给大家介绍一句名言
叫做千里之行始于足下
这是老子说的
不是老子
老子是于老师
老子是老子好
千里之行始于之下
我觉得它非常适合我们第三课时的循环神经网络
神经网络是一步一步的
根据时间不往下走去计算得到我们最终的输出
我们千里之行始于足下
我们要脚踏实地
先把基础选修知识的基础打扎实了
再往后去学我们的paper论文的知识好
最后也预祝大家能在论文班收获满满
我是于老师
期待在cv paper bassline当中与大家相见
这节课就到此结束
感谢大家的收听
