各位同学大家好
欢迎回来
我们接着往下讲
第四部分
反向传播算法
在神经网络当中
信息的流动有两个非常重要的方向
一个是前向
一个是反向
前向传播指的是输入数据逐渐的从前向后逐层地传递到我们的输出层
而反向传播呢则是从损失函数开始
从后向前传递信息
传递什么信息呢
传递的是一个梯度信息
大家看一下
传递的是梯度信息
而我们前向传播的传统是输入层的数据
也就是我们样本
我们输入一样的
这是神经网络当中两个非常重要的传播的概念
传播的方向一个是前项
一个是反向
在前面我们学习了多层感知机
我们知道了数据怎样从前到后的传播
这一小节我们就来学习梯度
它是如何从后向前进行反向传播的
我们知道反向传播指的是梯度
从后向前传播
那么传播这一个梯度反向传播它有什么作用呢
它的作用其实用来进行权重的更新
全职的更新就可以使得我们的模型输出更接近标签
也就是进行网络更新
在这里有一个非常重要的概念
叫做损失函数
我们这里提到使网络输出更接近标签
怎样去衡量我们模型输出与标签之间是否接近呢
它们差异有多大呢
这一个就需要用到损失函数了
损失函数它是用来衡量模型的输出与真实标签之间的差异和差距
在这里我们通常会定义成这么一个形式
我们看一下
这里其实是一个函数
这个函数接收两个变量
一个变量叫做y hey he呢我们可以理解为模型的输出
这是模型的输出
y hat这个应该在这里了
然后这个y呢就是我们样本的真实标签了
就是label这个函数去计算这两个变量之间的一个差异
得到的值呢
我们通常会称之为loss值
在这里loss值关于损失函数呢
我们在下一个小节会详细的讲解
在这里大家知道损参数用来衡量模型输出与真实标签之间的一个差异
这个值越小的话
就能表示我们的模型它的表现越好
反向传播它的作用那么大
它那么神奇
那么它的原理是什么呢
其实原理是非常简单的
大家在高等数学当中都学过这一个这个原理
它的原理其实就利用到了微积分当中的链式求导法则
链式求导法则
不知道大家是否还记得链式求导法则
这里就给大家举一个非常简单的例子
帮助大家回顾链式求导法则
这里呢我们举一个右边这个的这个式子的例子
首先我们看一下y呢
它是关于u的一个函数
也就是优势自变量y是因变量
u呢要是关于x的一个函数
就是x是自变量
u是因变量
在这里我们要求导y关于x的梯度的偏导的时候呢
就要利用到链式求导法则
我们看到y是不能直接与x找到联系的
它需要通过这个中间的变量u所以我们y要先找到u的偏导
然后u再找到与x之间的联系
然后u在求x偏导
这里我们可以用一个简单示意图来理解这个过程
它的一个链式的过程
由于我们是从y开始求导
我们用一个方框来表示一个数据
也就是y
然后我们有一个数据是u
因为y呢是由u得来的
就是u输入到了这个函数
称之为f的函数
这一函数输出才得到y的值
在这里我们一开始y是找不到与x的联系的
所以还要往前走
再往前走
要找到这一个关系式
我们再用一个方框来表示我们的x数据
我们用矩形表示数据
然后一个箭头呢表示是操作
我们看这个
那么这个箭头应该是一个叫做g的这个函数
好x uy之间的关系就可以用这一个这个示意图给表示出来了
我们x计算得到y的一个过程呢
就是用x输入到这个g函数得到uu
在输入的f函数得到y
接下来我们要求y对于x的偏导导数的时候
就从y a出发
这样找到关于u的关系
u才能再找到关于x关系这么一个过程
就是链式求导法则
我们看到这就像一个环
一个环紧紧相扣
这就是一个链式求导的一个过程
我们知道了反向传播
它是利用链式求导法则来求取的
下面我们就举一个简单的多层感知机的例子来理解
在多层感知机在一个神经网络当中
我们的梯度是如何从后向前传播的
在这里我们就要借助到一个计算图的概念了
这里我们看到右边这一个示意图的下半部分
这就是一个计算图
计算图
它主要有两个主要的结构构成
第一个结构叫做节点
第二个结构叫做边
我们先来看第一个结构
节点节点它就表示的是数据
我们来看一下这个示意图
当中方块就矩形的部分就是节点
我们看一下这些节点有哪些呢
有x我们知道x是数据
w one权重矩阵一
它也是数据
然后再一个z它也是数据
我们看到最后面的o输出
它也是数据
也就是节点呢用矩形表示
它表示是数据
另外一个主要的结构就是边哪边
它其实用圆圈来表示
就是这些箭头所连接这个圆圈
这些圆圈呢表示就是操作
我们看到这个叉叉就是乘法是个five
就表示激活函数
还有最后面这一个这一个给大家这个表示的就是loss
bun计算loss的一个操作
因此这一个计算图就是由节点
由我们数据和操作构成的
计算图呢它是非常实用的
它可以帮助我们去理解网络的计算过程
同时对我们进行反向传播的理解是非常有帮助的
我们下面来看一下这一个计算图
与我们多层感知机之间的一一对应的一个关系
这一个三层的感知机大家应该很熟悉了
我们可以先把权重矩阵写出来
这一个第一个权重矩阵我们可以把它写成w one
第二个权重矩阵是在这里的w two
这里呢大小就也可以写一下
非常简单
我们就输入乘以
输出4x5
这里5x3
但是我们这里用不到这一个shift
用不到这些形状
大家知道有这么一个东西就ok了
然后我们来看一下这一个计算组是如何去描述
我们上面这一个三层的多层感知机的
首先我们有一个输入x
我们输入数据
输入数据
我们知道要乘以一个权重叫做wy
然后得到一个z的变量
这个z变量在我们示意图当中是没有显示出来的
为什么这里要用一个z变量呢
这是由于我们计算图当中每一步操作都是最细粒度的操作
也就是呃会把一个一个网络层的多个操作
它一定要拆开
我们每一个细腻度最小的操作都要把它给绘制出来
那么在这里我们就可以把这一个x w写出来
把这个公式写一下
也是我们写到最上面
大家看到我们最上面第一个呢应该是x乘以w one
得到这里之后
我们在隐藏层我们知道要经过一个激活函数
在前面也讲了
如果没有激活函数
我们的多层感知机其实等价于一层的
这里我们就把z呢输入到了这个呃出的这个激活函数当中
得到了h隐变量啊
隐藏层的成员的值
这一个应该是fy
这里是第二个操作
第二个公式得了h就来到了隐藏层
我们隐藏层再往前就往往后传播
那么就是要h乘以我们的w two
也就是第二个权重矩阵的
我们h乘以w two就得到我们的输出了
在这里呢我们输出暂时是不带上计算函数的
ok就来到了o这里
我们再来看一下
就来到了o这里
得到输出呢
我们就要把输出输入到损失函数当中
同时要把标签也输入到我们的损失函数这一个l当中
这里就表示了损失函数
前面我们也讲了
损伤数接收两个变量
一个变量模型输出
另外一个变量是标签
它这个函数呢输出的值就能只是这两个变量之间的一个差异
这里输出的是一个l这个值lost值我们通常称之为loss值
这个值越小的话
就表示了我们的模型输出与标签之间它是越接近的好
这就是前向传播的一个过程
我们看到这么一个三层的感知机
它从x从前向后就是前向传播的一个过程
就可以通过这一个示意图这个计算组来描述
我们知道前向传播
下面我们再来看反向传播
反向传播就是从lost开始
我们的梯度怎样的
从loss不断的向前面传
我们下面来看一下
首先我们要给大家定义一个计算符
定义也就是这一个pd一个这个计算符
这个计算符呢它表示x与y要根据形状要做一定的去变换
然后才可以进行相乘
为什么要用这个计算符呢
因为我们这里会涉及向量或矩阵的求导
这里就要可能涉及一些转制啊
或者是左乘右乘的关系
这也为了避免我们计算的复杂
为了更简便简洁的让大家清晰的去理解这个反向传播的过程
所以对于矩阵的这一系列操作
我们就简化了
只用这一个计算符来表示
表示xy要进行相乘就ok了
那么下面我们首先来看一下
从loss出发
我们求取距离它最近的第一个变量的一个梯度
我们先来看求取w two
也就是从后向前向来先来求这一个先求w to的一个梯度是什么呢
w to的梯度
我们知道链式求导法则
我们要找关系
首先我们从l出发
因为l直接与w to是没有关系
找不到联系的
所以我们l呢要找到与o的联系
然后o呢再找到wto的联系
这样就是一个电视的求导的一个过程了
我们回到这个公式来看一下
首先我们要l对o求偏导
然后o呢再对w to求偏导
我们可以通过上右上角这三个公式
我们知道o与wto之间的关系呢
求一个偏导
他们求导之后应该是等于h的转置
等于h的转置
因为这是一个向量的形式
这样我们就得到了pl pw to的偏导
就这一个形式
接下来我们再往前求一个偏导
就是来找这个数据这个变量h它的一个导数
其实关系也是一样的
l直接找是找不到与h之间联系的
首先l要找到o的联系
o呢在从后向前找到h的联系
这样我们同样来看一下公式也非常简单
偏l po po ph
同样我们通过第三个公式呢
我们也是可以得到这么一个形式的
就是它的偏导数等于w to的转置乘以偏r偏o好
那么接下来我们再往前就是从后我们再往前看来找一下pl pz
它的梯度又是怎么计算的呢
我们同样还是知道要找l与z之间的联系
它们之间的关系还是要从后l找到o o找到h h再找到z
我们看这条路
ok我们再回到公司来看一下这里同样的偏r偏o po ph
偏h偏z
我们通过上面这个公式二
我们是可以知道偏h偏z就是对这一个激活函数进行求导
就是它的激活函数的导函数
这个圆圈当中
圆圈当中加一点表示除元素相乘的意思
好在这里我们发现有一个要注意的地方了
偏l偏z它的导数
我们看一下它的梯度
这一项乘以后面这一项
这一项是偏l ph
也就是等于我们上面这一项
大家看一下
就是求偏l偏z的时候呢
我们要用到了偏h偏r偏h的梯度
看一下这里就是我们求这一项的梯度的时候
要用到它后面一层的一个梯度
因此我们在求z的梯度的时候呢
没有必要从l再重新求一遍
而是我们直接拿到h的梯度
再乘以一项
这一项就是h与z之间的关系
这一个梯度就可以了
这就体现了一个反向传播的过程
我们发现这一个梯度呢直接传给了传给了z这一个地方
然后再乘以乘以我们偏h偏z就可以得到偏l偏z的一个关系了
大家重点关注我们公司这里的一个相关联的两项好
我们再往下求一下
来观察一下这个反向传播的一个过程
我们再往前求
往前求
就是来到了第一个权重矩阵w one
我们看一下同样的l是直接与w y是没有关系的
我们要通过这个箭头找到他的联系
同样的偏l po偏偏h偏h偏z偏z在找到关于w one的联系公司
这里也是一样的
刚刚就是诶公司这里公司这里也是一样的
刚刚就是从l的o o的h h到z z的w y
这么一个链式求导的一个过程
在这里呢我们知道可以看到前面这三项
其实就是上面我们所求的偏l偏z了
这里我就直接偏l偏z
然后我们要乘以项偏z偏w one
我们通过右上角来看一下右上角这个z和w one之间的关系是公式一
我们通过这个公式一呢就可以得到得到偏z偏w one的的值就是x转置
从这一个等式我们也发现了
我们要求w one的梯度
只需要上一后一层z的梯度乘以一项x转置就可以了
这又体现了梯度的从后向前传播的一个过程
我们再看一下w one
它的梯度呢是通过z的梯度进行传播而来的
我们看一下现在整个前向和反向传播的过程
我们都讲了
我们从从头再来回顾一下前项和传
我们再重头回顾下前向传播和反向传播它的一个差异
我们看一下这个示意图当中的黑色的线是原始的前向传播的一个过程
我们的x呢从前向后传播到后面
然后我们反向传播的时候
这些数据就是我们的梯度了
我们的l我们的梯度从l这里出发
然后传到o o呢
又传到这个h h再传到z z
正传到w one这么一个从后向前的传播的一个过程
它就是反向传播的一个过程了
还有这里我们要重点的理解
在这里我们要重点理解这两个变量之间它是相等的
等价的这么一个过程好
这就是前向和反向传播的一个过程
在这里呢再给大家讲一下这个计算图是非常有用的
在这里再给大家讲一下这个计算图呢是非常有用的
这个我们到后面讲解循环神经网络讲它的梯度消失
梯度计算的时候也要利用到这一个计算图
在计算图当中我们怎样求一个变量的梯度呢
其实就是从我们的loss出发
找这个连接的通路
有多少条通路
它的梯度就有多少项进行相加
现在讲可能很难理解
就是有多少条通路
比如说我们l到w two
它有几条通路可以走呢
我们发现现在只有这么一条路
所以他的梯度现在只有一项
如果说我们wto还用到了别的地方
在与l联系的话
一定要还要找到别的通路再去联系
比如说我们l two在进行某种计算
然后再连到了这个l的话
那么我们对于w two求梯度的时候
我们要从这里找l从这个方向找到与wto的联系
进行链式求导
得到一项
同时我们l还要从这一个方向再去找与w to的联系
得到第二项
把这两项进行相加才能得到我们最终关于w two的梯度这个概念呢
我们在后面讲
喜欢设计网络上还会详细讲
在这里呢就提前给大家讲解
有这么一个概念
我们计算图求反向传播的时候要找通路
有几条通路
它的梯度就有几项进行相加的好
下面我们就知道了反向传播的具体过程
我们是将梯度从后向前传播传播之后
求取到每一个变量
它的梯度之后
我们怎样去更新我们的权重呢
这里就要用到梯度下降法了
下降法就是权重沿着梯度的负方向进行更新
使得函数值减小
这里要注意是沿着梯度的负方向
为什么我们的全职沿着梯度负方向就可以使得我们的函数减小呢
这一个概念还是我们高等数学当中的微积分当中的概念了
在这里如果要理解梯度的概念
我们要讲导数方向
导数和梯度这三者之间的概念了
首先我们来看一下导数导数
它指的是函数在指定坐标轴上的变化率
这里我们要强调一个叫做指定坐标轴
如果我们在一元函数的话
我们几乎很少听到这个词叫做指定坐标轴上
我们一般会说是函数的变化率对吧
比如我们来看一个简单的例子
就是示意图当中左边这个它是一个y等于4x平方
是一个曲线
一个一元函数
那么我们求它的导数
其实就叫做求它的变化率
比如说我们现在在这一点想求它的一个变化率呢
我们导数我们求它的导数
我们通常就叫做求它的变化率
就是找出变化率
就是x有一个小的增长
德尔塔x然后在这里看一下它的y变化了多少
我们叫做德尔塔y
那么它的导数d y d x呢就是就是德尔塔y比上德尔塔x了
这就是导数的概念
导数呢也可以理解为一个斜率
就是在这一点切线
这个切线的斜率用来描述它陡峭程度
这就是在一元函数当中
我们导出了一个定义
在这里要强调一下是指定坐标轴
因为我们指定也是x轴上的一个变化率
那么什么是方向导数呢
方向导数要在多元函数当中才有这么一个概念了
方向导数它的定义是指定方向上的变化率
怎么理解这个指定方向呢
我们来看右边这个示意图
这个示意图是一个二元函数函数表达式可以理解为这样
它是二元函数
你接受x和y
然后函数值是z
因此它是可以在三维空间中画出来这个函数的曲面曲线
在这里就可以把这个函数非常形象的描述成一个山坡
那么山坡的高度呢就是这个函数值
假设现在我们在这一个函数这个位置再画粗一点
这个位置在这个位置上
我们想求它的变化率
那么该怎么求呢
由于在这一点它有很多个方向
在不同方向上它的变化率是不一样的
假如说我们这一点在这个方向它有一个变化率
在这个方向它有一个变化率
然后我们就可以平行于y轴
它有一个变化率
平行于x轴
它也有个变化率
这些变化率就是方向导数了
也就是在不同方向上的指定方向上的一个变化率
那么方向导数其实它是有无数多种
因为我们知道360度
各方360度上
它是有无数个角度的
无数个方向的
那么多个方向导数
我们肯定会关心哪些是最小的
哪些是最大的
这里我们有一个最大的一个定义
就是使得方向导数最大的那一个方向就是梯度的方向了
这里我们看一下梯度的定义是什么
梯度它其实是一个向量
一个向量一定会有一个方向
梯度的方向呢就是使得方向导数取得最大的那一个方向了
这里我们来看一下
如果在这一点呢
哪一个方向是它变化率最大的
那肯定是朝着山顶的这个方向
这一个方向上它的变化率最大
找到了方向导数最大的那个方向
这个方向其实就是梯度的方向了
但是在这里我们发现梯度的方向是朝着山顶去增长的
也就是如果沿着这个方向走的话
我们的函数值是增长的
但是在这里我们看到了
我们提出下降法是要使函数值减小
也就是要让我们的损失值减小
让我们的模型输出更接近标签
因此我们要沿着梯度的负方向
也就是这个方向想要看这个方向去去改变
去改变我们的权重
改变我们的参数
才可以使得我们的函数值减小
我们看到这个方向就是朝着我们坡地山坡的底部去走的
因此朝这个方向去走的话
我们的函数值可以迅速下降
也就下降的更快一些
这就是为什么梯度下降法当中
我们的全职是沿着梯度的负方向更新的
因为梯度的方向是增长最快的方向
负方向就是下降最快的方向了
如果对于导数方向
导数和梯度还是理解的不是很透彻的话呢
建议大家拿出高数书翻一下
就很容易理解了
通过前面我们就知道梯度下降法就是让我们的梯度沿着负方向去更新
就可以使得我们函数值减小
在这一个操作过程中
还有一个非常重要的概念叫做学习率
这学习率呢是用来控制我们更新的一个补偿
如果没有学习率的话
我们的更新很有可能达不到使我们函数值减小的这个目标了
下面我们举一个例子
就是没有学习率的时候
我们进行更新
看一下我们函数值是否能减小到我们的最小值呢
在这里我们先定义一下前面我们所讲的
我们权重沿着梯度的负方向更新
它的更新公式就是这一个
这是我们当前的参数值
权重值减去它的梯度
减梯度也就是沿着它的负方向更新
我们利用这一个公式看一下进行更新
我们的函数值它能否减小呢
这里我们同样还采用前面这一个简单的一元函数
就是y等于4x平方
然后我们对这个函数进行求它的导函数
就是就是8x非常简单
那么在这里我们假设我们起始点等于x是等于二的
就是x0 是在这个位置
我们从这里出发
然后来求它的梯度
求它的导数
然后利用梯度下降法
看一下能否使得我们的y值从这个位置应该是16
因为我们二代进去就是二的平方乘以四是16
我们是否能从16逐步的下降到零呢
我们利用上面这一个公式来看一下
首先我们要求它的梯度
也也就是在这里呢
也就是求它的导数
我们求导数也就是把x0 等于二代入它的导函数呢
就得到这个16就是2x8的16
然后我们要利用这些公式了
就是当前值减去它的梯度
减去它的导数
2-16等于-14
这里就是经过一步梯度下降之后呢
我们的自变量x来到了-14
它的函数值是多少呢
我们把-14带回到我们的这个函数表达式当中
是x平方当中
它就应该等于四乘以-14的平方是784
我们发现经过了一步更新之后
好像函数值并没有减小
并没有减少呀
他一开始是16
我们经过这一个梯度更新之后
这一个梯度下降法更新之后
函数值来到了780
784在哪里啊
这一个坐标轴都没有办法展示出来
他应该在这个位置
784
我们函数值并没有下降
反而是飙升的
那么我们只爱更新一步
观察下会发生什么情况
我们接着求它的导数
就是在x一的时候
它的导数
我们把x一带入导函数8x当中
代入8x当中
就是八乘以-14
就是得到-112
然后再利用我们梯度下降这一个公式
就是这个公式x一减去它的导数就是-14
减去112
减去负的112
就是加上112
最终得到98
我们看到第二步更新
这是第二诶
我们看到这是第二步更新
这是第二步更新
第二步更新之后
我们的自变量x值呢就来到了98
那么这个时候它的函数值是多少呢
代入这一个4x平方
它就变成了3万多
我38416
我们从748经过一步提速下降之后来到38000多
38000多更高了
这就飞升了
就飙升到38000多
我们我们跟大家看不到这一个这一个位置了
我们发现这一个我们仔细看一下这个更新公式没有错呀
为什么我们采用了梯度的负方向去更新
它的函数值没有下降
反而是飙升了
这就是由于它更新的互长太大了
也就是速度太快
容易翻车
翻车了
我们函数值没有减少
反而飞升了
在这里我们就要加上学习率的概念
用学习率来控制它的更新不长
下面我们来看一下利用学习率之后我们的函数更新的过程
在这里还是上面这一个一元函数的表达式
y等于4x的平方
这里呢我用程序使用了四种不同的学习率
给大家展示这个函数值优化的一个过程
这里呢我们先定义一下
加上学习率之后
更新公式是怎么样的
我们来看一下这两个对比
左边呢是没有学习率的时候
右边是有学习率的
这个学习率呢就乘以它的梯度就可以了
就用来控制这一项
也就是控制我们后面这一项的大小
这里呢我们学习率通常是小于一的
通常小于的
我们有0.1或者0.01
0.01
0.001的一些常用的一些学习率
在这里我们也可以看到
其实为什么学习率不是一呢
我们在没有学习率的情况下
它其实等价于这个学习的是音
因此我们学习率通常是要小于一的
如果你等于一的话
非常容易产生我们上一页ppt所展示的那样函数值逐步的飞升
这里给大家展示了四种不同学习率
我们来看一下不同学习率的情况下
我们的一个优化过程
我们的函数只是一个下降的一个变化
首先第一个是斜率等于0.26
第二个是0.25
这是0.24
这里是0.1
来看一下这个图例
好我们一个一个来看
我们先看大的学习率的时候
首先我们看0.26的时候呢
我们起始点呢
我们所有的起始点还是在二这个位置啊
然后我们看到这一个函数值是逐渐的逐渐的往上走的
并且呢它的跨度越来越大
每一步跨的这个增长的速度越来越快
我们看下面可能每一步只增长可能是十几
到上面呢我们一步就增长了
一
一步就增长了几十
我们发现这学习率太大了
与我们上一届ppt所展示的学习率等于一的情况是一样的
学习率没有下降
下面我们降低一下学习率
看一下0.25的情况
这个情况非常有意思了
我们看到从起始点x等于二的时候
他跳到了负负
又回到了二
它的函数值永远等于16
等于16
这是非常有意思的
也就是我们函数还是没有下降
就证明我们的学习的可能还是太大了
那我们接着下降减小的学习率呢
假如我们学习来到了0.24的情况
我们从这里出发
发现我们的损失函数值
我们如果是loss function的话
就是lost
我们发现我们的函数值终于下降了
不像前面的飞升或者是不下降
我们看到这个函数值慢慢的慢慢的下降到这里
然后在这里震荡很多步
但是它还是趋近于lol loss值
y值等于零的这一个这个位置的
趋近于这个位置
但是我们发现这个过程太慢了
我们能否有更快的方法快速达到这个零这个最低点呢
我们来看我们将学习率调到了0.1的时候
我们这个优化过程是这样的
从二这个位置一步的
他就来到了接近于零的部分
我们看这个位置呢可能是可能是一吧
我也继续去算
大家可以理解为这个位置大概就一附近
然后一步就可以到这里非常低的地方
再经过一步就几乎来到零的地方了
这就是学习率的作用
它可以控制我们更新的波长
很多情况下呢我们需要小的学习率
要一步一步的去接近我们的最低这个极值点
极小值点
在这里就通过这一个小例子再给大家直观的去理解学习率的作用
而且我们通常的学习率一般会初始化为这些值比较小的值
只有特殊情况下
我们才会有初始学习率为一的时候
非常特殊的时候
我们才会用一好以上
第四部分就是我们反向传播算法
以及梯度下降更新和梯度下降更新当中会用到的学习率的知识点
下面我们稍事休息
接下来回来再往下讲
第五部分
损失函数
