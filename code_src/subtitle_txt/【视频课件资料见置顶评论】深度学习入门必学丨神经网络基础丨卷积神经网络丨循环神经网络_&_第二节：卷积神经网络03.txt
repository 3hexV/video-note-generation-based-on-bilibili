上面就给大家讲解了卷积网络当中卷积层的具体操作
下面讲卷积网络当中的第二个重要操作
磁化要讲实话呢
我们还要回顾之前就给大家讲过了
图像识别特点当中的第二个特点
第啊第三个特点了
就是下采样图像不会改变图像的目标
图像的类别
通过下采样呢
我们可以降低图像的计算量
减少特征冗余
我们还是看这一只胖猫
这只胖猫呀
胖猫呀
512x51的图像
我们下采样到了32x30
图像分辨率下降很多
但是不会去改变它是一只胖猫对吧
所以我们要利用这个特点
要在神经网络设计过程当中设计一些结构
要设计一些操作
设计一些结构
这个操作呢就是磁化层当中的磁化操作
下面我们看磁化操作它具体是怎样操作的
磁化它是用一个像素来代表来表示一块区域的像素值
它用来降低图像的分辨率
要理解词的话
它的核心就主要来理解像一个像素以及一块区域就可以了
一块区域的像素值怎样被一个像素代替呢
在磁化里面主要有两种方法
主要有两种方法
第一种呢是max pro取最大值来代替
另外一种呢是取平均值来代替的平均磁化
另外一种叫最大磁化
我们来看右边这个示意图
这示意图呢就给我们展示了最大磁化的操作过程
以及平均磁化操作过程
我们先看最大磁化
我们首先在输入图像当中选一块区域
然后再去看这一块区域当中
我们用哪一个像素来表示它
我们这里呢很好的用颜色给我们区分开了
选了一个2x2的区域
我们先看绿色的区域
选了这四个像素值
然后我们看经过石化之后呢
这个绿色这个2x2的区域
他用100来代替了
这100呢就是在这个2x2区域当中的最大的像素值
这就是max ring
同样的我们可以看到右边这个粉色的粉红色的2x2区域
最大项目是哪
是184
我们看一下下面是不是也是184
同样的橙色的橙色的很好看的都是12
那么它就12了
然后我们看蓝色的最大是45
所以最大这个最大石化之后
这个区域用45来代替这一个2x2的区域
这就是磁化一个过程
下面我们再来看平均磁化
平均磁化字面意思我们就可以知道了
这一块区域当中的像素值的平均值来代替它
这里呢就不计算了
大家可以算一下
口算一下这四个值相加除以四就是36了
我们也可以看一下最简单的这一个
四个都是12的
我们口算一下
他平均只是说闭着眼睛就知道它是12了
因为四个字都是一样的
这就是最大值和平均磁化它的像素值的操作的一个过程
磁化这个名词一开始也很难理解
这里给大家画一个图来帮助大家理解一下ping为什么叫ping
为什么叫磁化
我们此话怎么去理解它
我们可以想一下
大家有没有打过篮球
在篮球操场上
如果哪一天下了雨之后
我们在操场上可能有一块区域
它会有一个积
就是有积水
这个积水的过程它就是一个磁化
这一个积水呢我们就可以理解为它是一个池子
一个词说词
而这一个积水它怎么来的呢
就是他把周围的这些水啊
这个雨水都给吸引过来了
都可以收集过来了
就是叫做磁化
它形成了一个水池
这就是磁化的一个过程
呃这里可能要画大一点吧
就是这样
他把周围的水池周围的雨水都收集过来
形成了一个水池
这就是一个磁化的过程
对应到我们像素当中
我们看一下就是一块一块区域
你给我收集成一个一个小的区域
这就是一个磁化的一个过程
这是磁化的操作的过程
下面我们还有一个重点就是来关注它的尺寸的一个变化
它的尺寸如何去计算呢
它的尺寸其实计算与卷积操作是类似的
完全类似的
在这里要给大家讲一下这个ko这个k的这一课呢呢
其其实就对应到我们卷这个词汇当中的窗口的概念
然后别的计算和我们卷积计算都是一样的
输入的大小减去ko加上两倍
padding除以strike的加一就可以得到我们输出了
这里就不带着大家去计算了
那么我们通过这一个尺寸的计算公式
我们发现磁化成它与卷积操作有很类似的地方
我们在磁化层当中呢
它是没有可学习参数的
它直接利用这些像素值就可以算出我们输出的这个值了
最后关于词汇的卷积再给大家讲一点
就是在现在的一些神经网络模型当中
就很多就不怎么用磁化了
就是因为磁化它就是降低图像分辨率
很多时候就用卷积当中
我们卷积的stride使用一个不长等于二的卷积来代替磁化
来降低图像分辨率
我们知道前面看到stride卷积
如果我们的sat等于二的话
我们图像分辨率是可以下降一半的
而通常我们此化它的分辨率也是下降一半
所以现在很多模型当中
石化可能不怎么用
用的话也只用一次或者是两次
而更多的呢会利用卷积当中设置ride等于二来代替磁化的一个功能
从这里我们也可以从另外一个角度去看磁化
其实磁化它可以理解为一种特殊的卷积
大家要理解怎么理解这个词化
它是一种特殊的卷积呢
这里给大家讲一下
比如我们来看一下最大磁化
那么最大磁化我们可以理解为它的ko是2x2的
那么kl里面的值是什么呢
看到里面值我们就可以设置为不是最大值所在的那个像素
它的权重是零
最大值所在的那个像素
它权重是一
那么我们用这一个可能这个可能去卷积绿色的这个位置输出
它就是100
然后我们在红色这个地方一样的这ko这里是零
这里是零
这里是零
这里是零
用这个可能去卷积这个粉色的
它的输出也是184
通过这个过程
大家有没有发现磁化它可以是一个特殊的卷积
下面我再来看平均磁化就更简单了
平均磁化它的可能是什么呀
就是14/4分之一
1/4
1/4
就是你ko的有几个像素
它就是几分之几
然后用这些ko去卷积
去卷积每一个部分
它就可以得到相应的这一个磁化的输出的像素值
所以我们也可以从另外一个角度看磁化层
它是一种特殊的卷积层
这也可以帮助我们去理解为什么现在一些卷积层网络中看不到磁化了
下面再给大家讲一下磁化层的三个重要作用
第一个是缓解卷积层对位置的过度敏感
第二个是减少冗余
第三个是降低图像分辨率
从而减少参数量
这个二和三应该很好理解
我们通过图像识别的特点的
第三个
我们降低减少了这只肥猫的分辨率
它还是一只肥猫
因此它是减少冗余的一个过程
其次我们降低分辨率
输的数据少了
自然它则需要的参数量也就少了
这二和三好理解
那么第一个他能容忍一些过度敏感
这一个怎么去理解呢
下面我们通过一个简单的例子来理解
第一个缓解卷积层对位置的过度敏感
这个例子我们是输入一个图像
然后利用一个一层11x2的一个卷积核进行卷积
然后呢用一个ko啊
这里没写哦
ko我们用一个2x2的ko
我们这样写应该很清楚了
2x2的一个ko以ko去磁化它
然后我们来看一下这些数据的一个变化情况
我们先看原始的卷积池化之后
然后再看我们对原始图像进行一个扰动
再来看卷积池化之后
这个特征图发生了怎样的一个改变呢
我们原始图像很好理解了
它是一个只有只有只有只有这一这一列它是零
然后呢我们通过这个1x2的卷积
就是左边减去右边
所以它的输出特征图呢
我们看到第一行可能是零的
因为1-1
第二行也是零的
1-1
第三行呢就是1-0
所以是一第四行
第四列啊
刚刚前面都说错了
全都列
第四列呢它是0~1就是-1
后面的一样的就是全力
然后得到这是卷积的output的特征图
接下来我们要用一个2x2的一个ko去磁化
它这样磁化我们用的是最大磁化
最大磁化max prin
做了石化之后
我们就得到了下面这一个这一个特征图了
好下面我们来看一下
我们对原始图像进行稍稍的一个扰动之后
我们再经过卷积磁化之后
它的特图有什么发生什么变化呢
我们的改变呢就是在这个位置
重点在这个位置
把零的这一列呢加了一个一
然后我们卷积之后发现这一个相应的位置它就被发生改变了
也就是我们的卷积直接反映了你输入特征图
稍稍的微小的改变
也会在我们卷积输出之后得到响应
也就是我们只改了一个像素
我卷积之后也会发生这一改变
接下来我们再来看神奇的地方来了
就是我们再次采用这一个2x2的max pro之后
我们的特征图仍然我们的特征图能容忍这一个细微的改变
我们看到左边和右边的这个特征图都是一样的
都是一个这个两列都是一
这个位置就被我们忽略掉了
这一个细微的敏感性被卷积
这一个卷积层的敏感性被磁化给缓解掉了
这就是磁化当中可能缓解卷积层对位置的过度敏感的一个功能
好以上就是对卷积网络当中两个重要的操作
卷积操作以及磁化操作的详细讲解了
下面给大家介绍第一个大规模商用的卷积神经网络
dnf它的一个网络结构
让大家知道一个简单的一个基础的卷积神经网络
它大概是怎样去设计的
下面这个示意图呢
就是给fight在一起8年乐观所提出来的这篇论文当中所提出来的网
卷积神经网络结构示意图
这里呢给大家介绍了每一个网络层
我们一一来看一下它有哪些网络层
第一个是c层
这个c层呢这个convolution就是做卷积
就是对原始的输入图像
它的输入图像是32x32的一个灰度图
这个卷积呢它的坑呢我们来看一下蝌蚪是一个4d的张亮
这个我们在多通道卷积里
大家应该知道我们看到为什么ccd的呢
这里是一通道
这个一表示输入通道是多大
5x5呢是我们卷积核的大小
六呢代表的是卷积核
有几个卷积核
我们通过前面的知识点讲解
我们就知道了
我们的输出应该是一个六通道的
因为我们有六个卷积核
一个卷积只能输出一个通道
所以我们有六个通道的输出特征图
那我们的输出值大小是多少呢
是28x28
这里我就不给大家去计算了
我们可以用前面的公式代入进来
就可以得到这一个关系
再往下就是一个stride
再往下就是s2 层
这个s呢在当时刚提出这个时候呢
我们的pulling它还不叫pulling
它叫下采样
super sampling
它叫下采样的一个过程
所以这里用s
不过到后来呢我们都一般喜欢用p就是ping pl p2 或是p4 
大概是这个意思
然后在s2 层是最大磁化
它的磁化窗口呢是2x2
这里重点来看stride
它是一个二
这摔的呢就明显的去改变了特征图的尺寸的一个变化
从28x28变到10x14
再往下呢就是c3 肯定于二个卷积
第二个剪辑我们看一下它的ko还是是低当量输入
由于有六通道
所以他这76它有几个卷积核呢
有16个卷积核
因此它的输出有16个16个通道
然后是10x10的
再往下还是一个磁化
在这里我们就发现它是堆叠的去使用了卷积磁化
卷积磁化这个思想呢
就是猫的视觉系统当中所实验当中的一些重要结论了
生物的视觉系统它也是抽象的
从低级到高级
他反复的去抽象
而在举行神我们的设计过程当中
它也是逐层的去抽象
不断的使用卷积池化去提取我们的特征
好经过两次卷积做大之后呢
就来到比较重要的
我们要在这里砍一刀了
通常我们卷积神仙也会在这里砍一刀
一分为二
左边啊
我们写到上面来吧
左边称之为features
也就是我们卷积的话输出最后一个输出特征图
我们会称之为它是特征
而后面的呢是称之为classifile
classify分类器
也就是后面我们先来看后面它是什么
后面它就是三个fc层
是这个叫概率的输出
对概率的输出也是这个图片它分类概率的输出
而后面这一层呢也就是我们前面所学到的多层感知机的概念了
这我们通常会认为后面的全连接层
它是一个分类器
它接收特征
然后输出分类
而前面卷积磁化部分我们会称之为features
在这里在pytorch代码实验过程当中呢
我们的modules里面我们定义module
定义model的时候
我们通常会用这两个这两个概念
这两个名词大家在这里要知道
到时候我们会定一个self的features
然后这个features是一系列model构成的特征提取器
然后我们还定义一个self class fire
这个class fire呢可能就是一系列的全年基层构成的一个module
构成的分类器了
好这就是卷积神经网络基本的一个结构
堆叠的使用卷积池化
然后后面会接一些进行输出分类
重点要关注两个概念
我们堆叠使用卷积式化
我们可以理解为它是特征提取器
而后面的c呢我们可以理解它分类器
这就是卷积神经网络的一个基本的结构
了解了卷积神经网络基本结构
下面给大家简单介绍一下卷积神经网络发展的历史
这一个呢我们在cp paper的这一个
我们在cv baseline的paper里面也会详细给大家讲解每一篇论文
这里呢大家知道一下它发展的历史就可以了
第一最早呢我们一定要追溯到1980年福岛邦彦提出的新政之基
第一个卷积神经网络的初心
他堆叠的使用了sc细胞
s细胞和c细胞这一个层级结构
第二就是刚刚我们已经给大家详细讲到了lina fi乐坤所提出的li fi
第一个大规模商用的卷积神经网络
使用在了美国的邮政系统当中的手写邮政编码的识别
第三就是第一个技惊四座
震惊四方的卷积神经网络net
它已超出第二名10.9个百分点的巨大差距
拉开第二名的巨大差距
夺得了2012年的i r s v r c的挑战赛的冠军
并且它拉开了卷积神经网络统治图像任务
图像领域任务的序幕
所以这一个可以是一个巨大的里程碑
大家都知道这是一个巨大的里程碑
要重点关注2012年
再往下呢就是一些在i l s v r c挑战赛当中夺冠
或者是取得了优异成绩的模型
并且是被并且被广泛应用的模型
这里大家也需要去认识一下第一个啊
这里应该说叫第四个
他是google net
是由谷歌提出来的
所以google net呢提出了一种inception的结构
也就是分叉的结构
我们看到它一层当中有很多个分支
这里在命名上也很有特色
给大家提一下
这里的l是大写的
为什么呢
这是由于他们要致敬linu
我们看到中间就是一个linut的名字了啊
致敬乐坤所提出linu
然后再往下第五个就是vg g v g g
它的网络结构是非常清晰简洁的
它将层级结构发挥到极致
我们看一下这个结构非常清晰
就是使用了123455个卷积池化的一个结构
然后图像分辨率呢也就通过磁化下降了五次
每一次下降一半
也就下降了二的五次方式下降了32倍
所以最终输出的它是一个7x7的
我们输入是224x24
这就是绝v gg的网络结构
虽然他没有夺冠
但是vgg这一网络结构呢广泛地使用在各类的图像任务当中
作为特征提取器
再往下第六个也可以认为是另外一个里程碑意义的网络结构
rene rent所提出的残差结构
使得神经网络可以达到了上千层
从而提高了卷积神经网络分类的能力
renet结构或者是思想呢已经广泛应用在现在各种最优的网络结构当中
再往下第七个也是一个非常经典的卷积层方法结构
dance night
dance night呢是基于restant思想上
把rest night这个残差的思想啊发挥到极致
也就是稠密的连接
他提出了一个module
在这个module当中
每一层的特殊都与后面的特图进行连接
这叫dance net稠密连接的一个概念
我们看到这里很多细的弦很稠密
这就稠密连接dance net概念
再往下最后一个就是ios b r c最后一届挑战赛当中的冠军的模型
s1 n s net
他提出了一个s e module
利用这一个s e major
它可以提高卷积性矛盾的分类的能力
这就是先进化的一个简史
简单的给大家介绍了一下最早的cn雏形
到现代深度的卷积神经网络模型的一个发展
好到这里
我们这节课的主要知识就讲完了
下面我们要对这节课的知识点进行一个简单的回顾
我们这节课主要讲了五个部分
首先介绍了猫的视觉皮层的实验
从中我们得到了两个重要的概念
叫做具有层级的结构
视觉具有层级的结构
其次细胞有局部感知的特点
还有一个感受野的概念
第二就是发展史上的三个第一
第一个行人出行新政之基
第一个大规模商用的简称用了linuf
第一个你既经是做的卷心
完了as net再给大家介绍卷积神经网络的发展史之后
就给大家详细介绍我们卷积神经网络当中的卷积操作该如何操作
在卷积操作当中
我们知道有三个主要的概念
第一个是卷积核的概念
第二个是heading
我们填充的概念
还有ride不长的概念
在卷积过程当中
我们还有单通道和多通道的卷积
在多通道的卷积过程当中呢
有一个非常重要的地
有一个值得去注意的地方
就是我们多通道的卷积
我们的卷积和一个卷积
它是一个3d的增量
我们一定要注意3d和张亮的卷积和它并不是3d卷积
它其实是2d卷积
什么时候它是3d的卷积呢
我们要看我们的卷积核在几个维度上进行滑动
它就是几维的卷积了
这是关于卷积操作知识点
再往下第四部分就给大家介绍了磁化操作
在磁化操作当中给大家讲解两种常见的磁化方法
一个是最大化
另外一种是平均磁化
此话呢我们也可以从另外一个角度看
它
它就是一种特殊的卷积操作
为什么呢
我们在第四部分也已经给大家讲解了
并且我们给大家讲解词话当中的这个ping怎么去理解这burning呢
这个英文翻译我们可以从一个下雨
我们球场上下雨之后
我们的雨水呢会集中到收集到一个很小的一个水池当中
这就是一个磁化的一个过程
最后还有第五个重要的知识点就是图像识别当中有三个特点
分别是特征
具有局部性特征
很出现在任何位置
同时下载量过程当中
我们这一个胖猫
不管你54512x512的
还是下采样到32x3色的
它还是一只胖猫
它不会改变我们图像的目标
这是图像识别过程当中的三个特点
123这三个特点呢
理解这三个特点呢有助于我们理解卷积网络当中的磁化和卷积操作
以上就是我们这节课所讲的知识点知识的内容了
好到这里
我们中篇关于卷积神经网络的就介绍完了
我们在下节课内容当中就会介绍确认任务的首选网络
循环神经网络
这节课的全部内容就到此结束
感谢大家收听
下一课再见
