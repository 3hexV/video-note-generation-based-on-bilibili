各位同学大家好
欢迎回来
我们接着往下讲
第三部分
循环整体网络在第一第二部分就给他铺垫了序列数据
以及在语言模型当中会出现的一种随着时间不等
人家计算量呈指数的增长
这一个问题
为了针对序列数据解决这一个问题
就学着设计出来了循环神经网络
这个神奇的网络来解决以上存在的问题
在循环神经网络当中
它可以反复地使用我们的权重矩阵
就可以避免了我们时间部的增加带来的参数的激增
这是他一大特性
除了循环的去使用权重矩阵
它还引入了叫做hidden state隐藏状态的这一个概念
hidden st很重要
这一个hidden sth呢它用于记录历史的信息
为什么他要记入历史的信息呢
这就是与序列数据它最大的特性有关
因为前后的数据它有一定的关联性
比如说前面我们提到的一个例子
就是cast和fifteen之间的一个联系
因为是猫
所以它是15个小时
这就是其历史信息的重要性
而在循环神经网络当中呢
就引入了一种hidden stay隐藏状态的概念
用于记录历史信息
下面我们就来看一个2n的示意图
2n示意图呢我们通常有两种描述的方式
在这里我也找了一个图给大家看了一下两种方式
左边这一种呢是把它给重叠起来的方式
右边呢是展开的方式
左边这一种重叠的方式呢不太好理解
就是刚入门的时候
我刚学的时候看到这一个左边这种左边这种形式很难看清楚
到底安它是如何工作的
这个循环又是如何实现的
这个是比较难理解
但是难理解的我们还是要把它讲一下
首先这个x是输入
这个o是输出
而中间呢有一些权重矩阵
u以及w还有v这三个矩阵
它作用呢与我们前面第一课时所学到的多层感知机当中
的作用是一样的
直接进行矩阵相乘就可以得到输出的数据
下一个数据
而在这里呢引入的h呢叫叫做隐藏状态
而在这里与多层感知机不同的是
我们前向传播过程当中
在中间还有个hidden st
也就是说隐藏状态
这隐藏状态它不仅接收当前时间部的输入数据x
同时他还会考虑上一个时间不给他传回来的这个h
我们从这一个重叠的这个c图是看不出来这个概念的
就是上一个时间不有个t减一的概念
这里看不到的
这里我们只能看出这一个箭头h的计算呢它也来源于两个数据
有两个数据
这是一个
这是第二个
然后再进行计算
得到当前时间过的h
然后再把h乘以及全职举重w再输出得到o
这就是左边这个重叠的示意图
在这里我们要强调一下
这有三个权重矩阵u v w
他们三个呢是循环去使用的
也就是每一个时间过呢
它都用u b w这三个矩阵
因此这一个网络称为循环神经网络
下面我们来看具体它是如何工作的
我们把它展开
就能知道一个r n如何去处理一个数据
我们知道rn它是针对序列数据而生的而诞生的
所以它有一个非常重要的概念
叫做时间不也就是时间的概念
我们看到这里
这里中间这个是时间不t就是在时间不踢第t个时间步的时候
它的计算的一个过程
然后左边呢就是它上一个时间不叫t减一
右边的是下一个时间不t加一
我们从第t个时间步看一下
时间不t我们看它是如何计算得到它的第t个时间步的输出
o的ot的
我们倒着看就比较好理解
我们看一下ot它是怎么样的来的呢
o t它要通过乘以一个权重矩阵是什么呢
是ht乘以这个权重矩阵
然后我们看ht它又是怎么得来的呢
我们再看这个ht它怎么得来的
ht它的计算就是第t个时间步的隐藏状态
它的计算不仅仅考虑当前时间布的收入数据x他还要考虑历史信息
也就是上一个时间步的隐藏状态
hp减一
这一个ht减一呢就给当前时间不t提供了历史信息
我们知道在时间序列任务当中
历史信息是非常重要的
这就是ht的计算了
他要考虑当前时间步的输入数据
以及上一个时间步的隐藏状态
综合的去考虑
去计算得到当前时间部的隐藏状态
然后用于我们的输出
同时呢这一个隐藏呢也会向后传递
用于下一个时间不隐藏状态
它的计算
这就是在时间不t我们的ot输出是怎样得来的
下面我们还是可以举之前的一个例子
就是我们的average
average
average fifteen
好
我们看一下这里我们可以把它看成是一个文本生成的一个过程
我们在t减一时间簿呢输入第一个元素
第一个数据输入数据c输入进来之后
cast的信息呢会从ht.一向后传递
就是传递到了第t个时间步
这一个cs的信息就会通过ht减一向后传播
传播到下一个时间步
然后ht减一呢它还会用于当前t减一时间book的输出
ot这里呢我们期望的o t减一它是average
也就是我们期望输出是这一个
然后呢到了t时间不
我们的x收入呢变成了average
也是第二个单词
输入进去之后
我们期望输出是15
fifteen
也就是这个猫它每一天平均大概睡多少个小时
我们期望是15
在这里我们看一下
在t时间部的时候
我们肯定希望他要考虑一下我们前面的历史信息
我们这里反复使用的名词叫历史信息
历史信息在这里是什么呢
就是这cast就是猫
我们的猫是一个历史信息
从哪里来呢
我们前面提到了ht减一会传入到当前时间步t的计算
所以我们发现在我们时间不t想要输出15的时候
我们希望要考虑到历史的信息猫这个信息
那么这个信息是如何来的呢
就通过这里看一下
我这个光标通过这里
然后通过hd一向后传播
来到bt个时间步综合去运用这一个历史信息
然后得到我们输出15
这就是循环神经网络如何利用历史信息
如何去记录传递历史信息的这么一个过程
他就是利用了这个隐藏状态
大家应该了解了一个循环神经网络它是如何工作的
以及我们如何去看一个循环神经网络
它的结构图
下面我们要通过数值运算去对比多层感知机与循环之间
网络之间的前向传播数据的一个变化
来理解循环神经网络rn它的前沿传播是如何进行的
在这里呢我们先来回顾一下多层感知机
它的数据的根据算就是左边这个c意图
那第一节课给大家讲解了多层感知机
非常简单
我们的输入呢可以看成是一个大的x
然后他要乘以一个权重矩阵
乘以x h w x h这个矩阵之后加上一个偏置项
然后经过一个激活数就可以输出到隐藏层的数据是大的
h再往下我们还在用h乘以一个hq
一个权重矩阵再进行输出
就得到o而在这里呢我们看一下右边这个示意图
右边这个示意图就是我们把rnn网络给展开了的这个结构示意图
它的输入我们来看一下下面这两个公式
它和多层感知机非常相近
它也是有一个hidden layer
隐藏成隐藏状态这个数据
但是呢这个数据不仅仅要考虑输入数据
x也要考虑上一个时间步的隐藏状态
我们看这些公式怎样的
它是输入的x呢
要乘以一个w h w x h的权重矩阵
然后隐藏状态的计算还要通过上一个时间不定藏状态乘以一个
我把我们来写xxt吧
这里x t w x h
然后他还要考虑ht减一乘以一个w h h就是另外一个权重矩阵
考虑这两项
这两项综合学计算得到我们当前时间book的隐藏状态
这里同样的它还有一个机函数啊
就是这么一个形式
它主要多的就是这个地方
我们对比一下这个go就这个位置吧
就多了这一项
它会综合的去考虑历史的信息
这就是时间序列任务
它主要的特性好
得到了隐藏状态之后
隐藏信息之后
我们就可以输出了
这个输出也是一样的
成一个权重矩阵就ok了
在这里要给大家强调一点呢
在n网络当中隐藏状态
它的基函数我们一般用的是t h跳下去
它有一个很好的特性
大家也知道了
就是输出的值域它是零均值的
如果我们求期望的话
它是-1~1
它的均值我们可以保证它是零均
它的均值是零
并且呢我们把这个数据压缩到-1~1之间
它可以防止数值的呈指数级的一个变化
另外我们看到这个i n网络呢
它会根据我们时间不t不断的反复去乘乘以这个值
如果这一个值不把它约束到某一个区间的话
它连续相乘就会呈指数级的一个变化
可能变得更大或者可能变得更小为零
这并不是我们所希望的
因此在n网络当中
这个hien day它的基函数采用的是t h
我们不要随意去改变它是r
不要把软弱给用进来
虽然我们在卷积神经网络当中非常广泛的去使用read
但是我们要知道应用场景以及smars型函数
我们也不要用进来用到这里来
因为s型函数他的词语是指哎呀
是bank
是开区间0~1的
哦对了
这里有个tbl
这里大家注意我们ten h的值域是开区间
不是闭区间
这里要注意一下
注意一下
这里是错的
这里是错的
它是一个开区间
就是要用利用tnh把我们数值压缩到-1~1之间啊
这是我们循环神经网络当中前向传播运算过程当中的一个计划函数的
要注意的地方是天
好我们知道了循环神经网络它的前向传播是如何运算的
下面我们再来看一个具体的文本生成的例子
通过这一个例子来了解2n它的工作原理
这个文本序列我们假设是这样的
有1234566个元素
这六个字是一句歌词
大家不知道有没有同学知道他是谁的
歌词是周杰伦的周董的歌词
我们就用这一个文本序列作为样本来观察怎样实现文本生成
文本生成呢就是我每次输入一个字符
一个最基本的元素
然后以期望输出这个字
后面下一个字是什么
然后再基于下一个字再输入我们的网络
我们网络再输出后一个字
下下一个字也是第三个时间簿
第三个时间簿呢又希望说出第四个时间簿
第四个时间过的字呢又希望说出第五个时间
不就这样一就这样沿着时间步的概念一直往下走
就不断的去生成我们的文本好
那我们来看右边这个c图
一开始呢我们第一个时间不就是想这个字
这一个字输入到我们第一个时间部的循环神经网络当中去
计算这个o1 oe的
我们的标签呢是要也希望我们oe输出要这一个字
那这里大家系同学有没有发现在第一个时间步的时候没有历史清晰
怎么办呀
就是他没有h0 的这一个隐藏状态
隐藏层通常呢我们第一个时间不可以给它输入全零的
就是全零的这个数据给它输入进来就ok了
然后我们第一个时间不结束之后
我们来看第二个时间
不第一个时间不就是要输入药这个字
这个是输入进去我们的循环
这些网络还是反复地使用我们之前的群众举证的
他是不会都会有新的权重矩阵的循环的使用
大家记住这一个循环使用就可以了
然后这里一个w h h
然后有个输出是w h q啊
这三个矩阵循环就是用反复的使用
那第二个时间簿呢
我们的药输入进来乘以w x h
再加上h一乘以w h h
再加上一个偏置
再经过一个t h的基函数
就可以得到h2 h有两个作用
首先是当前时间步与w hq乘以这个权重矩阵w x q
w h q就可以得到我们的输出o2 就是希望它的标签是有
这是第一个作用
然后hr还有一个另外一个作用
就是传递给下一个时间
不下一个时间步来到了三时间簿
三sm 3呢同样的还是输入这么一个一个字
然后也是乘以w x h
然后这一个h2 呢还是继续乘以w h h
大家注意
这里还是继续使用w h h
然后输出呢他还是使用w h q
还是这三个
还是这三个矩阵
然后接着往下的第四时间簿
第五时间簿就不给大家详细展开了
从这里我们可以知道rn它有两个很重要的特性
就是第一他利用了隐藏状态来记录来捕获历史信息
怎么理解呢
比如说我们来看这一个第五个时间步
我们第五个时间不
我们希望我们的模型它要输出g这个字
但是g这一个字它的输出我们可能要考虑它当前时间输入的是深
输入是深
然后下一个字是g它的概率有多大呢
可能不太好算
那么我们可以综合去考虑生之前的一个字
也就是再往前去考虑它的历史信息
当出现直升
那么我们出现直升
再出现g它的概率就很高了
更容易的去预测到我们出现直升的时候
我们要给它输出一个g
而这个值它的信息是通过隐藏状态这样传递到第五个时间过的
我们再来看这里
这个值的信息呢是通过这个隐藏状态传到第五个时间
所以我们的第五个时间部
虽然说我们输入的是生
但是我们还可以有隐藏状态告诉我们哦
之前有个值
我有直升
那下面一个是什么呢
直升机
所以这样就可以预测出来g了
这就是隐藏状态工作的一个原理
下面我们再来看rm的第一个重要的特性
也就是它的参数的数量不会随着时间步的增加而增长
我们从这两个例子
这个例子当中也看到了
我们的权重矩阵呢都是反复的使用
循环的使用都是这三个
都是这三个w h h是计算隐藏状态的
我们在公司当中也可以看到
这是一个这是两个
这是三个
它不会随着时间t的改变而改变的
我们看到这个下标都没有t就是它不会随时间t而改变的
我们这三个群众取证是反复去使用的啊
这就是rn它的两个主要特性
以及它的前向传播的一个过程
了解了i n的前向传播
下面非常有必要的介绍r n它的反向传播
r n的反向传播有一个很高级很神奇的名字
叫做通过时间反向传播
或者叫做穿越时间反向传播
大家是否能感觉到一种时光倒流的感觉
既然加上时间
它的反向传播就相对于我们前面所提到的多层感知机更复杂一些
更复杂一些
所以接下来几分钟可能大家需要动动脑子了
我们这里有大量的公式
不过大家不用担心
我们的反向传播
可以通过计算图很清晰地帮我们理清楚梯度是如何从后向前传播的
大家是否记得计算图的概念
在计算图当中有两个主要的结构
一个是节点
一个是边
节点表示的是我们的数据
我们看一下右边这个示意图
这边这个示意图我们的节点呢都用了这个矩形框来表示
也就是它是具体的神经元的数据节点
然后另外边呢什么叫边边
它就是操作它是一系列的运算
我们在这里呢就我们的圆圈来表示
就是一系列的运算
整个计算图呢就由节点和边来构成的
我们在多层感知机也用了计算图来帮助我们去理解
去观察梯度是如何从后向前传播的
现在我们接着继续用计算轴来帮助我们理解循环神经网络的穿越
时间反向传播
首先我们还是以最简单的先找距离输出距离loss最近的那一个变量
我们先来看w q h它的梯度我们该怎么算呢
在这里也把公式给列出来了
我们发现它是一个c马下求和的一项
大家是否记得第一课时的时候
我强调了补充了一点
就是我们通过计算图去找梯度的话
就是从l出发找你要求的这一个变量
有几条通路
也就是找关系找联系
就从l出发找我有几条通路能到达w q h
有几条通路它就有几项进行相加
大家是否记得我在讲计算图册有讲过这个事情
有几条通路就有几项进行相加好
现在在这里我们就按照这个准则来找一下他的联系
首先呢我们从l出发
通过o3 
我们通过时间不o3 这里找这里
这里找到了一条通路
找到了l与w q h它的一个联系
是l通过o2 
然后这个位置嗯它有正向的箭头
所以反向它也是能通过这条路找到w q h q w q h他联系的
他还有时间不在t一的时候
他也用了w q h好
我们再通过这一个oe又找过来了
就有三项
为什么要三项呢
因为它时间不总共有123
再来这里大家看一下
这里是t一等于一
t等于二的时候
t等于三的时候
然后呢这里我们通常会把总的时间步设置为大t就是另外大t
所以我们的大t是等于三的
大家记住这一点
后面会反复使用这一个大t等于三
好到这里我就知道了
这个西格玛他是要从小t等于一开始
然后到大t了
这里大t是三
所以我们就三项每一项呢是什么
我们看这个proud这个计算符
关于计算符如果大家不熟悉的话
再回到我们第一个课时讲反向传播的时候
给大家介绍这个计算符
我们看一下计算符就是l到o的关系
然后o在于w q求一个偏导就可以了
这里呢我们看一下关于o关于wh的偏导呢
我们可以通过公式
这里我们假设这是公式一
上面是公式一
下面这个公式我们可以通过公式就可以得到了
这个关系式还是非常简单
比较容易求的
这里呢我们重点要知道它有一个西格玛
我们这里有t下
把t下好
再往下我们来再往前球
也就是从后向前再求一个数据
就是关于隐藏状态的
在这里呢为了方便我们先求时间
不最后一个时间不也就是距离l最接近的那一个h它的梯度
我们先来求h3 的梯度
这里我们还要再强调一下
前面提到了大t等于三
所以这里关于偏l偏h3 的梯度呢就是在这个公式了
我们看到这里用t来表示
我们用t来表示这个位置
看一下公式
我们是关于p2 ph大t也就是我们这一个这一项
现在我们要求这一项梯度这一项的梯度啊
这一项梯度怎么求呢
同样的我们从l出发找一下联系
我们发现只有一条通路
只有一个一条通路能找到h3 的联系
所以呢我们看到它的梯度也很很好去计算
也没有什么西格玛
就是一个pd一个相乘的关系
l到ot ot再到ht这个公式也非常简单
我们还是这样
可以通过公式二就很方便地计算出来了
l关于ht的它的梯度是什么
再往下我们来看一个相对而言比较复杂的了
就是h2 
我们现在假设我们要来求h2 的梯度
现在还要再写一下
大t等于三
现在我们思考一下
关于h2 怎么求
通过前面两个梯度求取
我们也知道我们要从l出发去找一下这些天这些通路呃
有几个通路能与hr联系上
那么它就有几项
现在我们先一项项来找找最简单的h2 的
就是在第二个时间步的时候
它的o2 肯定有一个联系了
所以这里有一条
然后呢我们发现h2 呢它还会用到第三个时间的隐藏状态的计算
所以这里也有一条通路
我们看一下
这里有一条通路好
这就是两条通路能联系到l和h2 
所以它有两项相加
在这里呢
我们来看一下左边这一个公式
这里呢我们暂时可以把这个t呢设置为二
我们可以把它想象成二
这里我就不写了
大家可以想象成二就ok了
然后我们看一下它有两项相加
哪两项呢
我们一项一项来看吧
先看第一项
第一项就是关于ht加一的
也就是h3 的这一个这一条通路的这一项
这一项呢我们又用到了梯度反向传播的一个概念了
我们只需要拿到ht加一的梯度
然后再乘以一下h偏h t加一偏h的这个就可以了
我们知道前面这一项呢
就这一项它是比较复杂的
要从后面反复去计算的
我们现在就不用再去计算l到ht加一了
因为刚刚我们求偏l偏h3 的时候
已经得到这一项
先这一项是三算了
我还是写出来吧
我感觉这样讲的话
大家会晕掉
因为这个还是非常多的
现在呢我把这个t加一写成三了哈
t加143
这个t是二
然后后面的t都是二的
t都是2t2 
然后这个t加一呢是三
这个t呢是二
就是在看公式的时候呢
有时候我们的下标比较多
比较复杂的时候可能不好理解
我们可以就是具体化
比如说t它是一个通向
那么我们把它具体成二
我们带进去来帮助我们去理解这一个通向通式的这个公式好
我们现在把这个t一都改为了t加一都改为3d就很好理解了
再回来看第一项
我们这一个h3 的梯度
我们刚刚求取得到了的
因为刚刚这一个偏l偏t我们是三的
我们在上面已经求到了关于h3 的梯度
在这里我们直接利用h3 的梯度再乘以一下就可以了
这一项就在这里
就是第一项
这应该很简单
然后我们再来看第二项
第二项就是时间不第二个时间步的时候
它的输出的这一个通路了
第二条通路这里光标这里
那么就是偏r偏o2 
偏o2 
偏h2 
这个很简单
这个也可以通过我们的公式
就是可以通过我们这个公式公式二就可以得到
就可以得到wq去了
好这样求到了h2 的t t我们没有发现
我们有没有发现h2 的梯度的计算相对于h3 而言比较复杂了
它不仅有一项有两项了
那么我们考虑一下
如果我们再往下求h1 
它的梯度呢
h一的梯度又会有多少项呢
我们来看一下
h e的梯度
同样的我们通过的一个准则
我们找通路一个个来找
首先我们可以正向传播
早就比较好理解
我们先不画箭头
首先在第一个时间步的时候
它的输出是不是有一条通路能联系到我们的l呃
箭头不要画箭头是从后向前的好
然后呢我们h我们看一下
它又应用到了我们的h2 
就是应用到了hr的计算过程中
所以h2 呢我们看一下h2 
首先看在第二个时间部
它有一个输出又联系到了这个l这样有没有玩呢
其实没有完
我们看到在h一应用到h2 的时候
h2 呢它又应用到了h3 
所以在h3 这里又有一条通路
所以现在是三条通路的啊
我换一个颜色来写吧
因为呃方便我们去区分公式当中的有三项
这里呢我换红绿蓝吧
换三种颜色给它区分
我们在h一的梯度的时候呢
它是有它是有三项的啊
我们看一下这一这一项
是红色的
然后找个绿色的是第二个时间布这条通路啊
这应该大家应该没问题吧
这三个问题
这三个通路能找到h
所以我们h的梯度会有三项
那么这里就不再去写h一的这个梯度了
我们要写成更通式
写成通项公式的形式
因为我们知道如果时间大题更多的话
这个通路会更长
所以我们不能再写这一个公式
我们要来直接看一下左边的公式是一个通项公式的形式
这个颜色好像不太对
或者浅蓝色好
我们看一下这个通项公式就会写成这个形式
我们通过h也发现他有三项
而这个三怎么得来的呢
就要通过这一个西格玛
但是我们看一下这个西格玛呢
它的i是从t开始
不是像上面的从一开始了
我们看一下从小t到大t之间有多少项
它就有多少项
下面我们就一项项把这一个公式给展开
然后来观察一下原始的这个计算图当中
我们去对比一下公式计算图当中是否是一致的
好像颜色又错了
我们首先要用红色的
为什么要红色呢
待会展开公式你们就知道了
首先我们带进去这个w h h转至
然后大t我们知道大t是三
然后第一个i呢对了
这里要写一下
我们当前大t是三
然后t等于什么呢
t是等于一的
因为我们求的是h1 
也就是第一个时间步
因此一开始这个是二
也就是一个平方向
然后再乘以w h q t
然后偏l o o什么呢
我们来算一下大t是三
小t是一
所以等于四四减去当前的i呢是当前i s1 
所以是三好
我们看一下这个公式
这一个红色的这个公式呢恰恰就对应了我们现在计算图当中
红色这一条通路
这个曲线这条线索所走的这个位置是我们看一下
因为从这个偏l偏o3 
我就知道它是这一条线
是红色的好
下面我们接着把公式展开
现在就要换另外一个颜色了
到这里大家应该猜出来
我为什么要换三种颜色去画那三条通路了啊
这个绿色好像不太好看
我们换成黄色吧
我们换套黄色
换成黄色
把中间这个换成黄色
啊这样的形式好
我们加上加上什么呢
我们接着把这个i再往下加一的话
就会变成了直接写了哈
这里就w h h这里是应该是一次方
然后乘以w q h t l
大家知道偏o什么呢
这里算一下
其实口算也知道它是偏o two
偶尔的也就是l是走到偶尔这条这条通路来求取h一的梯度
同样的我们接着可以往下就是到了第一个时间步了
终于来到第一个时间步了
这个再加再加上我们发现i呢现在已经加到三了
我们最大也是三
所以这就是最后一项
所以这一个h一呢它有三项与我们计算图当中都找到三条通路
应该是一一对应的
这个等于什么呢
这个等于w我们发现就没有了
这个w是零
因为t大t是33-3=0
所以我就不写了
不写出来了
然后呢它只剩w q q h转置
然后偏l偏o一好
到这里h一的梯度就有写出来了
它是有三项构成的
也特也与我们计算图当中所找到的那三条通路是一一对应
这里有一个非常重要的地方
大家去关注的
要重点关注的就这一项
就是通过这一项我们看一下
它是一个指数的形式
并且它的指数与时间t相关
这个t如果越大的话
这一个指数是随着t增长而变化的
在这里我们看到w h h这个权重矩阵呢是在这个位置
为什么我们求h为什么我们求h一的梯度的时候
会反复的用到w h h这个权重
这是由于我们隐藏状态不断的向后传递
传递的时候要乘以一个w h h的权重矩阵
在这里我们来看一下在这个位置
这个我再画出来吧
在这个位置大家看一下
就是由于我们的隐藏状态的反复的乘以了w h h这个权重矩阵
所以它会乘一个指数的一个关系
这一项w h h呢它会引发循环神经网络梯度消失或者爆炸
为什么这个我们留到下两页ppt再给大家详细讲解好
到这里我们就对隐藏状态的梯度都进行求取了
再往下我们再来求取距离
输出成最远
距离loss最远的那两个权重矩阵w h x w h h它的梯度是什么
我们来看一下
直接给出公式
其实这两个呢与我们的w q h有那么一点点类似的地方
就是它有很多项相加
大家要注意
我说的是很多项
我们w q h前面已经写出来了
它就只有三项相加
但是这里呢它就不止三项了
为什么呢
我们来看一下
我们假设我们来看一下
首先我们从h3 这里是否能找到一条通路啊
这也是一条通路
然后通过hr这里能找到一些通路
然后h e这里能找到一条通路
然后呢其实我们知道h一的梯度呢又会通过去计算h2 
这里又有一条通路
然后在h2 h2 又传递了h3 
h3 这里又有通路
所以这是一个循环使用的一个过程
这里就很复杂了
我们就不再找通路
我们直接来看一下公式的一个定义
刚刚我通过找通路的方式把这个问题更复杂化了
其实它有一个更简便的方式呢
就是利用反向传播的思想
我们求取w hx的梯度
只需要找到就是后一层的梯度就可以了
后一层是什么呢
后一层是隐藏成
就是h我们只需要找到ht的它的梯度就可以了
我们来看一下它这个w h x它的梯度等于什么呢
它等于还是一个西格玛求和
也是从t等于一到大t也就是有多少个时间
不就有多少项进行相加
然后呢加什么呢
我们来看一下
这就是我们反向传播技巧了
我们只用找到ht的梯度
再乘以一下
就是偏h偏w hx就可以了
这就体现了反向传播的一个过程
我们只需要拿到ht的梯度就可以了
我们不需要再去找l到w h x那么长的一个路径当中的梯度
然后我们大家知道l对于ht的偏导
就是ht的t度是非常复杂的啊
这样的话我们就可以得到等式等号最右边的一个形式是这样的
就是偏偏ht
然后乘以一个ht的转置
h t的转置是通过我们这个公式一而得来的
好到这里
我们来看一下w h x它的梯度里面用到了ht的梯度
而ht的梯度呢它是一个递归的一个形式
我们看一下上面这一个也是它的一个通式的形式
ht的t to里面用到了ht加一
我们看到用到ht加一
那么我们去求ht加1t度的时候
肯定要用到了
就是t我们要用到了呃
ht我们要用到ht加一的梯度
那么我们又要求ht加一售
肯定又要用到ht加二的梯度
我们不断求ht加一的是梯度的时候
也要用到这些公式
这就是一个递归的形式
一直往后递归到什么时候呢
我们要递归到h大t大t的时候就停止
所以我们发现这个h它的梯度求导还是比较复杂的
这是一个递归的形式
因此对于我们w h x它的梯度
如果我们把那个ht的梯度可以展开的话
它还是很多项的
这就是为什么刚刚一开始我画这个通路有没有发现很多条
但是在这里呢我们的西格玛只是从一到大t只有三项
这是由于我们利用到了梯度的反向传播
我们只需要ht的梯度就可以得到w h x的梯度了
好这就是关于rn循环神经网络
通过公式推导
大家应该能理解了
为什么这个叫做穿越时间反向传播
通过时间反向传播呢
这是因为我们的梯度从时间不t等于三
一直往后传
t等于二
还要找
再往下到t等于一
这个梯度穿越了时间的传播
所以叫做通过时间反向传播
通过前面的推导
大家应该熟悉了喜欢人王的反向传播算法
我花了那么大的功夫给大家推到这个梯度的反向传播算法
是为了帮助大家去理解循环神经网络存在的一个致命的缺点啊
就是它的梯度会随着时间t呈指数变化
呈指数变化会引发什么呢
引发梯度消失或者爆炸
我们先来看一下为什么它是呈指数变化
这就是在我们隐藏成隐藏状态的梯度h的梯度里面体现的了
这也是前面我强调了这一项
a w h h t减i这一项它是随着t时间不t呈指数变化的
那么呈指数变化
它有什么缺点呢
就是会不断的减小或者是不断增大
怎么去理解呢
这里要给大家喝鸡汤了
大家有没有喝过这样的鸡汤
就是你每天进步一点点
365天之后你就会进步很大
如果你每天退步一点点
365天之后你就会很大
就是右边这个示意图
这一个公式就是我们假设的人呢
一开始都是一
如果你每天进步一点点
就是1.02
然后进步了365天
你就可以从一变到了1000以13407.4
这个进步是非常大的
如果你每天退步一点点
看着退步很小
只有0.02的差距
但是经过365天之后又退步了
从一衰减到6x10的-4次方
从这一个例子我们就知道了
每天进步一点点是非常重要的
虽然不多
但是随着时间的累积
我们就会进步很大
好喝完了鸡汤
我们回来继续看这个问题
通这个鸡汤应该我们就知道了
为什么我们的梯度随着时间t存储的变化之后呢
会引发梯度消失或者爆炸
我们这个w h w h h这个矩阵
那么我们连续乘很多项
它肯定就是趋向于零的对吧
趋向于零
如果他是大于一的话
那么我们是不是趋向于正无穷啊
爆炸了
数值太大了
这就是梯度消失和梯度爆炸的原因所在
第三部分就给大家讲解了循环神经网络的工作原理
以及它有一个致命的缺点
就就是它的梯度会随着时间不t的变化
呈指数变化
因此引发梯度消失和爆炸这一个致命的缺点有没有方法改善呢
有的就是利用门控单元的循环神经网络
就是我们后面第四第五分会给大家详细介绍的单元的循环神经网络
当然同样的我们稍事休息回来之后
再接着往下讲
第四部分
门控循环单元g r u
