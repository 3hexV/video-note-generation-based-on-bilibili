各位同学大家好
欢迎回来
我们接着往下讲
第五部分损失函数
损失函数的概念
我们在第四部分讲反向传播的时候
也提到了
它是用来衡量用来计算模型输出与真实标签之间的一个差异
一个差距
而我们整个训练的过程的目的
目标就是要让我们的模型输出更接近我们真实的标签
所以损失函数它对于模型训练它是非常重要的
它起到了一个桥梁作用
它连接了我们模型输出与标签之间的一个关系
在这里我们给损失函数定义为这一个形式
它是一个函数
接收两个变量
一个变量呢是y hat
也就是模型的输出
另外一个y呢就是label
这一个y是label
是标签计算
这两个变量之间的一个差异值就叫做loss值
损失值
在我们体模型优化训练模型的时候
通常会听到三个概念
非常容易混淆
在这里也列出来给大家讲解一下这三者之间的关系以及差异是什么
这三者分别是损失函数
loss function
代价函数cost function以及目标函数object function
我们在阅读文章过程当中会经常碰到这三个名词
一开始接触的时候很难理解这三者到底是一样的还是不一样的
我们就经常听到优化损失函数
优化目标函数
感觉是一样的
但其实它们是不同的含义
这里我们先来看损失函数与代价函数的关系
损失函数呢它其实描述是单个样本
单样本的差异
差异值
当然我们在具体工程实现
就是代码实现的时候
我们计算的时候呢是会一个会以一个batch
一个批量的形式去计算的
这里我们损失函数呢与我们代码当中的损失函数还是有一点差异的
大家知道这一点就可以了
然后我们代价函数呢它其实是总体
它是求总体样本
就是一整个数据集
它的一个落实值的一个平均值
我们看到它还是利用到lows function求求取每一个样本呢的loss
之后进行一个西格玛求和
这里总共有n个样本
所以它还要乘一个n分之一取一个平均值
所以它是对一个样本的总体求取这个差异
来观察总体样本的一个差异值的
这就是损失函数与代价函数的一个区别
但是我们知道我们优化loss值和优化cos值其实它是一样的
那么再往下我们来看到底什么是目标函数
目标函数呢其实它有两项
主要有两项构成的
一个是co
一个是regulation tm
也叫做正则项
正则相关于正则呢
正则项我们会在第七部分给大家详细讲解这一部分内容
这里我们要知道目标函数其实它是有两项构成的
一项呢是cause
这cos也就是要让我们的模型的输出与标签更接近
也就是要让这一项最小
然后正则项是什么呢
正则项通常是用来控制我们的模型复杂度
让我们的模型不要太复杂
如果太复杂了呢
很容易产生过拟和现象
减轻它的过敏和现象
这就是目标函数的构成
它主要有两项
一项用来表示模型输出与标签之间的差异
另外一项是模型本身复杂度的一个情况
这就是这三者之间的一个差异
一个不同
我们损失函数讲强调是单个样本
而cost function呢我们强调总体目标函数
强调整个训练过程
我们整体的一个目标
既要求我们的模型输出与标签更接近
也要求我们的模型的参数不能太复杂
在一般情况下
我们讲训练模型在讲优化的时候
主要就是讲让这一个损失值loss值越小就ok了
我们一般不会去讲cost function
去优化
cost function
去优化目标函数
我们在口语的时候
我们一般就是这样优化loss就可以了
那么在公司定义的时候
肯定还是整体的目标函数去优化的
那么对于这一项的优化呢
其实它是在优化过程当中会加入这一个正则项的
我们在计算loss的时候是不带上这一项正则项的
这里我们就不详细展开了
下面就给大家介绍两种非常常见的损失函数
分别是m s e均方误差
均方误差呢主要是在回归任务当中使用
另外一个是交叉熵函数
交叉熵损失函数呢主要是在分类任务当中使用
我们先来看第一个mc均方误差损失函数m s e主要用在回归任务当中
它的计算方式非常简单
就是计算书模型的输出与标签的差值的平方的均值
这样说很绕口
我们直接来看公式
公式呢就是这个形式
我们来看一下它有什么分子
就是yi这个吧
我们看一下这个公式公式里面分子就是yi减去yi的vivip vip
其实这个p2 我们在一些标注这个谢这一个公式的时候
这些变量里面经常有个p p呢
就是predict
就是表示预测的意思
也就是我们的模型的输出
这个vip就用我们的标签y减去y p
其实y p减去y也是一样的
因为我们这里要求一个平方
求一个平方
求平方之后呢
要进行一个求和求和之后再乘再除以n除以n也就是乘以n分之一
其实就取均值
因此可以理解为做差求平方取均值
这就是m s已均方误差的函数的一个计算公式
非常简单
它在回归任务当中广泛的使用
下面我们来举一个例子来理解这一个m s e
假如我们的一个标签呢是一二
那么模型预测的是1.5
1.5
那么它的mc就是这么去计算
首先这个1-1.5
减去1.5
然后求平方
然后二再减二也减去所对应的模型的输出
这里也是1.5
就2-1.5
然后求平方
两项都计算完之后
要除以它的一个个数
总共12
就是最终得到0.25
这个0.5就表示了这个label与pd之间的一个差异值
这个mc还是非常简单理解的
第二个常见的损伤数就是交叉熵了
交叉熵损伤数它在分类任务当中经常的使用
它其实是源自于信息论的
它主要是用来衡量两个分布之间的差异
这里我们要关注一个关键词叫做分布分布的差异
我们知道分类任务我们主要就想得到一个概率分布
就是概率嘛
两个概率
两个概率分布之间的差异该如何衡量
怎么去理解这一过程呢
我们就要详细讲解交交叉熵了
这里先给大家列一下交叉熵的计算公式是这样的
hp q这1p呢就是我们真实的分布
我们表示真实的分布
也就是样本真实的分布
我们的标签它的一个分布
而q呢是我们模型所输出的这个概率分布
这个是模型的model的model的输出模型输出分布
在这里要强调一下
那么我们h p q这一个是不等于hq p的
也就是它不像我们的距离函数
我和你的距离是1米
那你和我的距离也是1米
在这里呢分布之间的差呃
分布之间的距离它是没有这种呃
对称关系的
这里大家一定要注意啊
就是模型输出要去逼近真实分布的这一个计算公式
那么计算公式是这样的
首先这个样本在真实分布下的概呃
概率乘以这一个样本在模型输出的概率
再取一个对数
然后求和求和之后有一个符号
这就是交叉熵的计算公式
那么怎么去理解这个过程呢
下面我们要详细讲解交叉商
要想深入的了解交叉商务的概念
我们要从信息熵的概念讲起
这里给大家讲一下信息熵的概念
信息熵它是信息论当中的概念
它是描述信息的不确定度
信息熵它的计算公式是这样
它是一个期望什么的期望呢
就是整个事件当中所有可能取值的信息量的期望
这一个公式就定义为这样
我们知道期望就是概率乘以它的可能取值
这里可能取值是什么呢
是它的信息量
信息量就指某一个事件
它的自信息
我们再来看自信息的概念是什么呢
自信息它的计算公式是这样的
它是描述某一个事件的信息量
比如说我们明天是否下雨这一个事件
这个信息量是比较大的
这一个事件的信息量要大于昨天下雨这个信息量
因为昨天下雨这个事件已经很确定了
所以它信息量是零
因为昨天下雨还是不下雨
我们已经知道了
所以这一个事件它的信息量是零
它是没有什么信息量的
那么明天是否下雨
这一概率是比如说是0.5
可能下雨
也有一半的可能不下雨
这一个事件它的信息量就要大于我们昨天下雨的这个事件了
这就是自信息的一个概念
那么我们要对整个试验集进行求取
这一个信息量的一个期望
就是整个信息熵了
下面我们来看右边这个示意图
这个示意图呢我就绘制了一个概率从0~1不同的概率的情况下
这一个信息熵它的取值
当我们发现当这一个事件就某一个事件它是0.5的概率的时候
它的信息熵是最大的
0.5就表示这个事件很不确定
很不稳定
我们不确定它发声还是不发生
因此它的信息熵是最大的
也就是信息的不确定度
我们看到这就可以联系到我们这个描述文字的描述了
我们从公式里知道
当我们的概率是0.5的时候
我们这一个事件这个信息它的不确定度是最大的
因此我们可以通过信息上来描述信息的不确定度
信息熵越大
这个信息越不确定
信息熵越小
这个信息越确定
我们来看最小的时候
比如说这概率值等于零的时候
这个就很确定了
因为我们知道这个事件不可能发生
了解了信息熵的概念
下面我们就可以来看一下到底交叉熵与信息熵
以及中间有一个相对上这三者之间的关系了
首先给大家介绍相对上它又称为kl散度
它是用来衡量两个分布之间的差异的
它的计算公式是这样的
dk l pq这里同样的与我们前面所提到的交叉商的pq是一样的
我们的p呢是真实的分布
一定是我们真实存在的
我们要去逼近的这一个分布
而q呢是我们模型的输出的一个分布
它计算公式我们直接来看最后面这一个等式
他也是一个期望的一个过程啊
它其实就是这么一个公式了
这就是相对熵
它是用来计算p分布和q分布之间的差异
然后交叉商我们的公司也在这里
我们给大家列出来了
然后信息熵的公式也在这里
我们上一页也详细讲了
信息熵是用来描述信息的不确定度
在这里呢这三个公式之间它是存在一定关系的
其实我们对这个相对熵进行展开的话
我们就可以得到交叉熵
它是等于信息熵加上相对熵的
在这里如果大家把相对熵的这一个p xi成进去的话
曾经在一个括号里面的话
那么就可以得到了交叉商减去信息熵了
这里就不给大家展开
反正呢他们三者之间就存在这么一个关系
交叉熵等于信息熵加上相对熵
在这里我们重点来看这个结论
就是我们在优化交叉上的时候
等价于优化相对上
为什么呢
因为我们来看一下这个等式
我们等式左边是交叉熵
右边呢是相对熵加上信息熵
信息熵它是关于p分布的一个期望
这一个值呢它是一个常数
因为我们p分布就是真实的分布
它是固定的
因此我们优化等号左边的交叉三
它就是等价于优化我们右边的相对熵
因为信息熵是一个常数
所以我们通常就用交叉熵来衡量两个分布之间的差异
这就是交叉熵与相对熵与信息熵之间的一个联系和关系
讲公式可能还是很抽象
下面我们就以一个具体的例子来讲解一个交叉熵的计算公式
我们就以一个四分类的例子吧
假设有这么一个四分类的任务是有猫有狗
有青蛙
有马这么一个四分类任务
那么我们有一个样本
它真实的标签呢是青蛙
这里是一
那么label就是我们的p分布
这里我们就可以理解为这是我们的p分布
然后我们的模型输出呢就是一个q分布
我们看一下这个模型的输出
认为是猫的概率是0.05
认为是狗的概率是0.1
认为是青蛙的概率是0.7
认为是马的概率是0.15
然后我们来看一下这个交叉熵的公式
我们算一下
首先第一项i等于一的时候
我们这里是这里应该是i i等于一
a等于二
i等于三
a等于四
我们i等于一的时候
p p x一就是在猫的时候看一下
这个概率是零
就是真实分布下猫的概率是零
然后要乘以log模型输出的分布下是0.05
然后再加上第二项
零乘以log 0.1
然后来到第三项
一乘以log 0.7
再加上零乘以log 0.15
在这里大家其实可以发现只有一项是有作用的
就是只有一项能产生直的
就是我们标签所在这一项
因为其他项呢都有一个零
因为零乘以任何数都是零
所以是没有值的
因此我们在计算交叉三的时候
主要就这一个标签所在的那一项就ok了
那一个类别
那大家在用计算器算一下这一个log 0.7的话
应该是负的0.36
然后呢前面还有一个负号
最终我们就可以得到0.36
所以我们0.7的一个概率与一之间的一个差异
它的交叉熵的差异值是0.36
这就是一个交叉熵的计算公式
在这里细线的同学应该发现了
我们这个predict它是一个概率分布的形式
但是我们的模型输出没有办法保证它是一个概率分布的形式
我们知道矩阵的乘法有时候他可能得到负数
有时候他可能是得到大于一的
但是我们的概率它不可能有负的
也不可能大于一
那么我们怎样才能使用交叉熵
那么我们怎样才能保证我们的模型的输出是一个概率的形式呢
下面我们来看一下
要想让我们的模型符合概率的形式
我们首先要总结
要知道我们的概率它要有哪些性质
有哪些特点
我们需要去满足的
这里总结两个
第一我们概率值一定是非负的
我们的概率值不可能是负数
第二我们概率之和肯定要等于一的
我们不可能所有的概率相加是大于一
那是不可能的
怎样让我们模型输出的数据能满足这两个性质呢
这里就要给大家介绍一个
非常在这里就要给大家介绍一个非常常用的激活函数
或者叫做输出的一个函数
soft max函数
这个函数呢可以说是教他上的好伙伴
好基友
可以说他们一定是成对出现的
如果没有soft max函数的话
我们没有办法保证数据是概率分布
等于是那么就没有办法用交叉商去计算这个落实值了
下面我们就来看一下soft max函数
sofm函数
它是怎样实现概率的两个性质呢
怎样将我们的数据变换到非负
并且概率之和就是一
它的操作非常简单
执行两个操作
我们来看一下
首先对于我们的输入z i
他是执行的一个指数
就是e z i
然后再除以一项所有指数之和
这一个这一项除以这一项
这样就可以实现将我们数据变换到符合概率分布的形式了
怎么能实现这一个过程的呢
我们来看一下
首先它取了一个对数
就实现了一个非负
就符合了我们概率的第一个性质
我们概率值不能是负的
我们看一下右边这个指数函数
我们知道指数函数呢它都是大于零的
它都是非负的
所以这个指数非常巧妙
就是分子这里用了一个指数
它来实现恢复
然后呢分母它除以一个指数之和来实现所有概率之和
它是唯一的
所有模型输出的每一个元素的值之和是一
这就是soft max函数的操作了
这是非常神奇
非常简单的一个操作
就可以把我们的数据变换到符合概率分布的形式
下面我们就举一个简单的例子
让大家来理解sofm函数它怎样操作的
怎样实现这个过程的
假如我们有一个模型
它输出了三
它有三个输出变量
三个神经元输出
那么每一个神经元呢第一个成员输出是二
第一二个神源输出一
第三个成员是个0.1
我们从这里知道它肯定是不符合概率的一个形式的
因为它是大于一的
然后我们把它输入到soft max函数当中
它就可以将我们的二转换到了0.7
将一转换到了0.2
将0.1转换到了0.1
这样就符合了一个概率分布的形式了
我们知道我们看一下这些求和是等于一的
它的具体操作呢其实是这样的
我们首先要求这一个指数之和
也就是我们看一下指数之和
我们现在定义为西格玛s8 s它是等于一
首先要看第一项是二
然后再加上一的
第二项是一
再加上一的0.1
求得字和之后呢
我们就可以去计算每一个神经元的输出了
第一个神经元的输出就是就是一一的平方除以s我们可以用计算器
大家可以算一下
这一个呢就等于0.7
然后第二个神经元的输出就是e的一次方除以s了
然后第三个就是一的0.1次方除以x
这样就是一个soft max函数的计算过程非常简单
它就可以将我们任意的数据都可以变换到符合概率分布的形式
上
面就给大家介绍两种常见的损失函数
其实损失函数它也非常非常多
为什么有那么多呢
这是由于没有一个函数它可以适合所有的任务
没有一个损伤是可以适合所有的任务
因为我们损失函数它的设计会涉及算法的类型不同
会考虑求导是否容易
以及我们数据当中异常值的分布等等问题来设计我们的损失函数
关于更多的损失函数
大家可以到pytorch的这个官方网站
这个文档网站当中去看一下
这里面提供了18个存参数
好像现在到19个损伤数了
当然如果大家不想去看官方文档的英文文档
也可以上这一个知乎里面查看一下这篇文章
这篇文章里面总结了pytorch当中的lost bounce以及一些代码的实现
好到这里
第五部分损失函数的知识点就讲完了
我们稍事休息
接下来回来我们接着往下讲
第六部分全职初始化
