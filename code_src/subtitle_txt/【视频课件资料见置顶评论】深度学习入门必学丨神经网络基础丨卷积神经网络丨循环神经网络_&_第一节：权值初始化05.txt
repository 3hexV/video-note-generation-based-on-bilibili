各位同学大家好
欢迎回来
我们接着往下讲
第六部分全职初始化
全职搜索化对于神经网络而言是一个非常关键的一个概念
甚至可以得到更好的模型的指标
模型的效果
我们该使用什么方法来对全职进行初始化呢
在这里给大家举一个非常简单
但是是错误的方法
叫做所有的全职都出手为零
要是所有的所有的w都是零
这一个会导致什么问题呢
它会导致模型的退化
让每一个网络层它就等价于一个神经元
这是为什么呢
这是由于如果所有的全职都初始化为零的话
这些地方都是零的话
它会使得我们一个网络层当中的所有人的神经元
它的值都是一样的
不管你怎么去训练这些神经元
比如说隐藏成这五个神经元
它所有的值永远是一样的
这就导致了我们这一个网络层会退化
比如说五个神经元它的值都是一样的
因此它是等于一个神经元
比如说一个程序员的值只不过乘以五五倍而已
这就是一个错误的方法
那么怎样的一个初始化方法是良好的初始化方法呢
通常我们会采用随机初始化法
也就是从一个概率分布当中随机的去采样
然后赋值给权重
通常我们使用比较多的是从高斯分布当中进行随机的采样
然后去复制给我们的全职
在这里我们知道高斯分布它主要有两个参数来确定它整个分布
第一个是均值
然后第二个就是标准差s对于君子而言呢
mean而言呢
我们通常呢都是零
也就是要让我们的权重它有一个对称性
负值有一半正的只有一半
然后有一个非常关键的一个变量呢就是标准差
标准差它是用来控制我们权重大小的
也就是让我们的权重不能太大
也不能太小
怎么理解这一个标准差
这一标准差又该怎么设置
怎样去影响我们权重的大小呢
这里就要给大家回顾一下三西格玛准则
在概率的书本当中
课本当中有讲三西格玛准则
他是什么意思呢
他说的是在高斯分布当中
这个数值会在mu减去30格玛到μu加三四格码当中
在这一个区间上
它的数值的分布呢就达到了99.73%
也可以理解为大部分的数据都是在这个区间上
那么我们来看一下
如果我们采用零均值0.01的标准差这样一个高斯分布
它的权重大概是一个什么样的区间呢
应该是零点去
就是负的0.03到正的0.03之间
这个值就不会很大
我们发现就可以通过这个s t v这个这个标准差来控制我们整个权重
它的一个范围
我们看左边这个示意图还有很好的帮助我们去理解这个三西格玛准则
也就是在均值中间
均值的左边有一部分数据
右边有一份数据
它是对称的
并且从均值往左边走算三个西格玛的长度
以及均值往右边走算三个西格玛的长度
在这个区间当中呢
我们数据可以说99.73%的数据都落在这里面
可以说大部分数据都在这个区间
这是由于我们不能让我们的权重太小
如果都很小的话
都接近于零
那不就相当于我们上面讲到的所有的权重都是零了嘛
不能太小
我们也不能让权重太大
因为如果权重太大
会使得我们的一些值会落入基函数当中的一些饱和区域
比如说如果我们神经网络当中使用了sigma函数
我们来看右边这个sigma函数
如果我们权重比较大的话
我们权重乘以一个输入的值
他就会得到比较大的值
这个比较大的值之后
经过我们的sigma之后
它就会落入我们以前提供的这个饱和区域
饱和区它是饱和的
在饱和区域有什么弊端
有什么缺点呢
哈那就是我们的梯度非常小
接近于零
梯度接近于零
我们就可以理解为梯度消失了
梯度消失了
就不利于我们模型的训练
让我们模型训练非常困难
因此我们要控制我们的权重不能太大
而控制我们的权重大小的关键参数就是这一个概率分布当中的标准差
那么这一个标准差到底该设置为多大比较合适的
还是多少比较合适呢
这一个有没有一个范围
或者说有没有很好的方法帮助我们来确定这个标准差
答案当然是有的
我们接着往下看
通常我们不会手动的去设置概率分布的标准差
而是采用自适应的方法
在这里给大家介绍一个叫做zio初始化的方法
这个方法是由这一篇论文
这篇论文提出的
在2010年的时候提出来的zb的方法
他就提出了这样一个分布
这分布呢我们来看一下均匀分布
它的均值是零
我们知道这一个均值是假设这个是大a
这个是大b它的均值是等于二分之a加b的
我们发现a和b其实它是一个对称的
就加了一个负号而已
所以它的均值是m它均值是等于零的
也是符合上面我们所提到的
尽量要让我们的数据保持一半是负的
一般是正的一个对称的关系
均值为零的关系
其次它的htd它的标准差呢是什么
我们来看一下
我们知道均匀分布它的标准差应该是根号12分之大
b减a的平方
在这里我们就不展开了
从这里我们知道这个标准差是由在这里面的小a和小b来控制的
在这里小a表示的是输入神经元的个数
小b呢是输出神经元的个数
因此这一个s t b它是会变的
它会根据当前网络层所连接的情况而改变
这就是一种自适应的设置随机分布当中的标准差的初始化方法
lavia初始化
大家要记住这一个zio初始化方法
除此之外呢
还有一个也是非常常用的初始化方法
叫做凯明初始化方法
这一个是在2015年提出来的一种出装方法
它也是会根据输入神经元以及输出神经元的个数来控制它的标准差的
在代码实现的过程当中
有的框架也会把凯明初始化方法称为msra方法
大家要知道这两种初始化方法其实是一样的
为什么称为m s i a呢
msi其实在2015年的时候
何凯明大神他还是在msi
也是微软亚洲研究院
微软亚研院里工作的
因此这个初始化方法有的时候也称为msi初始化方法
凯明大神呢已经跳槽到了facebook
如果想了解凯文数字化方法的话
可以去看一下这篇论文
这里就不详细展开了
以上就是全职初始化的知识点
下面我们来讲解正则化方法
正则化方法我们听这个正则化
很难去理解正则化方法它是用来干什么的
正则化在我们中文当中很少有这么一个词名词去给我们解释
正则化方法
其实它是减小方差的策略
通俗的可以理解为是减轻过敏和现象
那么什么是方差
什么是过拟合呢
下面我们先从方差的概念去讲解
方差
其实它是从误差分解当中得来的一个概念
我们要知道误差它是可以分解为偏差与方差与噪声之和的
也就是误差等于偏差加方差加噪声
下面我们来看一下什么是偏差
什么是方差
什么是噪声
偏差
它是刻画了数据扰动而造成的影响
而噪声呢它是表达了在当前任务上
泛化误差
下界关于偏差
方差
噪声的概念的理解呢
推荐大家去阅读一下周志华老师的西瓜书
这一个概念呢我也是直接从周志华老师的西瓜梳理
直接摘抄下来的
我们有没有发现这个概念其实非常难理解
下面我们就以一个直观的曲线来告诉大家
噪声方差偏差到底是什么东西
我们先来看右边这个示意图
这一个示意图呢是一个训练的准确率的曲线图
这里有两个曲线
一个蓝色的曲线是训练集
因为蓝色曲线是训练集
橙色的曲线呢是验证集的曲线的准确率曲线
在这里我们要看一下纵轴
有一点要去修改的地方
就是我们纵轴的一点
并不是在这个位置
我们说一点呢要画到上面来
大家看一下
一点要到这个位置
由于这个模型它精度太高了
所以不便于我们去绘制这样一个噪声
所以大家知道这一点就可以了
我们一点把它弄到这里来就ok了
然后我们来看一下到底什么是招生
什么是偏差
什么是方差呢
首先我们来看一个噪声
下界怎么去理解这个期望泛化误差下界其实就是不管你是什么算法
你也只能达到这一个这一个精度
这个精度
假如说我们这一条曲线是99.99%
那么99.99%与这一个一之间呢
还是有一个差距的
这个差距就叫做噪声了
就是不管你什么模型算法过来
只能达到99.99
这就是一个期望泛化误差下界
那么这一个区间
我们看到这里已经列了这个一到99.99%之间呢
就是这个噪声
再往下我们来看什么是偏差
偏差
也就是我们在训练集当中
我们看一下这一个区间
这个区间这个区间就是我们的偏差
bios好这一边呢就是我们训练集到达期望泛化误差
下界这个区间它的一个差距就叫做偏差
再往下我们来看什么是方差
方差它刻画的是数据扰动带来的影响
这一方法呢就是在这一个区间了
这个区间其实就是验证集与训练集之间的一个差异
就叫做方差
我们也可以这样去理解
当训练集的时候达到这一个这一个精度
你换了一个数据集
你数据扰动了
你数据变化了
你只能达到这个精度
那么这个精度之间的一个差异就叫做方差
我们知道了方差的概念之后
我们再回过头来看一下我们的政治化方法
regulation
它是来解决这个问题的
解决方差过大的问题
它要减小我们的方差
减轻过敏和现象
那么什么是过拟合现象呢
过程和现象其实它就是方差过大
也就是它在训练集上表现良好
在测试集上表现糟糕
我们可以右边这个曲线图来理解
就是我们蓝色的曲线
它的精度非常高
比如说达到99.99%
但是我们的验证集上它只能达到80%
那么这里就相差了
这里它就相差了19.99个百分点
这一个差距就非常大
就说明我们的模型过拟合了
在虚拟机上表现特别好
在测试集上表现比较糟糕
这就是过拟合现象
而我们正则化方法呢就是要去减小这个差距
大家看一下这个箭头
我们要减小这个箭头之间的差距
让我们方差去减小
从而减轻过敏和现象
这么说可能还是比较抽象
下面我们看一个具体的例子来理解怎样是过拟合
在进行一些比赛的时候
我们一些选手也经常调侃自己的模型就是线下操作猛如虎
线上提交250
用这句话来调侃我们的模型过拟合了
也就是在线下我们训练的这些数据集上
我们的模型表现非常好
但是一提交到线上
结果一看他就250了
也就是我们过拟合现象
下面我们以右边这个示意图来理解这样的一个过拟合
右边呢是一个曲线拟合的例子
在这里有两种数据集
哪一个是训练集
是蓝色的点
一个是红色的点
是测试集
我们发现我们的模型
我们这一个曲线模型
红色这个模型它很好地拟合了每一个训练集当中的数据点
我们看到这个曲线经过了每一个蓝色的点
我们看到所有的蓝色点它都经过了
因此在训练集上
它的loss
它的loss我们可以理解为四零loss
它是等于零的
虚拟机上它都拟合了
然而这个模型我们看红色模型
它在测试集上呢
它的落实north可能就很大了
很大
为什么呢
因为这一个曲线太复杂了
它弯弯曲曲的经过了每一个蓝色的点
我们看到这里去
这里相差就很大了
因为我们红色的点与我们测试集的数据相差就比较大
并且呢他没有经过任何一个测试集的点
这不是我们所希望的
这就是一个典型的过拟合现象
那么我们希望曲线我们的模型应该是怎么样的
应该是符合整个数据的一个趋势
应该是这样的
大致一个这么一个方向的一个曲线
下面我们就来讲解采用什么方法可以解决这一个过拟合现象
减轻过拟合现象
减小方差呢
这里我们要回顾一下之前讲述函数的时候讲了三个概念
叫做损失函数
代价函数以及目标函数
这里之前就提了
我们的目标函数当中是有两项构成的
一项是cos
一项是正则项
这个正则项呢就是我们这个正则化方法当中会用到的一种约束项
它是一个约束
我们这一项呢会约束我们的模型
让我们的模型不能太复杂
比如说上面1p p t所展示的那一个曲线模型一样
它太复杂了
拟合了每一个训练集的点
所以它产生了非常严重的过拟合现象
这里我们要给它的目标函数加上一个约束项
可以使得我们的模型不会那么复杂
从而减轻过年和现象
下面我们就来看一下这一个正则项有哪一些呢
在这里给大家介绍两种非常常见的正则项
一种是l y
另外一种是l two
这个正常项表达式也非常简单
我们来看l one
l one呢就是对所有的权重的绝对值进行一个求和
这一项呢就可以放到这一个目标函数当中的regulation turn当中
这就是l one加了l one的目标函数
l two呢也非常简单
就是对所有的权重的平方进行求和
那么如果我们想要l想要用l to的话
我们就把l to加到我们的目标函数当中
这时假如说我们加的是l two
那么我们的目标函数
我们的目标不仅要让我们的cost
也就是模型的输出与标签之间的差异更小
同时也要让什么更小呢
也要让这一个更小
也就是要让我们的权重的平方之和也比较小
这一个作用有什么作用呢
我们可以来看右边这个c组
在讲解l one l two正则项的很多的文章当中呢
都会采用右边这个示意图
在这里就给大家简单讲解一下怎样去看右边这个示意图
在这里也提一下
如果对理论证明感兴趣的话
也就是花束的第七章里面有详细的理论证明的公式推导好
我们先来看左边的
左边的它其实是l one的示意图啊
右边的是l two的示意图
l two的示意图
这个示意图怎么看
我们来先看右上部分
这两部分这些彩色的一圈一圈的东西呢
它是lost的等高线
loss值的等高线
等高线
大家在学地理的时候应该知道有一个等高线的概念
就是在山坡的时候好
那么这个等号线什么意思呢
就是我们的权重
不管在这个圈上的哪一个位置
它所产生的loss值是相等的
那什么
在这里我们举个例子
比如说这个浅蓝色的
我们在这里有一个点
比如说我们的权重现在只有两个权重
我们具体化一点
假设这里是一
这里是二呃
这里是这里是这里是多少啊
假设我们这里是一
然后这里是二
好假设我们在这里有一个点
在这里有一个点
然后这里也有一个点
好
我们假设如果在这个点的时候
我们产生的loss
比如说浅蓝色的loss呢
这个等高线是0.5
那么我们这w one和wto是分别是二和一的时候
我们产生的loss值是0.5
同时我们当w one等于一
w two也等于的时候
也就是这一个点
它所产生的loss也是0.5
当在这个点的时候
它产生lost也是0.5
那么我们那么我们有那么多的解
我们该用哪一个解呢比较合适呢
如果我们单纯从loss只考虑cost function
只考虑它的代价函数
考虑它的loss值的时候
他们每一个点呢都是一样的
因为它们产生的损失都是0.5
它们没有差别
但是我们综合去考虑这一个l2 正则项的话
就是所有权重的绝对值之和要比较小
那么我们来看一下这三个点
肯定是这一个点比较合适了
肯定是左下角这个点比较合适
因为它的l y相加是二
这个是三
这个也是三
因此当我们没有加入这一个正则项的时候
我们只要cost
我们只看cost的时候呢
这一个等高线上的所有的解
它都是效果都是一样的
但是我们加入正则象征
我们来看一下会产生什么样的效果
在这里我们来看一下这一个黑色的曲线
黑色的曲线呢它就是他就是这个l one的它的等高线了
我们知道在这个曲线我们其实有很多个这样的矩形框
就在这个矩形框上
每一个点它的w one加wto的绝对值之和是相等的
这就是一个等高线
就是我们这个曲线就是w one绝对值加上w to的值等于一个
比如说我们等一个r
就是比如说这里
比如这里是0.8
那我们这里r就等于0.8
就是黑色这条线就是这样的
这个在学中学阶段应该都知道了我们的曲线
那么这一个黑色的曲线就是我们l2 正则项的一个等高线
那么我们要综合的去考虑loss值以及我们的l one这一项
不仅要让loss值比较小
也要让l one比较小
这时候我们发现在红色这条曲线的时候
假设我们假设红色这一个loss值等高线呢是0.1
那么我们看一下
我们在0.1这个等高线上有很多的解
比如说这有个点
这里有个点
在这些不同的点上
它所产生的loss值是一样的
都是0.1好
这次我们考虑了cost
同时我们的不函数还有另外一项叫做l one 20000
那么我们在这一个红色曲线上哪一个点
它的w one的绝对值加上w two的绝值是最小的呢
那么肯定是要与这一个矩形框相切的那一个点
通常呢我们的解都是相交在这个这个叫什么这个坐标轴上的
所以我们看一下这个黑色的点
这个黑色的点呢就是当我们loss值在红色这个线的时候
我们落实等于0.1的时候
在黑色这一个点
它的解是最优的
因为它在这一堆解这一堆点上
他的w玩家wto是最小的
在这里我们看一下
当我们的解发生在坐标轴上
会产生一个神奇的效果
那就是权重的一个系数性
我们发现当我们的解在这一个w to这个轴上的时候呢
我们w one它的值就是零了
我们看wy的值是零
wy的值等于零
也就相当于我们这一个权重给消失掉了
当我们的w one权重等于零
也就相当于我们这一个w就被消失掉了
就提供了一个稀疏的一个作用
这就是l one的一个作用
下面我们来看一下l two l two呢同样的这一个彩色的还是一个等高线
我们直接来看
假设这个红色曲线它落实值是0.1的时候
那么我们有很多点很多点
我们哪一个点哪一个几是比较合适的呢
是最好的呢
我们来看
那这里我们还是要看这一个loss与我们正则向这个等高线它的交点
它的相切点在哪里
这个圆圈呢就是我们l two的一个等高线了
我们可以写一下这l two等高线也很好理解
我们等高线它就是一个圆
为什么呢
我们可以写一下这个圆的表达式
其实就是wy的平方加上w two的平方
这一个就是我们l two等折项
那么它相加要等一个值
我们可以写为r平方
这个r呢就是我们的半径了
就是我们的半径
这就是l two正直向它一个等高线
从这个示意图我们可以看到
在红色loss值为0.1的这个等高线上面
最优的解呢应该是这个黑色的点
也就是他与这一个l two等高线相切的这个点是最优的值
因为在这一点上它是最小的
这个l two
l two正则项它有什么作用呢
它有一个非常大的作用叫做全职衰减
也就是它会收缩我们的权重
全职不会让我们的权值太大
为什么它有全职衰减的这样的神奇的功能呢
下面我们通过公式推导来理解l two真的像它也会称之为wait decay
我们在优化过程当中经常会看到一个名词
一个专有名词叫做维k全职衰减
这个全职衰减呢其实它就是l two正则项
也就是我们在目标函数当中加了一个l two等则
向
下面我们来看一下为什么加了l to等等项
它就有全职权重衰减的功能
下面我们来看一下目标函数
目标函数呢我们知道有两项
首先第一项是loss
第二项就是我们的正则项
这就是加了l to的目标函数
我们先来看一下
如果没有正则项的时候
我们的梯度下降法来更新我们的权重
它的表达式是怎么样的
方便我们公司的一个观察
这里我们看一下梯度下降
就是用我们的权重减去梯度的方向
当我们没有加这个正则项售
那么我们的梯度只有偏lost偏w i就是我们的这个梯度呢就是这一项
我们来看一下
只有这一项
如果我们加了正则项之后
我们来看一下我们的目标方程
加了一个
这个真的像它对于w的偏导是怎么样的
它就有两项了
我们通过这一个我们把这叫做公式一
我们通过公式一我们看一下两项
第一项呢就是偏loss偏w i
然后再来一下是这个l two的求偏导
这个也非常简单
我们来求一下
就是这个w平方就等于2w
然后二呢与这一个二分之能不打当中的二给抵消掉了
这里给大家讲一下这个1/2
这一个二就是为了与这一个w的平方的求导的时候
他会约掉
所以才设置了一个二
然后这个lama呢是一个系数
用来权衡我们loss值与l2 正则项它的一个比例关系
通常呢这个拉姆达它是大于零
小于一的一个值
一个数
它是用来权衡我们在优化的时候更关注于lost还是更关注于l two正则项
那么更关注的比例是多少
就通过这一轮目达来确定了
好我们看一下第二项
它的表达式求导出来之后
就等于能打乘以w i就是这一项第二项了
这里呢我们把能够达成w i给提取出来
也就是把wi的系数给提出来
提出来与我们这个前面的大概结合
就可以得到下面这个形式
我们看一下下面这个形式
下面这个形式呢我们先看一下这个减号
后面与我们无正则项的时候去对比一下
我们这一项与无正则向的这一项它是相等的
这个地方它是相等的
那么不一样的地方是什么呢
是减号之前这一项我们没有正则项的时候是w i那么解压了
然后就正则项之后
它就变成了这一个w i要乘一个系数
这个系数是一减那么大
但上面我们讲了
那么a通常是一个比较小的数
它是大于零小于一的一个小数
那么一点都不大
它的取值范围是什么呢
它也是大于零小于一的
因此我们就知道这个w i再乘以一项一减那么大
也就是一个小于一的数
这一项它是小于w i的好
那么我们再对比一下无证则象的这一个公式当中
它本身是w i
那你加了正则项之后会变为w i乘以一减那么大
也就是适当的大量减小了
这就体现出来一个全职衰减
全职缩小的一个功能
大家也要理解这一个方框当中的这个公式
我们推倒之后就是w2 要乘以一个小于一的系数
因此它实现了一个权重缩小
全职衰减的一个功能
所以l2 正则项它也称之为rate decay
下面我们来看右边这个示意图
也是我们上面一个过拟合的现象的举例
现在我们加入了l two正则项之后
加入了wait decay之后
得到了这样一个模型
是蓝色的曲线
这个模型我们发现蓝色这个曲线模型呢就有效地减轻了过敏和现象
就相对于红色这一个没有为decade这个模型呢
它过你和现象就减轻了
它的方差减小了
我们怎么去理解的方差减小呢
我们可以看一下
给个对比
我们可以假设一下
比如说这个训练集的lost
这是测试节lost
我们来画一个表格
比如说上面这个是没有wait decade
那么我们在没有wikate这个红色的曲线
这个模型它在训练集上产生的落实值
我们前面可以说了
可以理想化
它经过了每一个的训练点
所以它落我们可以理解为零
那么我们的假设这个红色曲线在测试集这些粉色的点上
它产生了落实
假设假设是一
我们这里随随意的假设
因为区间应该是这样这个这么一个概念
然后我们看一下
在是一个蓝色曲线
我们加了vt d k
假设它的训练的loss是多少呢
我们可以理解为零点
比如说0.2
因为它产生了一定的loss嘛
因为这个这个曲线它没有很好经过每一个训练集的点
那么在接下来我们可以去看一下这个蓝色的曲线
对于这一个红色的点
它承认落子大概是多少呢
我们可以理解为大概是零点点五
为什么呢
因为这个蓝色的曲线啊
它距离蓝色的点更近一些
距离红色的点更远一些
我们可以直观上去理解
那么我们看一下它的方差是多少呢
大概去理解一下
在没有维持dk
它的方差两个相差呢是一
而我们加入了with decay之后
这一个蓝色的曲线它的方差就比较小了
是0.3
是0.3
这里呢我们采用的是loss来衡量模型的一个性能
那我们在分类任务当中可以用accuracy
我们的准确率来衡量这一个性能都可以
我们主要来观察这个差异的一个变化
就是没有wedk的时候
它的训练集和测试集上的差异比较大
当我们加入vk的时候
确定集和测试集上的差异就有所减小了
我们看从一减到0.3
这就是减小方差
减轻过你和现象
这就是正则化方法
它所要做的事情
下面再给大家介绍另外一种常用的正则化方法也是非常实用的
它是随机失活
我们中文翻译过来就叫做随机失活
非常简单
就四个字
但是四个字有两个概念
第一个叫做随机
随机的意思是它会以一定的概率去选中这些神经元
选中这些神经元之后
这些神经元会干嘛呢
会失火
也叫做失去活性
也就是它的权重变为零
我们来看右边这个示意图
我们先看下半部分
先看看下半部分
下半部分就很好地展示一个加入了就泡之后的全连接网络了
它一个变化
我们看一下
下面有标注
左边呢是我们正常的全连接的一个网络
右边呢就是使用了就怕之后的一个情况
我们来观察一个神源
比如说中间这一层这个神经元就有一定的概率被选中了
选中之后它的失去活性
它的权重都是零
所有的权重都为零
所以呢它不会与任何的神经元去连接
同样的我们来看第二个神经元
第二个神经元呢它并没有被选中
所以它还是可以与其他神经元相连接的
再往下看第三第四个神经元它也被选中了
也失去活性
它没有与任何神经元连接
这就是采用了jpa之后
这个网络它的一个连接的情况
这么做有什么好处呢
这个做的好处就是可以避免网络呢过度的去依赖某一个神经元
从而实现减轻过你和现象
下面我们可以看一下上面这个动图
这个示意图它就是在训练的时候
我们的前向传播
我们的模型可以说是是不重复的
因为这个随机的组合还是非常多的
我们现在假设每个神经元呢有0.5的概率存在
或者是0.5的概率
它是不存在的
我们看到绿色的呢就是保留下来的那些神经元
我们看到每一次前向传播呢
都是随机的去组合我们的网络模型的一个结构
这就是主炮带来的好处
减轻过拟合
在采用代码来实现就泡功能的时候
有一个需要注意的小细节
那就是在训练和测试两个阶段
我们下面来看一下
还是右边这个示意图
我们在训练的时候呢
神经元它是会有一定的概率去选择
会被失去活性
也就是消失
而我们在正常测试阶段就是会恢复变成正常的神经网络
我们可以理解为左边这个是测试阶段
所有的升级源它都会使用
而右边呢是训练阶段
训练阶段呢就会以一定的概率去选中一些神经元
去让它失去活性
我们在前面讲解多层感知机的时候
我们知道前向传播的计算公式呢是这样的
我们可以假设现在我们还是就算一个神经元
一个神经元呢它是根据输入的省略的个数乘以对应的全职
这个i呢我们假设从一开始
现在我们假设你想画一些
假设它有100个神经元
现在我们也假设假设是100
也就是说有100个100项相加
那么我们在训练的时候呢会发生什么变化
会发生有一定的概率去选中神经元去失火
也就是有一定的权重丢失了
我们这个概率假设是0.5的话
我们看一下
我们看一下
那么我们这一项还还剩多少项呢
如果是正常的时候
我们有100项
那么我们加上随机失火
如果我们用概率为0.5的话
我们大约呢就是50个对吧
要100x零点
我们取一个期望
我们现在理想化一点就是50个
本来他有100项进行相加
它的值是100
那么现在我们经过了一个0.5的概率
把一半的成员给丢掉了
大约也就是50
这样我们理想化一些就是50
而这个变化的关系呢
其实就是由于这个神经元的丢失而造成的
其实这个0.5呢就是我们的概率
那么我们下面来看一下
下面这个就对应到我们训练阶段
训练阶段它会有一定概率丢失我们神经元
而上面呢就对应到我们测试阶段
因为我们测试阶段所有的神经元都会存在
那么我们想象一下
在我们训练的时候
当你在测试阶段的时候
会导致我们的模型的下降
所以呢我们要对在测试阶段
这个神经元输出值要乘以一个概率p
这个p呢我们在这里举的例子是0.5
也就是最终会变为50
在这里呢大家不要去关注于这个数据值
我们只是举了一个例子
这里50肯定不会是等于50的
除了l one l two以及旧炮阵子化方法呢
还有一些比较高级的正规化方法
这里就不一一详细的给大家展开
在这里就简单的给大家介绍几种高级的自动化方法
第一个是bn batch normalization
这个可以说是现在神经网络当中的一个标配
这个方法呢我们将会在cb的base的论文当中会讲解这篇论文
所以如果对bn感兴趣的话
可以先自己去阅读一下这篇论文
或者直接到我们的cv的bass line当中去这一节课
除此之外
基于bn会针对不同的应用场景而进行改进
就是bn的一系列的改进
就有l n它叫做layer normalization
还有instance normalization
还有group normalization
它都是基于best normalization
针对不同的应用场景下去改进的一些正则化方法
这里就不一一详细展开了
这里如果感兴趣的话
都可以去看一下
这里我都把论文呢给大家列出来了
好以上就是第七部分关于正则化的知识点
到这里呢我们第一个课时找片网络基础与多层感知
所有的知识点就讲完了
下面我们对这一节课的知识点进行一个简单的回顾
这一节课的知识点还是非常多的
这节课呢我们主要讲了七个部分
第一个部分给大家介绍了人工神经元与人类神经元之间的联系
我们的人工神经元是怎样从类型当中抽象而来
得到了数学模型
再往下我们介绍了神经元以不同的连接方式
就可以构造成不同的人工神经网络
在第二部分
我们就介绍了多层感知机
在多层感知机给大家讲解了
我们的数据是如何从输入层逐步的从前向后传播
进行一个前向传播
得到我们的模型的输出
在前两传播过程当中有一个非常重要的概念
就是在第三部分给大家详细讲解了
就是非线性激活函数
激活函数
这激活函数它非常重要
它就使得了我们多层感知机成为真正意义上的多层
我们知道如果没有这些非线性激活函数
是1000层还是1万层的多层感知机
它都等价于一层的感知机
为什么呢
这是由于我们举证的乘法的原因
就是你1万个矩阵相乘
一直相乘
你能始终等价于一个矩阵
那么一个矩阵它就等价于一个一层的网络
所以机构函数对多层感知机是至关重要的
再往下我们就讲解了反向传播算法
在上面第二第三部分
我们知道数据是怎样从前传播的
那第四部分我们就给大家介绍了
利用计算图来讲解来求这一个反向传播
我知道反向传播算法就是利用t我们知道反向传播它传播的是梯度
我们要利用这个梯度进行权重的更新
怎么更新呢
就要用梯度下降法沿着这个梯度的负方向更新
十我们的函数值减小
什么函数值减小呢
这个就是在第五部分讲解的损失函数了
我们损失函数的作用就是用来衡量模型的输出与标签之间的差异
我们反向传播算法
我们要优化模型
目标就是要让这个损失函数更小
也我们的模型输出呢更接近我们的标签
这样就可以得到一个比较好的模型
损失函数当中呢给大家介绍两个非常常用的参数
一个是m s e
也叫做军事方误差损参数
这一个通行在违规任务当中使用
还有一个叫做cos and rob叫做交叉
三所是函数
三层次函数就比较复杂了
它是用来衡量两个概率分布之间的差异
会在分类任务当中使用
在使用交叉的过程当中
我们如何保证我们的模型它的输出是符合概率分布的形式呢
我们就要用到交叉熵的好基友好伙伴soft max激活函数
我们知道soft mac函数呢可以将数据变换成符合概率分布的形式
也就是它的值是恢复的
同时所有值求和是等于一的
这是sofm函数的一个特性
非常好用
也是非常常见的
也是只要出现交叉商
可以说就会出现so mac函数
这里大家一定要记住这一对好基友成对出现的
再往下我们讲了第六个部分就是全职初始化
良好的全职初始化
它可以使我们的网络模型得到更好的精度以及训练的更快
这个初始化过程当中呢
我们会采用初始化方法当中呢
我们通常会采用一个概率分布
从这概率分布当中随机的去采样得到一些数据
然后复制给我们的权重
这个概率分布当中有两个非常关键的内容
一个是君子
一个是标准差
均值呢我们通常要让它等于零
就是要让我们的数据尽量的一半的是负的
一半的是正的
另外一个标准差呢
它是用来控制我们的全职的大小
我们全值不能太小
也不能太大
而我们的标准它到底要设置为多少
才能使得我们的权重在一个合理的区间呢
我们通常就会使用自适应标准差的权重初始化方法
比如常见的zia方法以及改名初始化方法
再往下我们就是这一节课的最后一部分正则化方法了
正则化方法我们是用来减轻过你和现象
减小方差的
在这里我们要介绍两种常见的l one l two正则化方法
l two正则化方法呢又是非常常用的
在所有的优化方法当中都会去使用的l to正则项叫做wait decay
为什么叫权重衰减呢
我们也通过了梯度更新的求导公式给大家推导
由于我们的权重乘以一项小于一的系数
使得我们的权重相对于没有加入l two整的项目的时候减小了
所以我们会称l测试项为为dk全职衰减
再往下我们就介绍了非常常用的旧炮正则化方法
就是随机失活
让我们的神经元呢有一定的概率失去活性
从而增加我们在训练过程当中的多样性
使得我们的网络模型可以减轻过拟合现象
在就怕实现过程当中有一个小细节
大家要知道在这阶段我们所有成员都是激活的
所以我们在神经元输出的时候要乘以一个随机失活的概率
p jpp实现过程当中的一个小细节
好了
这节课我们就讲了1234567
内容是非常多的
去总结
做笔记好
以上就是这一刻全部的知识点了
在下一节课就会给大家介绍统治图像任务的神经网络模型
卷积神经网络
这节课全部内容到此结束
感谢大家的收听
我们下节课再见
