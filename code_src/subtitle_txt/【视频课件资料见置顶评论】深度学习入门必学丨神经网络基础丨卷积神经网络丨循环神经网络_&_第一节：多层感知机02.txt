各位同学大家好
欢迎回来
我们接着往下讲
多层感知机
多层感知机
它是在单层的神经网络基础上增加了一个或多个隐藏层
使网络有多个网络层
因此而得名的多层感知机
我们来看下面两个示意图
先看左边这个示意图
它在输入和输出层当中增加了一个隐藏层
这样它就变成了一个三层的网络
第一层呢是输入层
第二层是隐藏层
第三层是输出层
这就是一个非常简单的多层感知机
有的时候这个网络也会称为两层神经网络
因为它只有两个网络层
有权重参数
也就是只会计算有权重参数的乘
在多层感知机当中
我们的权重参数通常会写成一个矩阵的形式
因为我们看到他输入有四个神经元啊
它可以就理解为四行
然后输出呢有123455个神经元
它就可以写为五列
在这一个权重矩阵当中
每一列就表示一个输出神经元
它所连接的这个权重
我们看到第一列的话应该是有四个
我们看到这个矩阵当中一列它就有四个值
这四个值就恰恰对应到我们隐藏层当中
每一个神经元它所连接的个数
然后从隐藏层到输出层当中的这个连接权重呢
我们应该是多少行呢
应该是输入是五行
输出是三各成员就是五行三列的这个权重
这个权重的每一列就表示它每一个输出的神经元
它所连接的权重了
我们看到第一个神经元它有13455个连接
就要五个权重
所以这个连接权重矩阵就是5x3的这一个大小
左边呢是最简单的
只增加了一个隐藏层的多层感知机
我们来看一下右边增加了三个隐藏层的多层感知机
就是这样的
第一个是输入层employer
第二层呢是第一个隐藏层
第三层呢是第二个隐藏层
第四层呢是第三个隐藏层
第五层呢就是最终的输出层的up player了
这里我们也可以简单看一下它的连接权重矩阵
它的shape是怎么样的
这一个有四个输入及五个输出
第二个连接权重有五个输入
三个输出三
然后第三个权重它有三个输入
两个输出
最后一个权重有两个输入
两个输出
那这里我们可以看到它虽然是一个五层的多层感知机
但是它的权重只有四个
所以我们也可以理解为它是一个四层的多层感知机
这就是多层感知机的一个模型结构
它只是在单层神经网络基础上引入了一个或多个隐藏层
下面我们来看一个多层感知机
它的前向传播
它的数据是怎样从输入层到输出层的
我们这里简单的写一下
这里我们再继续定义一下一个权重矩阵w这个w呢我们称之为wh
也就是它是隐藏层的权重矩阵
经过这个权重矩阵的乘法运算之后
它的输出值是隐藏层的神经元的值
所以这个权重呢我们可以理解为它是隐藏层的
在下面一个权重我们可以理解为它是输出层的权重
另外经过这一个w这个矩阵乘法之后
他得到的神经元是输出的神经元
下面我们来看数据
它是怎样进行前向传播的呢
怎样是从输入到输出的
我们输入x我们可以写为大写的x
这个x呢我们可以理解为它是一个向量
这里我也可以写一下
它是1x4的这么一个向量
然后我们会乘以w h这个w h我们知道前面已经提到了
行呢是输入
它就是一个4x5的
所以它乘出来之后
他应该得到一个1x5的一个向量
1x5的向量正好就对应到了这个隐藏层的神经元的个数
这个乘法之后呢会加上一个偏置
这里我们通常就把这个片子给忽略掉了
那么我们就得到了隐藏层神经元的值h
这里h呢它应该是一个1x5的形式
一个向量形式再往下传播
我们的h就会乘以w h这个权重矩阵
而不是w h w o这个权重矩阵
我们写一下h呢是1x5
那么w是多少w o其实前面我们也写了
他是一个5x3的这么一个shape的一个形状
所以它最终应该得到一个output
一个o o这也是一个向量形式
应该是1x3的形式
这样就得到了我们的这一个输出层
在这里细心有同学应该发现我忽略掉了一个操作
叫做激活函数
理应我们这里应该要加上一个基函数的
我们输出也应该有个激活函数
这一个激活函数对于多层感知机来说是至关重要的
如果没有这一个非线性的激活函数
多层感知机就不能称为多层感知机了
为什么呢
我们接着往下看
如果多层感知机它没有激活函数的话
这一个网络会退化为单层的网络
为什么呢
我们来看左边这一个公式的一个推导
第一行就是隐藏层神经元的计算公式
它是输入x乘以权重矩阵w h还有一个加上偏置项
这是h的计算公式
然后往下我们来看output这个o它的计算公式也是一样的
h会成一个权重矩阵w o再加上一个偏置项
这里应该有个分界线好
这样我们把h可以带入到output这一个计算公式当中
就得到这一个右边这一个形式
我们看到这一项呢
其实它是h的
它把它带入进来之后
我们可以把这个矩阵乘法进行展开
展开之后得到等号最右边这个形式得到最右边这个形式就非常关键了
就是为什么我们没有激活函数的时候
网络会退化为单层网络
我们首先看后面两项
后面两项我们可以统一理解为它就是一个偏置
一个大小
一个b然后左边这一项呢
我们看到它的乘法是输入x会乘以两个矩阵w h和w o
我们知道矩阵的乘法是有结合性的
我们可以先把w h乘以w算出来
变成一个大的矩阵
这样一个形式
我们就发现我们输入只需要乘以一个权重矩阵
再加上一个偏置
他就得到了我们的输出o了
output了
从这一个表达式我们就发现
当没有激活函数的时候
我们两层的多层感知机它就退化为单层的多层感知机了
这一个原因所在就是我们的矩阵乘法它是可以结合的
从矩阵的乘法的结合线
我们就知道
无论增加1000个1万个隐藏层
它都等价于单层的网络
只要如果我们不加入激活函数的话
我们这个矩阵加入1000个
那么我们这一个输出它只不过是h乘以w h一乘以w h2 
一直到大乘以到wh 1000这么多个矩阵
然后再乘以w o然后我们这一系列的矩阵乘法
它其实还是等价于一个矩阵的
因此如果没有激活函数
我们的网络是会退化为单层网络的
这个原理就是矩阵乘法
因此我们通常要在隐藏层要加入g函数来避免网络的退化
也就是下面这一个形式了
我们要加一个西格玛
有的是在隐藏层计算的时候
我们输出呢要加一个奇偶函数
西格玛
从上面的多层感知机我们知道奇偶函数它是非常重要的一个概念
奇偶函数它有两个非常重要的作用
第一个就是上面我们已经知道了
它可以让多层感知机成为真正意义上的多层
否则它就等价于一层的感知基
第二个重要的作用就是引入了非线性
它可以使得网络可以逼近任意的非线性函数
这里是著名的万能逼近定理
大家感兴趣的话
可以搜索一下这个万能逼近定理
这一个万能逼近定理使得人们更乐观的去看待神经网络
因为在万能逼近定理当中证明了
只需要一层的多层感知机
加上激活函数
就可以逼近任意的非线性函数
这就从理论上证明了神经网络它是非常强大的
下面我们来看一下要想作为g函数
它需要具备哪些性质呢
这里给大家总结为三个
首先是要连续并可导
这里它可以允许有少数点上不可导
这是为了便于我们数字优化
后面去优化神经网络的参数的
第二个是机构函数以及它的导数要尽可能简单
在神经网络当中要进行大量的运算
如果基函数运算非常复杂
这会使得我们网络计算的效率很非常低
因此这里要求基函数以及它的导函数计算上要简单
要方便
第三个性质是基函数
它的导函数的值域要在合适的区间
不能过大
也不能过小
在网络更新过程当中
我们会使用到反向传播算法
在反向传播算法当中会使用到激活函数的导函数
如果这个函数值域非常大或非常小
这会直接影响到群众的更新
因此这一个导函数的值域不能太大
也不能太小
下面就给大家介绍三个常见的激活函数
进行单元函数
它们的函数曲线在下面我们看一下这示意图当中
蓝色呢就是我们的函数值函数曲线
橙色的是它的导函数曲线
再往下是它的函数计算公式
以及它的导函数计算公式
我们先来看最左边sick boy函数
sigma函数在早期的神经网络当中是非常常用的
基函数
在现代神经网络当中
主要是在rn循环神经网络当中使用比较多
我们看一下它的函数值有什么特点
它的函数值我们看到它是在0~1这个区间
这个区间恰恰符合概率值的区间
也就是0~1
因此sigmoid也经常用来做
因此sigma函数也经常用来做二分类的输出的基函数
把我们实数域上的值变换到0~1区间
符合我们概率分布的一个形式
这是它一个非常重要的作用
其次他在循环神经网络当中也作为各种门控单元的基函数
用来控制我们信息的流动
是保留还是遗忘
如果这个值越接近于零
就是1万越接近于一
就是保留
下面我们来看它的导函数
它的导函数呢大概是到这个这个位置
在导函数它有两个区域会称之为饱和区
为什么称之为饱和区呢
这是由于它的梯度非常小
它的导函数值非常小
也就是神经元进入了饱和状态
我们看到对应到我们的函数值呢是这个区域
这一个区域的神经元
它的值都是差不多的
都是接近于一的
几乎接近于一
我们会称之为饱和区
在这个位置应该也是一个保护区
这里也是sigmoid函数它的一个弊端
也就是如果我们的神经元大量的落入到饱和区的时候
它的梯度几乎是零
梯度是零
我们就没有办法再往前去传播我们的梯度去更新我们的权重了
这使得我们的网络模型的训练比较困难
这是sigma函数当中的一个弊端
往下我们来看一下它的导函数
它导函数计算是非常方便的
也符合了我们上面所提到的函数计算以及导函数计算要简单方便
它的导函数计算只需要它的函数值乘以一减去它的函数值就可以了
第二个常用的激活函数是天h双曲正切
这一个函数呢与sigma函数是非常相近的
我们先看它的函数曲线几乎是一样的
s只不过它的值域有所改变
是-1~1区间
这一个特性呢就非常符合一个对称
在神经网络训练过程当中
非常需要我们数值的一个对称性
也就是零均值
它的均值我们可以大致认为它是零均值的一个状态
这是它与sigma最大的不同
它的值域是-1~1
下面来看一下它的导函数的导函数其实与s型曲线也激活函数
s型这个函数也是差不多的
都是这么一个形状
然后呢左边和右边都有一个饱和区
这个保护区也是它的一个缺点
弊端所在在
因为在这个保区呢
它的梯度几乎是零
几乎是零
也就不利于我们神经网络权重的更新
不利于我们梯度向前传播
在这里我们可以看到s型函数和ta函数
它都有一个几乎是线性的区域
我们看到这一个区域
以及这个区域
我们看到它的函数值几乎是线性的
我们有的时候也会称这个区为线性区域
线性区域
这是tage和sigma两个基函数
它们的特点
下面就要介绍第三个常见的基函数
因为它不存在饱和区
因此他在现在的深度神经网络当中经常使用
特别的是在举行神经网络当中
隐藏层几乎不会看到有使用s函数以及双曲正切计函数的
几乎是用ru这一个计划数
我们来看一下ru它的函数曲线非常简单
就是等于y等于x
我们看一下它的表达式
就是与零进行一个求最大值
所以它在大于零的地方呢
就是等于x
也就是y等于x
然后小于零的部分它就是零没有值
这个函数的计算更简单了
也非常符合上面我们所提到的激活函数
它的运算要简单
然后看一下它的导函数
导函数这个曲线也是非常简单的
大于零的部分是一
小于零的部分是零
在z等于零的部分呢
它的是没有定义的
这一点就是我们在上面所提到的一个性质了
我回来看一下
就是奇偶函数
它需要连续并可导
但是它可以允许少数点上不可导
少数点上不可导
这里我们看到就是在这一点x等于零
这一点上不可导
但是我们在实现过程当中可以去认为z等于零的时候
我们可以把它的梯度设置为一或者是设置为零
这是ru它的函数计算方式以及导函数计算方式
我们看到都是非常简单方便的
针对于rl函数
它有一系列的改进的计算函数
这一系列的改进基础函数呢主要是在负半轴上进行一系列的改进
这里就不再一一详细讲解了
在这里我们需要知道有两大类的接口函数就可以了
一大堆是s型基函数和双曲正切基函数为代表的饱和基函数
另外一大类是rr为代表的非宝和激活函数
以上这三个基函数在神经网络当中使用都是非常广泛的
大家一定要重点认识这三种激活函数
好以上就给大家讲解了多层感知机以及基函数的概念
我们稍事休息
接下来往下讲
反向传播算法
