各位同学大家好
欢迎来到深度之眼的paper论文班
我是于老师
我们这节课接着讲选修知识当中的神经网络部分的第三个课时
晚篇循环神经网络
这也是我们选修知识当中的神经网络的最后一个课时了
我们接着往下看吧
同样的我们在进行循环神经网络讲解之前
在对上一节课中篇的卷积神经网络进行一个简单的回顾
我们知道卷积神经网络
它是针对图像任务而提出来的一种神经网络模型
那么它的提出呢得益于1962年猫的视觉皮层实践的一些结论
科学家们通过这一个实验的结论
相应的设计出了卷积神经网络的结构
在这个猫的视觉皮层实践当中得到两个对cn结构具有启发的结论
一个是视觉
它具有层级结构
它的识别过程特征识别过程是由低级到高级逐级的抽象的过程
第二个是细胞具有局部感受感知的特点
也就是细胞它有它特定的感受
也感受区域
了解了猫的视觉皮层实验
我们讲了cn发展史上三个第一
第一个cn雏形就是行政之基
第一个大规模商用的卷积神经网络
就是lina fla
fi呢
在1998年
大规模的在美国的邮政系统当中的手写体邮政编码进行应用了
第三个第一呢就是第一个基金视作震惊四方的卷积神经网络
alice net
由于他在2012年的ios v r c挑战在中超出第二名10.9个百分点
一个很巨大的差异夺冠
卷积神经网络又受到了大家的关注
并且拉开了卷积神经网络统治图像任务的序幕
对卷积神经网络简历史发展史进行简单介绍之后
我们又详细讲解卷积神经网络的两个重要操作
第三部分和第四部分
卷积和磁化卷积操作
我们可以认为它是一个特征提取的过程
它用卷积核当中的权重
我们可以认为是某一种特征的模式
我们拿着这一个卷积核在输入图像上不断的滑动
从左到右
如果有的话
我们就会给他输出比较大的值
如果没有的话
会给他输出比较小的值
这就是卷积的一个理解
并且也给大家展示了x nt这篇论文里面的第一个卷积层的
卷积核的可视化的图片
我们从那一张图就发现
卷积核它呈现的是一些边缘的特征信息以及是色彩信息
这个呢是由于它是在第一个卷积层
也就是我们图像的基本的元素
就是一些边缘
一些色彩
这是关于卷积操作
卷积核的理解
后面还讲了多通道的卷积
我们在做图像分类
利用卷积神经网络做图像分类的时候
大部分的卷积操作呢都是多通道的卷积
在讲多通道卷积的时候
强调了一点
就是卷积核
它是一个卷积核
它是3d的增量
三维的增量
但是我们所执行的其实它是二维的卷积操作
大家一定要认清楚一个卷积和它有三维这三个维度分别代表什么
第一个是cn input数据的通道数
然后后面两个就很好理解了
就是高和宽
这就是关于卷积的一些知识点
第四部分给大家介绍了实话操作
磁化操作主要有两种
一种是最大磁化
另外一种呢就是平均磁化
这个计算方式都非常简单
而且也给大家介绍了一个磁化
我们怎么去理解一个词化
给大家举一个例子
就是操场上篮球场下了雨之后
这个水会汇聚的汇聚集到某一个比较低洼的地方
这就是一个磁化的一个解释
而且此话呢我们可以看作是一个特殊的卷积
当然是我还记得我们有两种卷积核
如果最大石化的话
它的权重是一
如果是我们这个average ping的话
这个平均磁化我们的权重呢就更简单了
就是直接是平均值就1/4
这样我们就可以把磁化看作是一种特殊的卷积
因此在现代的很多的卷积神经网络当中就弱化了磁化的功能
有一些backbone呢它就不用磁化了
即使用可能只用一次或者两次
上一节课的第五个知识点就是图像识别的三个特点
这三个特点与卷积操作和磁化操作是息息相关的
这里再给大家总结一下
第一个特点是特征具有局部性
比如我们要识别老虎和猫这个主要的特征
就看两只耳朵中间有没有一个王字
有的话它就是老虎了没有
就是这个这只胖猫了
第二个特点就是特征
它可能出现在任何位置
就是这个王呢就根据他头头部可能在图像的任何位置
所以特征是可能出现在任何位置
第三个特点还是这只胖猫
我们假如这只胖猫原始图像是512x512的
我们把它下采样到30x3色
它是不会改变
不会改变它的属性
不会改变图像的目标
它人就是一只胖猫
人就是这只猫啊
这是关于图像识别的三个特点
它与卷积和策划操作息息相关的
这五个主要部分就是我们上一节课对卷积神经网络介绍的主要知识点
大家听完上一节课有没有认真做笔记呢
我刚刚回顾了这些知识点
你是否有忘记的
如果有忘记的
希望大家再回到上一节课再听一遍
你所忘记的那一部分
接下来我们就正式进入循环神经网络的知识
在这一节课会分为七个小节
第一部分给大家讲解序列数据
序列数据在我们日常生活当中非常常见
也是我们循环神经网络所要干的事情
序列数据
第二个部分给大家简单介绍语言模型
语言模型是自然语言处理技术当中
n r p技术当中非常重要的一个一门技术
我们需要去了解它
第三就开始进入循环神经网络的讲解
第四第五部分就给大家介绍更复杂的循环神经网络
第六部分呢就会对这一节课的知识点进行简单的回顾小结
最后一部分就对我们整个神经网络的课程的知识点进行一个总结
也就完成了我们神经网络一天的课程
早中晚
好同样的我们的老规矩
才可以事半功倍
我们看一下在这一节课循环神经网络当中
我们有哪一些目标呢
第一是序列数据
我们要知道我们在日常生活当中
哪一些数据是序列数据
哪一些任务它的数据是序列数据以及序列数据
它有哪些特点呢
这一个我们将会在第一部分给大家讲解
第二个是语言模型
在自然语言处理当中
语言模型是必不可少的一个内容
那么语言模型它是什么意思
我们该怎么去计算一个语言模型
这一个内容我们将在第二个部分讲解
了解了序列数据原模型做好了铺垫
我们就可以正式进入循环神经网络的讲解
第三部分就给大家介绍rnn我们的循环神经网络
在n n循环神经网络当中
我会详细的介绍数据是如何前向传播
以及梯度是如何从后穿越时间
通过时间的反向传播算法
讲完n我们再往下就要给大家介绍两种高级的循环神经网络
或者是说针对于rn
针对巡航神经网络进行改进的两种循环神经网络
第一个是gu
就是门控循环单元
另外一个呢是长短期记忆网络
l s t m这里没写出来
就l s t m这一个长短信息网络
这两个网络针对于rn就更高级一些
他们增加了一个门框的概念
有一系列的门
这一门有什么作用呢
我们将会在第四第五部分讲解这个门
好我们正式开始第一部分序列数据
序列数据它是日常生活当中常见的一些数据
这些数据有很大的特点
它主要的特点就是数据的前后有关联性
大家记住这是关联性
怎么理解这个关联性呢
我们来看一个句子
我们可以把一个句子看成是一个序列数据
每一个单词呢就是它一个元素最基本的元素
我们来看这个句子是这样的
cast average fifteen hours of live a day
他说的是猫
他每天平均可能要睡15个小时
在这里我刚量了两个元素
两个星期两个单词
一个是cs
我们看这个看猫
一个是15
我们看到这15它是严重依赖于这个主语的
也就是第一个元素
这一个如果是换成人啊
或者是澳大利亚的考拉
那这一个食物可能就不够了
他可能睡不够
这个数应该到了20
他平均每天会睡20个小时
如果是人的话
那这个数这一个元素他可能就是八八个小时或者七个小时
或者九个小时
或者有些同学可能会睡十个小时每天
但是肯定不会到15
所以我们发现这一个序列当中的第三个元素
它所出现的这个值严重依赖于第一个元素
而第一个元素出现
它会影响到后面一些元素的出现
就是这个15出现的概率呢严重依赖于第一个元素
这就是一个序列数据
它前后数据有一个关联性
这就是这个关联性的一个举例
那么我们日常生活当中
除了这些文本
还有其他很多序列数据
这里就给大家简单举个例子
这一幅图呢是来源于在第一个课时也给大家推荐了的
问答老师的生物学课程当中的一个ppt
里面给我们列举了很多序列数据的一些例子
我们一起来看一下第一个语音识别
语音识别任务呢它给我们的模型输入的是一个序列的音频
然后它要输出一个序列的文字文本
这一个版本呢也是一个序列数据
输入也是一个序列序列数据
这是语音识别任务
还有是音乐生成
它的输入可以是一个空集或者是一个整数
用来代表音乐的风格等等
我们输入的可以是空的
然后呢我们的模型就会输出这些音符
再往下我们看这是一个文本的分类
比如对一个电影的描述评论
我们要给这个文本一句评论
给它一个分类
它的分类有111星两星
三星4星五星
也就是给电影做一个评分
我们根据他的描述来自动的给它打分
我们看到这一个输入呢是一个序列数据
也是文本
再往下一个dna dna的序列
dna的序列我们知道dna它就是有四个剪辑tcga来构成的这么一个序列
而这个序列当中我们要去预测哪一些序列
构成哪些蛋白呢
这些分析的工作
这些数据d n a的数据i是一个序列数据的例子
再往下我们看第五个是机器翻译
我们输的是比如说是法语或者德语
然后要输出是英语
这就是不同语言之间它都是一个序列数据
我们的模型需要接受一个语言的序列数据输出
另外一个语言的序列数据
这么一个机器翻译的应用也是一个典型的序列数据的处理的过程
再往下我们看第六个例子
第六个例子它是识别视频的行为识别
我们输入呢是一帧又一帧的图像
然后通过这些图像
我们要输出一个类别
它是在禁止走路还是跑步等等这些行为识别
在这个视频的行为识别的任务中
我们看到这里有四帧图像
因此我们可以把每一帧图像或者叫做每一张图像
把它看作是一个基本的元素
这里就有一长串一个序列的形式
有序列长度是四的这么一个序列数据
还有我们有命名实体识别
比如说在nlp当中有命名实体这么一个任务
就是输入一个序列
一个文本
这要识别文本当中哪一些是人名
比如说我们要输入这一个句子
然后它能识别出来这两个是人名
这一些都是我们日常生活当中常见的序列数据
在序列数据当中它最大的特点就是数据的前后具有关联性
大家一定要记住这一个最大的特点就可以了
具有关联性
其次大家要知道序列数据一定要有一个先后或者说前后的关系
好我们了解了序列数据
下面给大家介绍自然语言处理当中常用的语言模型的概念
语言模型它是属于自然言处理技术当中的一门重要技术
在自然语言处理当中呢
常常会把文本看成是时间的序列
离散的序列
一段长度为t的文本
可以把它里面的每一个词作为一个基本的元素
我们可以标记为w one w two
一直到w t这里的t呢就是时间布的概念
大家注意这个专有名词叫做时间不
这个时间段呢通常是在序列任务当中我们会使用的这么一个概念
在后面我也会反复提到时间不第一个时间步
第二个时间
第三个时间一直到第七个时间不的这么一个概念
这个时间不这个时间步的概念
我们就可以理解为是序列出现的顺序就可以了
在这里我们语言模型通常会把一个文本就看成这么一个时间
不在2p当中
我们就把文本写成w one w two
一直到wt
那么语言模型它要干什么呢
它要干的事情就是告诉我们这一个句子它出现的概率
联合概率是多少
也就是这个序列的概率有这个句子出现的概率
比如我们在讲序列数据当中用的这一个例子
cast average fhours of sleep a day
这一个例子
这个句子呢它就有一个概率模型
在这个句子当中
我们来看一下它总共有多少个时间步呢
我们可以数一下一个单词
两个单词345678
因此我们知道这一个序列数据它的时间不总的时间不是t等于八
然后呢我们会把每一个单词看作是一个标签
或者是当前的一个输出
这里的w one w two一直到最后面的w就是这么一个形式
那么我们的语言模型要做的事情就是要计算出这八个单词联合概率
它的一个联合概率是多少
要告诉我们
这就是语言模型要干的事情
下面我们就举一个语音识别的例子
帮助大家来理解语言模型的作用
在这里呢大家来听一下题
我要说一句话
然后你们通过听我的这个语音
然后要写出中文的文字
看一下你们是否能写对呢
好现在请听题
厨房里的石油用完了好
大家再听一遍
厨房里的石油用完了
大家能否把我刚刚语音所读出来的写成中文的文字呢
现在给大家看一下答案
刚刚我读的是雏形里
刚刚我读的是厨房里的石油用完了
那这里就有一个多音字
多音词我们可能会去混淆
就是到底是我们汽车用的那个石油呢
还是我们食用的石油
美食的石呢
是石头的石还是美食的食
这一个问题
如果我们只通过语音
它是没有办法去解决的
是容易混淆的
可能出现的概率各一半
但是我们有整个句子的前后的一个信息
我们通过前后语义信息的关联
也就是序列数据
它最大的特性是什么
前后数据的关联
我们看到前面有一个厨房
那么厨房与石油这个石头的石的这一个关联
出同出现的概率可能是要低于厨房里面出现这个美食的
是这个石油的概率的
因此我们的语言模型干的事情就是这样
我们看一下
下面就是语言模型干的事情了
语言模型要给出厨房里的石油用完了这个十的概率
以及也可以计算出厨房里的十又用完了这个十
它的概率我们去比较一下这两个句子
它的概率的大小就可以得到我们一个正确输出了
下面我们来看一个语音识别的例子
就是我用搜狗的这个语音识别
看一下搜狗输入法的语音识别
它对于厨房里的石油用完了
他的输出是什么呢
我们来看一下
好我们看一下这里搜狗的语音输入法给我们输出的是这一个句子
厨房里的石油用完了
他还说出了是这一个石
石头的石
很显然不是我想要的这个美食的石
或者是没有这个判断
很显然这是他们不足的地方
大家有没有新的同学
有没有看到我在读前两个字的时候
他也有输出
我们看一下在前两个字啊
这里停下
我在读厨房的时候
他已经输出了两个文字叫
除非感觉还有点像的
或者说还是我的普通话不标准
反正他说出了两个
除非但是呢我还在继续给它输入音频信息
当它接收到后面的音频信息之后
他会将前面已经输出的文字进行修改
我们看一下它把除非后面会改成厨房
厨房里的
并且我们看一下这里有个厨房里的十
他一开始给我们输出是时间的十
看没看到这里了
它是时间的时
与我们的这个十还不一样
因此这一个过程这个语音识别的过程它就用到了语言模型
它会综合去考虑我们序列数据前后的一个联系
我们就知道了语言模型它的作用是什么
它主要要计算我们一个序列的
它的联合概率
我们学概率的时候就知道联合概率呢它可以拆成这个形式
我们来看一个例子
比如说有这么一个序列
我在听课只有四个四个元素
它的一个联合概率就是四项概率相乘
首先是我的概率
然后再乘以我的条件下出现在的概率
然后再乘以我在的条件下出现ta的概率
然后再乘以我在听的条件下出现k的概率
把它们存起来就可以得到了这个联合概率的
这个是我们概率论当中的知识
而我们在语言模型该怎么去计算呢
我们通常一个语言模型是这样来做的
我们可能要得到上面这四项的概率对吧
只要找到这四项
123这四项的概率
我们一乘就可以得到了
我在听课的概率
这里我们这四项概率怎么来呢
通常就是从语料库当中去统计这些持平
然后就可以得到以上概率了
这一个有一个专有名词叫做corpus
叫做语料库
这个是nlp技术当中的一个专有名词
语料库的意思就是我们收集来的这些文章文本信息
把它汇总起来
这就是我们的语料库
我们要去统计这一系列的文章文本信息
来看一下那个我字出现的概率
然后我自出现的条件下再次出现概率
等这个1234去统计
然后相乘就可以得到了
我们这一个语言模型计算出来的
我在听课这个概率
但是这么做有一个弊端
大家看一下
它的计算会随着时间t的增长而呈指数的增长
就是它的计算量会很大
如果我们是一个很长的句子
比如说这一个句子
我们的序列还有1000个元素
1000个词的话
1000个字的话
这个计算量是非常大的
我们再用这个方式去计算这个概率的话
显然是不可取的
这是它的一个很大的一个缺点
就是随着时间t呈指数增长
那么我们有没有好的方法
让这个计算量计算的参数不会随着时间t的增长而改变
其实是有的
我们接着往下看
就是我们要讲的循环神经网络了
第三部分我们就给大家讲解循环神经网络
到这里就给大家讲了什么是序列数据
序列数据它最大的特点是什么
以及语言模型的概念
以上的序列数据和语言模型都是我们的一个铺垫
到下面就开始正式讲解我们的循环神经网络
在正式讲解之前呢
我们还是稍事休息
回来之后接着往下讲
第三部分
循环神经网络
