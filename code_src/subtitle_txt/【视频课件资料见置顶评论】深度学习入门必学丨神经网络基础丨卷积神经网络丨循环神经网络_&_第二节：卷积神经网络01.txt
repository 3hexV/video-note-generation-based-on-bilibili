各位同学大家好
欢迎来到深度之眼的paper论文班
我是于老师
我们这节课继续进行选修知识当中的神经网络部分的第二个课时
中篇卷积神经网络
给大家介绍统治图像领域的一种神奇的强大的神经网络结构
在进行这节课之前
我们简单的对上一节课进行回顾
我们在第一个课时介绍了非常多关于神经网络基础的知识点
以及人工神经网络
它是由大量的人工神经元以某种连接方式连接得来的
而通过不同的连接方式
就构成了不同的人工神经网络
第二部分就给大家介绍了多层感知机的概念
多层感知机呢就在感知机的基础上增加了一个或多个隐藏层
使得它具有强大的特征
提取抽象功能
在多层感知机当中
我们知道了数据是如何从左到右
也是从输入层等到输入层的一个前向传播过程
在这个前向传播过程中
有一个非常重要的概念
就是在第三部分给大家详细讲解的激活函数
激活函数对于多层感知机而言是至关重要的
因为如果没有这些非线性的激活函数
多层感知机
它还是等价于单层的感知机
这一个大家应该知道是为什么了
因为我们的矩阵乘法它是有结合性的
比如说你1000个1000个矩阵相乘
它还是等价于一个矩阵
一个矩阵它就等价于单层的网络
在基环境当中给大家介绍三个非常常用的奇函数
这三个激活函数呢又可以分为两大类
一大类是s型函数和天h函数组成的饱和函数
另外一大类呢是以ru为代表的非法和计划人数
通过二三部分
我们知道了整体网络当中的数据是怎样前向传播的
然后我们介绍反向传播算法
反向传播算法就是将梯度从最后面的loss
这个loss值通过loss function逐步的从后向前传播的一个过程
这个梯度逐步的去传递到每一个变量
然后这些变量在用于权重的更新
权重更新使用了梯度下降法
在梯度下降法当中
再讲反向传播算法当中有一个很重要的桥梁
这个桥梁就是连接模型与标签的桥梁
叫做损失函数
损失函数它的作用就是衡量模型的输出与标签之间的差异
在损失函数当中
我们又介绍了损失函数
代价函数
目标函数
这三者之间的差异是什么
同时也给大家介绍两种常用的损参数
一个是m s e均方误差
这一个在回归任务当中用了很多
另外一个是交叉商
交叉商cos androp
它主要是用在分类任务
对交叉商
我们从信息熵的角度分析了信息熵
相对熵
交叉熵三者之间的关系
我们知道优化交叉熵就等价于优化相对熵
而相对熵它又叫做kl散度
是用来衡量两个分布之间的差异的
因此交叉叉商也可以用来衡量两个分布之间的差异
讲完了损失函数
我们讲第六部分
全职初始化
良好的圈子初始化可以加快神经网络模型的训练
甚至可以得到更好的效果
在全职初始化当中给大家介绍一种错误的初始化方法
就是全零初始化
也给大家介绍一种常用比较正确的初始化方法
就是通过高斯分布当中随机抽样得到数据
再复制给我们的权重
在高斯分布当中有两个重要的参数用来控制我们的分布
一个是均值
另外一个就是标准差和均值呢
我们通常都是用零和标准差
我们用什么呢
这个标准差它会控制我们权重的大小
我们知道我们的权重不希望它太大
也不希望它太小
而选择良好的标准差是初始化的关键
在第六部分就给大家介绍了自适应选择标准差的方法
lavia初始化方法
以及何凯明初始化方法
最后第七部分给大家介绍了正则化方法
regularization
正则化方法
我们从中文字面意思很难去理解它到底干的是什么
其实正则化方法它就是用来减小方差
减轻过敏的现象的方差
它又是什么呢
方差它是训练集与验证集
或者说测试集它们之间的指标上的性能上的一个差异
我们要想减轻这训练集与测试集之间的差异
指标上的差异就要用正则化方法来减轻它
减小方差
减轻过敏和现象
在政治化方法当中给大家介绍两个常见的l one l two正则化方法
其中这一个l two呢又会称之为vk全职衰减嗯w d v d k全职衰减
为什么它称之为全职衰减呢
我们通过推导梯度下降法的公式发现
加了l two正的项目之后
相对于没有加l two正能项的时候
他的权重w乘以一样因子会多乘一项因子
而这一项因子呢它是小于一的
因此加了l to智能项之后
我们的权重相对于没有加
它是会收缩的
会减小的
因此l two称之为with decay
全值衰减
另外还介绍一种常用的正则化方法
随机失活在全连接层使用比较多
在这里我们看到这个示意图当中
随机复活
他在训练的时候就会挑选一定概率去挑选一些神经元
让他失去活性
失去活性的意思也就是神经元不会与其他的神经元任何成员相连接的
比如这里会选中
然后打一个叉叉
这就是训练过程当中
每一次前行传播
它都会以一定概率去选择神经元
这就增加了网络的多样性
可以有效地减轻过敏和现象
在这里大家一定要注意
它是在训练阶段才会随机送
我
也就是在训练阶段才会以一定概率去选中一些神经元
而在测试阶段呢
他是不会执行这个随机失活的
他不会去随机选中某些人员失去活性
在测试阶段
所有神经元都要工作
正是因为在训练和测试阶段
网络表现不同
因此在实现过程当中有一个小细节需要去注意
那就是在测试阶段
神经元的输出还要多乘一项概率p
为什么要乘这一个概率p呢
我们也在上节课的第七部分给大家推导的公式
这就是上一节课神经网络基础的知识点了
上一节课我们介绍非常多的知识内容
下面我们就正式进行第二个课时
卷积神经网络的讲解
这节课呢我们会分为五个部分
首先给大家介绍卷积神经网络它的发展史
它怎样从猫的视觉系统当中抽象得来
建模得到现在的卷积神经网络的结构
我们将会在第一部分发展史当中介绍
然后给大家介绍卷积网络当中两个最重要的操作
卷积操作以及磁化操作
第四部分给大家介绍第一个大规模商用的卷积神经网络
lina fi以及卷积结构
它的一个进化史
最后就是对这节课进行一个简单小结了
才可以实现事半功倍
我们一一来看一下
第一个就是猫的视觉皮层实验
我们从猫的视觉系统实验过程当中得到哪些重要的结论
这个结论与我们的卷积神经网络现在的结构它又有哪些联系呢
我们将在第一部分给大家讲解
第二个就是卷积神经网络发展史上的三个第一
第一个卷积神经网络的雏形
新任之机
以及第一个大规模商用的卷积神经网络
linux linuf
以及第一个震惊四方
技惊四座的卷积神经网络
ne这三个cn历史上的第一
对于我们理解卷积神经网络的发展是非常有帮助的
卷积操作
这两个操作卷积池化是整个卷积神经网络当中的最核心的操作
这三个特点对于卷积神经网络它的设计都是有一一对应关系的
这些卷积会对应到图像特点当中的第一第二部分
而磁化呢会对应到这三个特点当中
第三个部分
而至于怎么去对应
为什么这样对应
他们中间有哪些操作
我们将会在二三部分给大家介绍好
下面我们就开始讲第一部分
卷积神经网络的发展史
卷积神经网络它是针对图像任务领域提出来的一种神经网络模型
下面这一个示意图呢就是一个简单的卷积神经网络示意图
它是linuf
也就是第一个大规模使用的卷积神经网络
而卷积神经网络呢它经历了数代人的发展
才有了如今的成就
大部分的图像任务都被卷积神经网络所统治
比如说图像分类任务是吧
分割任务
目标检测
图像检索等等
与图像相关的一系列任务的最优的方法
算法都使用到了卷积神经网络
从这里我们就可以看出来卷积神像哦
它的强大之处
然后卷积神经网络有如今的成就并不是一蹴而就的
他的发展经历了数代的贡献
下面我们就来探究一下卷积神经网络
它是如何一步一步的发展而来的
最早卷积神经网络它的发展
它的结构设计还是受到了视觉系统的启发
那这一个生物视觉系统的实验对于cn的发展呢是非常重要的
我们很有必要来了解一下这一个生物视觉系统的实验
这个实验呢是有这两个科学家
就是右边这两个科学家
这两个科学家呢也在81年的时候获得了诺贝尔医学奖
他们在1962年就做了这一这样一个实验
它从猫的视觉系统当中发现了视觉系统中存在的一种层级结构
以及发现两种重要细胞
s细胞和c细胞
这两种细胞呢也恰恰对应到现在的卷积神经和
当中的卷积操作和磁化操作
并且不同类型的细胞它会承担着不同抽象层次的视觉感知功能
这个实验具体是怎样操作的呢
我们接着往下看
首先呢第一步是要找一只猫
好像有点废话
肯定是要找一只猫了
找一只猫之后呢
要在他的大脑上打开一个口
我们看到要打开大约一个3mm的口
然后插入电极去探测它大脑当中
去感知他大脑中神经元的一个激活状态
我们在中学阶段学生物的时候就知道神经元它就是一些电极的响应
我们可以去探测这些神经元
它的电压的高低
从而可以知道这些神经元是激活还是抑制或是不激活的状态
这就是为什么要打开猫的大脑插入电极的原因
插入电极之后呢
就要把猫给固定下来
我们要把它眼睛给固定下来
给他看特定的东西
就会让这些猫看一些各种形状的光条
我们来看大概是这个位置的
有一些垂直的水平的或者斜着的某种角度的光条
或者是圆点的光斑
或者是大的圆点
小的圆点
这些光斑给他看不同的这些光条
在这里大家看到有一个显示器就是固定住
也把毛毛的头给固定住
让他的眼睛去看看这一些光条
然后再来观察他大脑当中的这些神经元是否激活
在这一个猫的视觉系统使用过程中
就发现了很多有意义有价值的现象和结论
我们来看第一个就是神经元
它存在局部感受区域
这个名称局部感受区域我们也翻译为感受眼
这里的感受也呢我用高亮标出来的这一个感受
也也正是现在卷积神经网络当中所提到的这个感受也的概念
我们先说现在卷积神经网络当中提到感情后
也是指这一个神经元
它能看到前面输入层的哪一些细胞
哪一些神经元的值或者说可以理解为神经元
它与前面的哪一些神经元相连接了
这一个概念
恰恰是生物的视觉系统当中的一个现象
局部感受区域
我们来看一下生物的视觉系统是怎样的
这些感受区域是怎样的
我们来看右边这个示意图
这一次意图呢展示了几种细胞的感受
也它的感受区域
我们先解释一下这个图例
就是a x呢这个叉叉呢表示有响应
这个三角形的表示没有响应
我们怎么去理解
我们来看这一个地方
先看这里是c细胞
这是低细胞
然后这里展示了五个细胞c d e f g这五个细胞
我们每一个细胞呢在这里应该是一个方框
才是前面我们所看到的那个屏幕
这个方框是那个屏幕
我们在屏幕里面不同的位置去显示一些光条
我们发现有一些c细胞呢
它在这一个这个叫这一个对角线
就这一个对角线的周围呢
它是有响应的
我们可以看到这里是叉叉
然后远离这一个对角线之后都是三角形
也就是没有响应的
因此我们发现c细胞它的感受区域
他的感受也
他的感受也是
这里我给大家画一下
就这个区域
然后我们再来看低细胞
低细胞一样的
这个是那个显示器这一个细胞呢
它的感受区域与c细胞正好是相反的
它在这一个对角线的周围呢是不感受呃
不激活的
就是你在这一个对角线周围放一些光条光斑
这个成员他是没有响应的
没有激活状态
然后它是在这个区域
恰恰与c细胞是相反的
同样的我们再可以看一下c e f g这三个细胞
它的激活状态
他的感受区域
我们看到一细胞呢
它的感受区是在这个区域以及这个区域f细胞呢这个呃特点啊
就在两端
这两端才会使得f细胞响应去感受去激活
而巨细胞呢也非常有特点
它是在这条对角线的左半部分是有响应的
右下半部分是没有响应的
这一个实验就得出来了
神经元存在局部感受的这个重要的结论
这个结论呢也有助于我们去理解
现在卷积神经网络当中常常提到的一个概念叫做感受
也一开始我接触的感受也概念非常难理解
感受也这个也是什么意思啊
感受有什么意思呢
其实这些都是从猫的视觉或者说生物的视觉系统当中得来的这些结论
下面我们再来看第二个有趣的重要的结论是
细胞对角度就是对这些光条的角度有一定的选择性
我们来看右边这个示意图吧
右边示意图我们重点来看下面这个这个c的部分是一个横纵坐标
横轴呢表示给他看
给这个小猫看的光条的角度
这个角度从60度到100 110度左右
这一个区间范围我们可以看到b这里就是它不同的不停的去转动
当给这个小猫看的这个光条大约在87度或者80摄氏度
90度大概是垂直
大概是垂直的时候呢
这一个神经元它的响应强度是最强的
而当这个光条逐渐平缓的时
候
像两边平缓的时候
我们看到在60度或110度的时候
它的响应就没那么强了
从这里就发现了
细胞呢对角度也有一定的选择性
下面我们来看第三个结论
细胞它对光条运动方向也有一定的选择性
我们来看右边这个示意图
非常有意思
上面呢是这一个光条在这个与起始位置在这里
然后它是向左上角移动
而下面这个光条是在左上角向右下角移动
在实验里发现有一些细胞它是对左上角这个运动方向响应比较强烈
我们来看这里
这一部分就是响应强烈的
而这个光条向右下角移动的时候
这响应就没那么强烈
从此也就发现了细胞的运动方向
它也是有一定选择的
以上就是在猫的视觉系统实验过程中
三个有趣的结论
在这个实验过程当中还有很多的结论
这里就不再一一去列举了
我们主要来看这个实验对于卷积神经网络它的启发有哪一些
在这里呢给大家总结为两点
第一点视觉系统它是分层分级的进行处理的
是从低级到高级的一个抽象过程
在现在的卷积神经网络也可以看到这一个结论存在
就是从低级到高级的抽象过程
对应到现在卷积神经网络结构当中呢
它就是堆叠的使用卷积和池化这两个操作
不断的去使用卷积池化
卷积策划来对特征进行逐级的抽象
逐级提取的这么一个过程
第二个结论就是神经元实际上是存在局部感受区域的
具体来说就是对局部敏感
这个结论对应到现在卷进成长后
就是神经元的局部连接了我们的神经元
不需要与前面的所有的所有的神经元去连接
只需要与前面的某一个区域相连接就可以了
这就是局部感受区域的概念
好我们了解了对于卷积神经网络结构具有启发意义的实验
下面我们就来看卷积神经网络的发展
首先给大家介绍卷积神经网络发展史上三个第一中的第一个
第一好像好绕口
我们直接来看吧
第一个卷积神经网络的雏形叫做新政之基
现在g的示意图
我们在右边可以看到
我们先说这个结构吧
这个新政之基是由日本的学者在1980年的时候提出来的
一种神经网络结构
它最大的特点就是具有层级的结构
这个层级结构也是从上面这个猫的视觉系统实验过程当中呃
得到启发的
我们来看一下右边这个示意图
它是怎样的层级结构呢
这里有两个网络层
一个这里比较小的
看不到
这里是s就是对应到之前提到s细胞
然后这一个c c层叫做c细胞
这就是从上一个实验过程当中的借鉴得来的这个思想
s细胞和四细胞
然后呢它堆叠的去使用了这个s细胞和四细胞
这就是一个层级的结构
层级的神经网络在前面也提到了
s细胞和四细胞
也可以类比到现代的卷积神王中的卷积层和池化层
虽然1980年的时候
福岛邦彦就提出了行政资金
但是新人基金并没有得到广泛的发展
这是由于它存在一些弊端
它最重要的缺点呢就是没有反向传播算法来更新全职
并不是像现在广泛使用的监督的卷积神经网络的模型
主要它还是没有反向传播算法这个内容
因此它的模型性能是有限的
在这里也给大家列举了一些辅导半圆
在这里也给大家列举了一些关于辅导方言的一些文章
这篇文章是早期辅导方言对于新政之基的一些研究
关于辅导方言这一个学者呢
我还想多说两句
就是大家可以去看一下他的主页
这里有主页
大家直接搜辅导方面
也可以看到他的主页
大家可以去他的主页去看一下他所发表的论文
他从80年1980年提出新任之机
到现在
21世纪10年代
他还在研究这个新生之一
他并没有放弃这个古老的神经网络结构
这一点是令我很震惊的
一个学者能坚持几十年的不断的去研究他所提出来的一个技术
一个方法
我觉得这一点是很值得令人钦佩的
因此非常推荐大家可以去看一下辅导方言
他的主页当中有哪些论文或者有哪些工作
下面给大家介绍发展史当中三个第一当中的第二个第一
第一个大规模商用的卷积神经网络
linuf li fi是乐坤大神所提出来的
乐观大家应该不需要去介绍了
其实lei的网络结构呢
早在1989年的时候
乐坤就已经开始研究lei的网络结构了
这边我给大家展示的是linux的一个网络结构
大家注意啊
这里不是linuf
这一个呢是1989年这一篇论文
但是还没有发展到dnf的网络结构
在这里也要提一下
乐坤在很多采访的时候也提到了
linut提出的设计呢也很大一部分是借鉴了新政之基
也就是辅导帮人的工作给了乐观很大的启发
或者是提供了借鉴意义
这是林黛玉行政之基之间的一个关系
而linut成功的大规模商用呢是来到了1998年的时候
乐观发表这篇论文
我们来看一下
在这里最下面这篇论文
最下面这一个lina fine这一篇论文
1998年这篇论文
这篇论文还是非常长的
好像有40多页
至少40多页
这一篇论文你就提出了linux成功地应用到了美国的邮政系统当中
的手写邮政编码的识别
也就是手写的数字体
0~0123456789这十个数字的一个分类任务
虽然lina fi它大规模地应用到了美国的邮政系统
但是他的能力还是非常有限的
只能应用于这些灰度图像
并没有在自然图像当中大放异彩
这也有一些原因
第一个原因是当时对于自然图像
它也没有那么多的标签数据提供给我们的模型去训练
其次当时的计算资源
计算机的性能也不足以支撑起大规模的运算
这两个缺点就限制了历代大规模的使用
广泛的使用
下面就要给大家介绍发展史上三个第一的最后一个第一
第三个第一
第一个基金试做的卷积神经网络
linu linux在2012年预计成名
他以超出第二名10.9个百分点的成绩
夺得了l s b r c分类任务的冠军
所以它还是非常重要的
能在这上面夺冠都是非常厉害的模型
这里ios v r c更详细的内容呢
我将会在cv的bassline当中的第一篇论文
第一篇论文s nine当中给大家详细讲解
感兴趣的同学可以到cv basslide第一篇论文当中去看一下
i r s b r c挑战赛
以及它与image net这一数据集之间的关系
好我们回到s net net
它以10.9个百分点的差距拉开第二名的差距夺冠
大家想一下
10.9个百分点是一个非常大的差距
因此f呢以这样的成绩夺冠
使得大家又把目光聚焦到了卷积神经网络的一个强大
神奇的神经网络结构当中
并且从2012年开始呢
就拉开了卷积神经网络统治图像领域的序幕
这么说是完全没有夸张成分在这里面的
现在最新的比如说图像分类
图像分割
目标检测等等一系列图像任务当中
最优的算法大部分都是用了卷积神经网络是一个结构的
卷积神网络是必不可少的
为什么s代它能一举成功呢
这里总结了三个成功的要素
首先是塑料
也就是我们的数据有大规模带标签的训练数据
这一个就得益于imagnet
它的功劳就是image net的功劳了
在image net当中提供了120万的训练数据
120万的训练数据给这个i l s v i c挑战在去训练
而因为金价当中呢存在1400多万的带标签的数据
这是第一个因素
第二个因素就是算力了
有了强大的计算资源
gpu在sn当中他提到他们采用了两块英伟达的gtx 580580
两块乘以二
还有第三个因素就是强大的卷积神经网络模型了
net在右边这个示意图呢就是s net它的结构示意图
我们从这里可以发现
它相对于上面第一个大规模商用的linubi
它的模型结构相对于是比较复杂了
我们可以看到它的收入呢是224x224的分辨率
然后我们看一下它的输出分类也与上面的linufi有所不同
这里输出的是1000分类
我们的linut只有十个类别
这个类别数量就大了100倍
并且呢它网络深度它是要大于linux深度的
从这里就可以知道
alice net比linux还要复杂许多
我们可以来看一下s net它的结构又有哪些呢
这里结构可以分为两大部分
可以在这里划一刀
砍成两半
前面的部分呢就是卷积磁化的部分
我们这里卷积它是没有标注出来的
我们可以写在这里
combo这里是卷积
然后有磁化
然后又有卷积
又有磁化
不断的卷积池化得到后面的特征图
这个方块叫特征图
特征图在输入的后面的一系列的fc层叫做全连接层
全连接层呢也就是我们之前所学过的多层感知机的概念
在s奶当中经过三个全连接层
就输出1000个类别
它的分类的概率了
这就是一个卷积神经网络基本的一个结构
有前面有卷积池化
卷积池化堆叠而来
这个堆叠是借鉴了猫的视觉系统
实验过程当中发现视觉系统它是有层级结构的
是由低级到高级的抽象
当大量的卷积池化
卷积磁化提取得到了高级特征之后
就可以输入到全连接层进行分类
这就是卷积神经网络的一个基本的结构
第一部分就给大家介绍了卷积神经网络的发展史
介绍了一个非常重要的生物实验
就是猫的视觉系统实验
这个实验对于卷积性网络来说是具有启发意义的
讲完实验给大家介绍了卷积神经网络发展史上的三个第一
第一个卷积神经网络出行
第一个大规模商用的卷积神经网络
第一个技惊四座的卷积算法后
这三个分别是这三个
分别是新任之基0.8x net
了解了卷积神经网络的发展史
我们稍作休息
接着回来再往下讲
第二部分
卷积层的操作
