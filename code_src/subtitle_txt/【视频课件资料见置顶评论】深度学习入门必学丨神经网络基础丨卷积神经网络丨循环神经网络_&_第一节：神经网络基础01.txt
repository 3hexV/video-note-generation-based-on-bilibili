各位同学大家好
欢迎来到深度之眼的paper入门班
我是于老师
这节课我们来学习神经网络与单层感知机
这一节课是属于选修知识当中的
神经网络部分的第一个课程
在选修知识当中
神经网络这个部分呢
我将会用一天的时间带领大家来了解
熟悉神经网络
为大家后面的paper学习打下基础铺平道路
在这一个部分
我将用三个课程来介绍神经网络
这三个课程我又称之为早中晚
也就是大家需要一天的时间就可以来了解
熟悉整个神经网络的概念了
在第一个课程
也就是我们这一节课
要讲解的是神经网络与单层感知机
在这一节课程中
我们会介绍一些关于神经网络的基础知识
一些激活函数反向传播算法
损失函数权重初始化和正则化方法
这些关于神经网络基础知识
在第二个课程也就是中篇
我们会介绍卷积神经网络
这个统治了图像领域的一个神经网络结构
其中会重点讲解它的一个发展历史
以及两个非常重要的操作
卷积和池化
第三个课程就是晚篇
介绍循环神经网络
一个统治了序列数据的神经网络结构
主要会讲解RNG
R u l
STM这三个常见的循环神经网络的结构
这就是我们关于神经网络这个部分的课程
的一个安排
它分为早中晚
我们需要一天的时间就可以了解
熟悉整个神经网络了
在正式学习神经网络之前
我们非常有必要给大家介绍
人工神经网络与人工智能
机器学习
深度学习这四者
他们之间的关系是怎么样的呢
他们的关系其实是一个包含关系
我们看到下边这个示意图
它们是一个包含的关系
概念最大的呢是人工智能
然后到机器学习
然后才到我们这一个课程要讲的神经网络
最后才是非常热门的深度学习的概念
在这里我们发现一些非常热门的deep learning
深度学习
它其实是属于神经网络的
而神经网络也属于机器学习的
从这一个图我们就知道
人工神经网络它的概念其实还是非常多的
由于人工神经网络的概念知识点是非常多的
我们一天的时间
可能没有办法把所有的内容都详细的去讲解
因此我们这一天的课时
一天的内容
只是带领大家进入神经网络这一个大门
进入门之后修行就要看个人了
那么在这里
我要给大家推荐一些非常好的学习资料
可以作为这一个课的补充
第一个需要推荐的是
吴恩达老师的深度学习课程
这个课程可以在这个网站上免费的
可以去获取到
它是一个视频的课程
可以去学习吴恩达老师的深度学习的课程
对于零基础入门神经网络入门
深度学习是非常友好的
它里面举了很多非常通俗易懂的例子
第二个要推荐的是李宏毅老师的
李宏毅老师是台湾的一个大学的一个教授
他的课程讲解例子是非常有趣生动的
这里非常推荐
可以去搜索一下李宏毅老师的相关一系列课程
大家可以到B站直接搜索李宏毅这三个字
就可以找到他一系列的关于机器学习
深度学习等等的课程
第三个需要推荐的就是deep learning这一本花书了
我们会俗称为花书
因为它的封面呢是一些花
所以会称为花书
花束呢它原版是英文的
但是也有中文的翻译
而且还是免费开源的
大家可以登录这个GITHUB上
可以免费获取到欢迎花书的中文版
话说呢更多的是作为一个进阶的教材
我们可以作为一个工具书
花书里面的知识点呢是比较丰富
比较全面
但是对于刚入门的同学来说还是比较COUT的
其实一开始很难读懂deep learning这本书
第四个需要推荐的就是周志华老师的机器学习
俗称西瓜书了
由于它的封面是西瓜
其实它里面讲了很多例子
也是采用挑选西瓜来做例子的
所以这本书也被大家俗称为西瓜书
西瓜书虽然是机器学习
但是从前面那一个关系图里面
理解神经网络也是非常重要的
因此在这里也推荐大家可以买一本
周志华老师的西瓜书
除了以上这四个资源
如果大家感兴趣的话
也可以到深度之眼当中的训练当中去找一下
关于花束的训练
以及周志华老师西瓜书机器学习的训练营
这两个训练营也是一个不错的资源
供大家去入门
神经网络
下面我们来看一下这节课
我们要讲的部分有哪些
这里我们总共会分为八个部分
这个内容还是非常丰富的
非常多的
第一部分我们学习人工神经元的概念
人工神经元与人类神经元它又是怎样联系的
我们在第一部分会讲解
第二部分来讲解第一个人工神经网络
多层感资金
第三部分讲解激活函数
激活函数对于单层感知机是非常重要的
为什么非常重要呢
我们会在第三部分讲解
第四部分是反向传播算法
第五部分是损失函数
第六部分是全职初始化
第七部分是政治化方法
第八部分呢
我们会对这一节课进行一个简单的小结
由于这节课我们学习到的知识还是非常多的
我们要对整个知识进行一个串讲
进行一个小结
在正式学习之前
我们要看一下这节课的学习目标
我们学内容
学知识
一定要带着目标去学
这样我们才可以事半功倍
首先我们来看第一个目标是关于人工神经元
我们要知道人工神经元
怎样从人类神经当中去抽象得来的
以及我们人工神经网络它有哪些连接方式
第二个学习目标是单层感知机
单层感知机对于人工神经网络是非常重要的
第三个目标是我们的激活函数
我们要知道常用的激活函数有哪些
第四个目标呢是反向传播算法
我们要了解反向传播算法到底传播什么东西
从后向前传播呢
第五个目标就是损失函数
我们要了解损失函数的定义概念是什么
第六个目标就是全值初始化
我们要知道全职初始化有哪些方法
它的方差又该如何去选择
第七个目标就是最后一个部分
我们正则化方法对于减轻过敏的现象
我们可以采用哪些方法来减轻它呢
以上就是我们这一个课程要学习的目标
我们一定要带着这七个目标
我们一定要带着这七个目标往下学习好
我们正式开始讲第一部分
人工神经元
人工神经元
它其实是从人类神经元当中抽象出来的
一种数学模型
我们来看下面这两个示意图
左边呢是人类神经元
右边是人工神经元
我们先看人类神经元它长什么样
我们在中学的生物课本当中应该都见过
这一个图
就是人类神经元的一个示意图
它最早是1904年被科学家所研究出来
发现的这么一个结构
我们简单来看一下这一个人类神经元
它的结构有哪些呢
在一个人类的生源当中
它有几个重要的部分
我们一一来看
第一个重要部分是树突
树突
它的作用是接收别的神经元传递给他的信息
我们看到这里有很多的输出
也就是它可以连接很多不同的神经元
给他揭示传递信息
接收这些神经元给他的信息
第二个主要的部分是细胞核
这个细胞核的作用
我们可以理解为
他对这些信息进行了一系列的处理
这是第二个部分
第三个部分就是这个很长的
我们这里称之为轴突以及轴突末梢
这是第三个部分
它的作用呢是用来传递细胞核
处理好这些信息传递给别的神经元
我们看到这里也是很多分叉的
也是它可以传递给别的很多的神经元
这就是一个人类神经元的主要结构
在这里我们可以把它抽象为三个部分
第一个部分输出呢我们可以理解为input input
也就输出它接收一些信号
第二个部分是细胞核
细胞核
我们可以理解为operations
也就是它进行一系列的处理
对这些信号这些数据进行处理
第三个部分呢是轴突
轴突末梢这一部分它可以理解为一个output
也就是输出
它会输出这些经过细胞核处理后的信息
传递给别的神经元
这就是一个人工神经元
我们可以简单的抽象出三个主要的部件
三个主要结构在1904年
研究出了人类神经元之后的39年
也就是1943年
有两个科学家就研究出了人工神经元
它是从人类神经元当中抽象得来的
一个数学模型
这个模型呢也称为mp模型
就是用这两个科学家的名字来命名的
这个模型称之为mp模型
我们看一下右边这个示意图
它就是mp模型的结构示意图了
我们看它有哪些结构呢
首先是input
这个input就是接收输入数据
然后有一个weight
也就是权重
这个权重呢就要与我们接收数据进行相乘的
再往下有一个圆形的
称之为sigma的一个操作
这里就是进行一个求和的操作
求和操作之后再往下有一个thread hood
它其实可以理解为一个激活函数
经过这个激活函数之后就得到最终的输出Y了
在这里我们来写一下这一个表达式
我们如何从输出的I到我们最终输出的Y呢
这里我们可以把输入I可以写成I乘以权重
W i
这里I呢我们要进行一个西格玛小I呢
它是从一到N
我们知道这里有N个输出
我们看一下这里是到IN的
这就是西格玛求和这个圆圈
这个神经元的一个操作了
操作完之后呢
他要经过一个threat food1函数
我们可以理解为一个function
这是一个切换数function f
然后这里输出呢它就是得到我们的output y了啊
这里应该是写成Y
写成O也可以
我们就可以得到Y
它表达式就是这样的
那这里我们重点来看一下threat to这个函数
这个函数呢其实它可以解为一个阈值函数
当这个值大于一定的阈值T的时候
它就会被激活
我们可以看一下这个方框里面
它其实就是一个一个函数的曲线了
我们可以把横轴理解为X
纵轴理解为Y
那么我们的黑色的曲线呢
呃在这个位置呢
它就是理解为阈值T
当我们X这个值大于这个阈值的时候
我们看到它就会输出这个一
如果小于这个阈值的时候
它就输出零
这里就正好对应到我们人工神
人类神经元当中的两种状态
一种是激活
一种是一致
所以我们这帮损它输出呢就两种状态
一种是一
一种是零
一呢就是激活零
就是抑制
从这里我们可以看到
NP模型与人类神经元模型它是非常相近的
我们来看这一个input
其实可以理解为输出
我们可以一一对应到输入的数据
不是直接简单的相加
而是对有些输出数据
它连接的强度比较强的地方
我们会给它较大的权重
也就是要乘以一个位置
前乘以权重
这里位置呢就是它一个连接的强度
再往下
我们看到有一个求和以及求和之后
有一个threat to
一个激活函数
一个function这个概念呢
我们可以把它理解为神经元当中的一个细胞核
也就是它会对这些数据进行一个处理
信息的处理
数据的处理就是在这一部分
再往下我们看到输出output这个Y这个部分呢
我们就可以连接
联系上这一个人类神学当中的轴突末梢
这样来这里诶
好我们回到这里
应该是连到这里
这个位置就是轴突末梢
可以理解为output
是我们NP模型当中
output这个Y从这两个示意图的对比联系
我们就知道了
NB模型几乎完全是从人类神经当中抽象而来
得到的人工神经元
我们了解了人工神经元的数学表达式
下面我们来看人工神经网络
它又是什么呢
人工神经网络
它是大量的神经元
以某种连接方式构成的机器学习模型
在这里我们要强调了神经网络模型
它其实还是属于机器学习模型的
人工神经网络的世界里
也有非常多的不同的神经网络
他们以不同的连接方式
就构成了不同的神经网络
我们来看左边这个示意图
就列举了一些常见的神经网络结构
我们先看左上角这第一个人工神经网络
待会我们也会详细讲解的
Perception
第一个人工神经网络
这个人工神经网络它的结构非常简单
只有输出和输出层
再往右边
这是一个前向传播的神经网络
它的数据是从左到右向前传播的
再往下看
我们来看一个RNAI
它是循环神经网络
这些网络它会循环的去使用
再往下看
第三列是一些常见的自编码
auto encoder的一系列神经网络结构
而这些多种多样的神经网络结构
它们不同之处就是它们的连接方式不同
所以不同的神经网络结构
我们只要知道它如何去连接的
我们就能了解这个网络结构了
因此我们要熟悉一种神经网络
我们最主要要了解它的神经元是如何去连接的
下面我们就来学习认识第一个人工神经网络
第一个人工神经网络
它是1958年由计算机科学家提出的
称之为perception感知机
我们在这个示意图也可以看到
左上角第一个就是perception了
Perception
它的数学模型其实与NP模型也是非常接近的
只不过他会把这个输入的地方呢
它会称之为input
也就是有一个input layer的概念
他把这一个输入X呢会称之为input
然后输出呢就是正常的一个输出output
Perception
它的数学运算与mp模型也非常接近的
我们看到左下角就是perception的计算公式
这个计算公式与NP模型也是非常接近的
首先是输入X会与权重W进行相乘
这里呢就用向量的乘法的形式了
这一个尖括号表示向量乘法这个地方
这里其实有一个权重
我们知道连接呢它是有一个权重的
表示这个连接的强度是大还是小
这里我们应该有W1W2
然后应该到WD应该有第一个
这里我们就可以把X和W写成向量的形式
进行一个乘法
乘完之后呢
它会经过一个偏置
这个B我们通常会称之为BIOS偏置项
这个我们可以理解为一个阈值
它们相乘相加之后会加上一个B
这个B其实就是偏置下
我们会称之为BIOS diai as偏置项
这个偏置项我们通常在画结构图的时候
一般不会把它画出来
我们看到右边这个结构图
也没有看到这个偏置项
这个偏置项是一个厂一个标量
我们可以理解为在这里它有一个偏置项
然后它的连接强度呢可以理解为一
如果要写的话
一般会这么写
就是我们偏偏置项
然后有一个强度强度是一
也就是加上一乘以BIOS这么一个形式
一般我们在绘制这个结构图的时候
一般不会把偏置项给绘制出来
大家要知道有这么一个操作就可以了
当计算得到这个值之后
我们输入到一个西格玛激活函数
这个激活函数我们在下面举了一个例子
当这个值输出的值大于零的时候呢
它就激活
也就是它输出为零
输出为一
如果这个值小于等于零的时候
它就输出为零
表示意志
也就是前面我们所提到的两种状态
当然这个激活函数西格玛
或者我们可以称之为function
它有很多不同的函数
我们在后面会详细去介绍
这里只是简单举了一个二值的激活函数
perception的提出呢
就掀起了第一次人工神经网络的热潮
也就是在1958年开始
人工神经网络就进入了高潮期
第一次高潮期
然而好景不长
1969年的时候
MUSIAI就证明了perception
它无法解决简单的诱惑问题
这个证明是感知机致命的一个缺点
自从MYSQY证明了
perception无法解决疑惑问题之后
人们对神经网络就持悲观态度
至此也引发了第一次人工神经网络的寒潮
也就是低谷期
为什么perception
无法去解决这个简单的疑惑问题呢
我们来看一下右边这个示意图异或问题
其实它就是一个逻辑运算
逻辑运算呢其实可以理解为一个二分类问题
零还是一
也就是真还是假的问题
我们来看一下
有两个逻辑变量A和B
当这两个变量都是零的时候
它的异或值是零
当这两个逻辑变量ab都是一的时候
它的优或者也是零
当A和B分别是零和一的时候呢
它的逻辑值就是一
这里我们可以定义一下
当零呢它就是甲
真假的假
当时一的时候我们就是真这样
它其实就是一个二分类问题
由于我们的输出变量只有两个
所以我们可以理解为它是一个二维平面上
进行一个二分类的问题
我们来看下面这个示意图
我们可以理解为坐标轴呢
我们可以理解为第一个坐标轴输出是X0
第二个坐标轴是X1
我们输出只有两个
一个S01
一个X1
在这个平面上
我们有四个样本
这里我们看到零零样本和一样本是同一个类
这里零零我们可以看到逻辑变量零零
它的类别呢是零
也就是甲我们这里用圆圈表示
也就是圆圈呢表示假
然后真呢就是三角形
三角形表示真
因此我们逻辑变量有四种状态
四个样本
它的类别应该是这样的
左下角和右上角应该是零
左上角右下角应该是一
那么为什么perception
没有办法去解决这个异或问题呢
其实perception在二维平面上它就是一个直线的
从perception的数表达式上去理解它
在二维平面上
其实它就是一条直线
一条直线没有办法去解解决这个疑惑问题
下面我们回顾一下perception它的数学表达式
在这里我们就不写通式了
由于我们是一个二元的输入
二维的输出
我们直接可以写成这样
X0乘以它的权重加上第二个神经元
我们可以理解X1乘以它的权重W1
再加上一个偏置项
这一项会经过一个激活函数
我们一个西格玛
然后输出就等于我们output了
由于我们的激活函数它是一个二次函数
我们暂时可以不考虑它
我们重点来关注里面这一项是什么
我们可以把里面这一项写成大X0
W0加上X1W1加上B然后是等于输出O
然后我们把这一个公式进行一个变换
我们把X1W1放到左边来
然后把第一项和第三项移到右边去
就是O减去X0乘以W0
再减去偏一个B
然后我们等式左边和右边同时除以W1
也就是让我们等号左边呢只剩下X1了
在右边呢把这个第二项提到前面来
为了我们后面好理解
也是W负的W0除以W1
然后要乘以X0
然后是我们的O这一项O呢再除以W1
然后再减去B除以W1好了
这一个表达式其实它就是一条直线的表达式
为什么这是一条直线表达式呢
这样看可能不太好理解
我们回顾一下中学阶段
我们经常用的一个直线的表达式是这样的
Y等于等于等于A写不出来
我写的这个Y等于KX加B这个表达形式
我们就会知道它是一条直线的形式
那么我们对应到刚刚推导出来的公式是这样的
我们这一项呢其实可以理解为一个斜率
然后这一项呢我们理解为它的偏置
它的截距我们看是不是是不是一一对应的
然后我们看一下X1就是我们的Y了
这里我们可以理解它是Y这个纵轴
然后这里的X呢就是我们的横轴
也就是它的X0
因此我们在二维平面上
我们的perception其实它就是一条直线
一条直线就可以把一个平面给切成两个部分
当直线的右边呢
它可以是一类直线的
左边是另外一类
因此它只能做简单的线性的二分类问题
它可以解决与问题或者或问题
我们看这样讲的
有这样一条perception
它就可以把货问题很好的解决掉
那么对于异或问题
它是没有办法用一条直线来解决的
我们假设随便画一条任何一条直线
我们没有办法让直线的一边是同类直线
另外一边是另外一类
因为我们看到这里
它是一个非线性的二分类问题
正因为MSI证明了这一个perceptual
没有办法解决非常简单的抑或二分类问题
所以导致了整个人工神经网络的第一次衰落
以上就是第一部分关于人工神经元
人工神经网络以及perception的知识
我们稍事休息
接下来讲单层感知机
